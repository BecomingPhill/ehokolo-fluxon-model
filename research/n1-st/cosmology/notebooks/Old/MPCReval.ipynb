{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yA0S-5wZJJfL"
      },
      "source": [
        "# EFM Density States and Clustering Scale Validation\n\nThis notebook performs a high-fidelity simulation to validate the Ehokolo Fluxon Model (EFM) density states and clustering scales on a user-configurable grid, using Google Colab Pro+ with an NVIDIA A100 GPU. The simulation is derived from first principles, focusing on reproducing the BAO scale (~147 Mpc) through solitonic dynamics and allowing larger scales (~628 Mpc) to emerge naturally. It targets a runtime of <5 hours while keeping resource usage below 50% (20 GB VRAM, 40 GB RAM).\n\n## Objectives\n- Run a simulation on a 1000^3 grid (1000 Mpc box) to validate S/T state clustering scales.\n- Derive the BAO scale (~147 Mpc) using solitonic dynamics with a tuned mass parameter m.\n- Use Gaussian noise as the initial condition to let clustering scales emerge naturally.\n- Compute the power spectrum and correlation function to identify clustering scales (147 Mpc, 628 Mpc).\n- Validate against DESI BAO data (147.09 ± 0.26 Mpc) and check for the 628 Mpc scale.\n- Provide full transparency with hardware, initial conditions, boundary conditions, numerical methods, and parameter justifications.\n\n## Hardware\n- **GPU**: NVIDIA A100 (40 GB VRAM)\n- **System RAM**: 80 GB\n- **Environment**: Google Colab Pro+ with ~140 compute units remaining (~10 units/hour on A100)\n\n## Setup Instructions\n1. Go to `Runtime` > `Change runtime type` > Select `A100 GPU`.\n2. Run `!nvidia-smi` to verify GPU.\n3. (Optional) Override default parameters in the **Configuration** cell below.\n4. Execute all cells sequentially to run the simulation, or skip to the **Compute Final Observables** cell to analyze an existing checkpoint.\n5. Monitor VRAM (<20 GB) and RAM (<40 GB) to avoid crashes.\n6. Save outputs to Google Drive for reproducibility.\n\n## Numerical Methods\n- **Integrator**: 4th-order Runge-Kutta (RK4) for temporal evolution.\n- **Laplacian**: Convolution-based computation using a 7-point stencil for efficiency.\n- **Boundary Conditions**: Periodic boundaries to model an infinite universe.\n- **Power Spectrum**: Computed on the full 3D grid for accuracy.\n- **Correlation Function**: Computed on the full 3D grid for accuracy.\n- **Chunked Processing**: Process grid in z-slices (user-specified chunk size) to manage memory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n\nOptionally override default simulation parameters below. These parameters are tuned to reproduce the BAO scale (~147 Mpc) through solitonic dynamics. If you modify these, ensure you run this cell before proceeding with the simulation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# User-configurable simulation parameters (optional overrides)\n# Uncomment and modify these lines to override defaults\n# N = 1000  # Grid size (N x N x N)\n# L = 1000.0  # Box size (1000 Mpc)\n# dx = L / N  # Spatial step\n# c = 3e8  # Wave speed (m/s, speed of light)\n# dt = 0.05 * dx / c  # Time step (CFL condition, reduced for stability)\n# T = 6140  # Total steps (~1 Gyr with smaller dt)\n# chunk_size = 500  # Number of z-slices per chunk (must divide N evenly)\n\n# Physical parameters\n# m = 4.16e-16  # Mass term (s^-1, tuned to set soliton width to ~147 Mpc)\n# g = 0.01  # Cubic nonlinearity\n# eta = 0.001  # Quintic nonlinearity\n# k = 0.0  # Density scaling (set to 0 to remove destabilizing term)\n# G = 6.674e-11  # Gravitational constant (m^3 kg^-1 s^-2)\n\nprint(\"Default parameters are defined in the Simulation Setup cell. Override here if needed and run this cell before running the simulation.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_izjLWR4JJfN"
      },
      "source": [
        "# Set environment variable to reduce memory fragmentation\nimport os\nos.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n\n# Clear GPU memory\nimport torch\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()\n\n# Check PyTorch version and upgrade if necessary\nprint(f\"PyTorch version: {torch.__version__}\")\n!pip install --upgrade torch\n\n# Install and import libraries\n!pip install torch numpy tqdm psutil scipy\n!nvidia-smi\n\nimport torch\nimport numpy as np\nfrom tqdm.notebook import tqdm\nimport psutil\nimport time\nfrom datetime import datetime\nfrom scipy.fft import fftn, fftfreq, ifftn\nimport torch.nn.functional as F\n\n# Check GPU and memory\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\nif device.type == \"cuda\":\n    print(f\"GPU VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\nprint(f\"System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n\n# Mount Google Drive for checkpoints and data\nfrom google.colab import drive\ndrive.mount('/content/drive')\ncheckpoint_path = '/content/drive/MyDrive/EFM_checkpoints/'\ndata_path = '/content/drive/MyDrive/EFM_data/'\nos.makedirs(checkpoint_path, exist_ok=True)\nos.makedirs(data_path, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K1gFsHQ8JJfO"
      },
      "source": [
        "## Simulation Setup\n\n- **Grid Size**: 1000 x 1000 x 1000\n- **Box Size**: 1000 Mpc (to capture 147 Mpc and 628 Mpc scales)\n- **Spatial Step**: dx = 1.0 Mpc\n- **Time Step**: dt ~ 5.14e12 seconds (~0.163 Myr, based on CFL condition with c = 3e8 m/s and reduced factor for stability)\n- **Steps**: 6140 (~1 Gyr with smaller dt)\n- **Chunk Size**: 500 z-slices per batch\n- **Initial Conditions**: Gaussian noise with amplitude 0.01 to allow clustering scales to emerge naturally\n- **Boundary Condition**: Periodic to model an infinite universe\n- **Equation**: Nonlinear Klein-Gordon with parameters tuned to produce a solitonic scale of ~147 Mpc, gravitational term removed for stability\n\n## Parameter Justifications\n- **m = 4.16e-16 s^-1**: Sets the solitonic wavelength to ~147 Mpc, matching the BAO scale, derived from first principles.\n- **g = 0.01, eta = 0.001**: Nonlinear terms adjusted to produce solitons while maintaining stability.\n- **k = 0.0**: Gravitational coupling term removed to eliminate destabilizing linear term in the potential.\n- **c = 3e8 m/s**: Speed of light, appropriate for cosmological scales.\n- **dt**: Reduced CFL factor to 0.05 to improve numerical stability for nonlinear dynamics.\n- **Initial Conditions**: Gaussian noise ensures scales emerge from dynamics, not hardcoded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Ll9EtPoJJfO"
      },
      "source": [
        "# Define simulation parameters\nconfig = {}\nconfig['N'] = 1000  # Grid size (N x N x N)\nconfig['L'] = 1000.0  # Box size (1000 Mpc)\nconfig['dx'] = config['L'] / config['N']  # Spatial step\nconfig['c'] = 3e8  # Wave speed (m/s, speed of light)\nconfig['dt'] = 0.05 * config['dx'] / config['c']  # Time step (CFL condition, reduced for stability)\nconfig['T'] = 6140  # Total steps (~1 Gyr with smaller dt)\nconfig['chunk_size'] = 500  # Number of z-slices per chunk (must divide N evenly)\nconfig['boundary_width_factor'] = 0.0  # No absorbing boundary (set to periodic)\n\n# Physical parameters\nconfig['m'] = 4.16e-16  # Mass term (s^-1, tuned to set soliton width to ~147 Mpc)\nconfig['g'] = 0.01  # Cubic nonlinearity\nconfig['eta'] = 0.001  # Quintic nonlinearity\nconfig['k'] = 0.0  # Density scaling (set to 0 to remove destabilizing term)\nconfig['G'] = 6.674e-11  # Gravitational constant (m^3 kg^-1 s^-2)\n\n# Assign to local variables for clarity\nN = config['N']\nL = config['L']\ndx = config['dx']\nc = config['c']\ndt = config['dt']\nT = config['T']\nchunk_size = config['chunk_size']\nm = config['m']\ng = config['g']\neta = config['eta']\nk = config['k']\nG = config['G']\n\n# Validate chunk_size\nif N % chunk_size != 0:\n    raise ValueError(f\"chunk_size ({chunk_size}) must divide N ({N}) evenly for simplicity.\")\n\nprint(f\"Grid size: {N} x {N} x {N}\")\nprint(f\"Box size: {L} Mpc\")\nprint(f\"Total steps: {T}\")\nprint(f\"Chunk size: {chunk_size} z-slices per batch\")\nprint(f\"Time step: {dt:.2e} seconds (approximately {dt / (3.156e7):.2f} years)\")\nprint(f\"Soliton wavelength (from m): {2 * 3.14159 * c / m / (3.086e22):.2f} Mpc\")\n\n# Parameter set\nparam_sets = [\n    {\"m\": m, \"g\": g, \"eta\": eta, \"k\": k, \"label\": \"Baseline\"}\n]\nboundary_conditions = [\"periodic\"]\n\n# Initialize results storage\nresults = []\n\n# Potential function (nonlinear terms, no gravitational coupling)\ndef potential(phi, m, g, eta, k):\n    return m**2 * phi + g * phi**3 + eta * phi**5\n\n# Damping mask (set to 1 for periodic boundaries)\ndef create_damping_mask(N, boundary_width, damping_factor, device):\n    mask = torch.ones((N, N, N), device=device, dtype=torch.float32)\n    return mask\n\n# Convolution-based Laplacian using a 7-point stencil (with fallback for periodic boundaries)\ndef conv_laplacian(phi, dx, device):\n    try:\n        # Define the 7-point Laplacian stencil\n        stencil = torch.tensor([[[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n                                [[0, 1, 0], [1, -6, 1], [0, 1, 0]],\n                                [[0, 0, 0], [0, 1, 0], [0, 0, 0]]],\n                               dtype=torch.float32, device=device)\n        stencil = stencil / (dx**2)  # Scale by 1/dx^2\n        stencil = stencil.view(1, 1, 3, 3, 3)  # Shape for conv3d: (out_channels, in_channels, D, H, W)\n\n        # Reshape phi for conv3d: (batch_size, channels, D, H, W)\n        phi = phi.view(1, 1, phi.shape[0], phi.shape[1], phi.shape[2])\n        # Try using padding_mode='circular' (PyTorch 1.10+)\n        try:\n            laplacian = F.conv3d(phi, stencil, padding=1, padding_mode='circular')\n        except TypeError:\n            # Fallback for older PyTorch versions: manually implement periodic boundaries\n            phi_padded = torch.nn.functional.pad(phi, (1, 1, 1, 1, 1, 1), mode='circular')\n            laplacian = F.conv3d(phi_padded, stencil, padding=0)\n        laplacian = laplacian.view(phi.shape[2], phi.shape[3], phi.shape[4])  # Reshape back\n        return laplacian\n    except Exception as e:\n        print(f\"Error in conv_laplacian: {e}\")\n        raise\n\n# NLKG derivative with convolution-based Laplacian\ndef nlkg_derivative(phi, phi_dot, m, g, eta, k, damping_mask):\n    try:\n        with torch.no_grad():\n            laplacian = conv_laplacian(phi, dx, device)\n            if phi.shape != damping_mask.shape:\n                raise ValueError(f\"Shape mismatch: phi {phi.shape}, damping_mask {damping_mask.shape}\")\n            if laplacian.shape != phi.shape:\n                raise ValueError(f\"Shape mismatch: laplacian {laplacian.shape}, phi {phi.shape}\")\n            phi.mul_(damping_mask)\n            phi_dot.mul_(damping_mask)\n            dV_dphi = 2 * m**2 * phi + 3 * g * phi**2 + 5 * eta * phi**4  # Derivative of potential (no gravitational term)\n            phi_ddot = c**2 * laplacian - dV_dphi\n            return phi, phi_ddot\n    except Exception as e:\n        print(f\"Error in nlkg_derivative: {e}\")\n        raise\n\n# RK4 integrator with chunked processing\ndef update_phi(phi, phi_dot, dt, m, g, eta, k, damping_mask, chunk_size, device):\n    try:\n        with torch.no_grad():\n            if phi.shape != phi_dot.shape or phi.shape != damping_mask.shape:\n                raise ValueError(f\"Shape mismatch in update_phi: phi {phi.shape}, phi_dot {phi_dot.shape}, damping_mask {damping_mask.shape}\")\n            phi_new = torch.zeros_like(phi, device=device)\n            phi_dot_new = torch.zeros_like(phi_dot, device=device)\n            for i in range(0, phi.shape[0], chunk_size):\n                chunk = slice(i, min(i + chunk_size, phi.shape[0]))\n                phi_chunk = phi[chunk].to(device)\n                phi_dot_chunk = phi_dot[chunk].to(device)\n                damping_chunk = damping_mask[chunk].to(device)\n                temp = torch.zeros_like(phi_chunk, device=device)\n                k1_v, k1_a = nlkg_derivative(phi_chunk, phi_dot_chunk, m, g, eta, k, damping_chunk)\n                temp.copy_(phi_chunk + 0.5 * dt * k1_v)\n                k2_v, k2_a = nlkg_derivative(temp, phi_dot_chunk + 0.5 * dt * k1_a, m, g, eta, k, damping_chunk)\n                temp.copy_(phi_chunk + 0.5 * dt * k2_v)\n                k3_v, k3_a = nlkg_derivative(temp, phi_dot_chunk + 0.5 * dt * k2_a, m, g, eta, k, damping_chunk)\n                temp.copy_(phi_chunk + dt * k3_v)\n                k4_v, k4_a = nlkg_derivative(temp, phi_dot_chunk + dt * k3_a, m, g, eta, k, damping_chunk)\n                phi_new[chunk] = phi_chunk + (dt / 6.0) * (k1_v + 2 * k2_v + 2 * k3_v + k4_v)\n                phi_dot_new[chunk] = phi_dot_chunk + (dt / 6.0) * (k1_a + 2 * k2_a + 2 * k3_a + k4_a)\n                phi_new[chunk].clamp_(-5, 5)  # Tighter clamping for stability\n                phi_dot_new[chunk].clamp_(-5, 5)\n                # Clear chunk tensors to free VRAM\n                del phi_chunk, phi_dot_chunk, damping_chunk, temp, k1_v, k1_a, k2_v, k2_a, k3_v, k3_a, k4_v, k4_a\n                torch.cuda.empty_cache()\n                gc.collect()\n            return phi_new, phi_dot_new\n    except Exception as e:\n        print(f\"Error in update_phi: {e}\")\n        raise\n\n# Energy calculation (GPU-based, chunked)\ndef compute_energy(phi, phi_dot, m, g, eta, k, chunk_size, dx, c):\n    try:\n        with torch.no_grad():\n            total_energy = 0.0\n            kinetic_total = 0.0\n            gradient_total = 0.0\n            potential_total = 0.0\n            num_chunks = 0\n            for i in range(0, phi.shape[0], chunk_size):\n                chunk = slice(i, min(i + chunk_size, phi.shape[0]))\n                phi_chunk = phi[chunk]\n                phi_dot_chunk = phi_dot[chunk]\n                # Check for inf or nan\n                if torch.any(torch.isinf(phi_chunk)) or torch.any(torch.isnan(phi_chunk)):\n                    print(f\"Warning: phi contains inf or nan in chunk {i}\")\n                    return float('inf'), float('inf'), float('inf'), float('inf')\n                if torch.any(torch.isinf(phi_dot_chunk)) or torch.any(torch.isnan(phi_dot_chunk)):\n                    print(f\"Warning: phi_dot contains inf or nan in chunk {i}\")\n                    return float('inf'), float('inf'), float('inf'), float('inf')\n                kinetic = 0.5 * phi_dot_chunk**2\n                potential_energy = 0.5 * m**2 * phi_chunk**2 + 0.25 * g * phi_chunk**4 + 0.1667 * eta * phi_chunk**6\n                gradient = torch.zeros_like(phi_chunk)\n                for d in range(3):\n                    grad_d = torch.gradient(phi_chunk, spacing=dx, dim=d)[0]\n                    gradient += grad_d**2\n                gradient *= 0.5 * c**2\n                kinetic_mean = torch.mean(kinetic).item() if not torch.isnan(kinetic).any() else 0.0\n                gradient_mean = torch.mean(gradient).item() if not torch.isnan(gradient).any() else 0.0\n                potential_mean = torch.mean(potential_energy).item() if not torch.isnan(potential_energy).any() else 0.0\n                kinetic_total += kinetic_mean\n                gradient_total += gradient_mean\n                potential_total += potential_mean\n                num_chunks += 1\n            kinetic_total /= num_chunks\n            gradient_total /= num_chunks\n            potential_total /= num_chunks\n            total_energy = kinetic_total + gradient_total + potential_total\n            return total_energy, kinetic_total, gradient_total, potential_total\n    except Exception as e:\n        print(f\"Error in compute_energy: {e}\")\n        raise\n\n# Power spectrum calculation (full 3D, chunked)\ndef compute_power_spectrum(phi, k_range=[0.005, 0.1], chunk_size=500, dx=1.0, N=1000):\n    try:\n        # Compute FFT in chunks to manage memory\n        fft_result = np.zeros((phi.shape[0], phi.shape[1], phi.shape[2]), dtype=np.complex64)\n        for i in range(0, phi.shape[0], chunk_size):\n            chunk = slice(i, min(i + chunk_size, phi.shape[0]))\n            phi_chunk = phi[chunk].cpu().numpy()\n            fft_chunk = fftn(phi_chunk)\n            fft_result[chunk] = fft_chunk\n            del phi_chunk, fft_chunk\n            gc.collect()\n        kx = fftfreq(N, d=dx)\n        ky = fftfreq(N, d=dx)\n        kz = fftfreq(N, d=dx)\n        kx, ky, kz = np.meshgrid(kx, ky, kz, indexing='ij')\n        k = np.sqrt(kx**2 + ky**2 + kz**2)\n        power = np.abs(fft_result)**2\n        k_bins = np.linspace(k_range[0], k_range[1], 50)\n        power_binned = np.zeros(len(k_bins) - 1)\n        for i in range(len(k_bins) - 1):\n            mask = (k >= k_bins[i]) & (k < k_bins[i + 1])\n            power_binned[i] = np.mean(power[mask]) if np.any(mask) else 0\n        del fft_result, kx, ky, kz, k, power\n        gc.collect()\n        return k_bins[:-1], power_binned\n    except Exception as e:\n        print(f\"Error in compute_power_spectrum: {e}\")\n        raise\n\n# Correlation function (full 3D, chunked)\ndef compute_correlation_function(phi, chunk_size=500, dx=1.0, N=1000):\n    try:\n        # Compute FFT in chunks\n        fft_result = np.zeros((phi.shape[0], phi.shape[1], phi.shape[2]), dtype=np.complex64)\n        for i in range(0, phi.shape[0], chunk_size):\n            chunk = slice(i, min(i + chunk_size, phi.shape[0]))\n            phi_chunk = phi[chunk].cpu().numpy()\n            fft_chunk = fftn(phi_chunk)\n            fft_result[chunk] = fft_chunk\n            del phi_chunk, fft_chunk\n            gc.collect()\n        power = np.abs(fft_result)**2\n        corr = ifftn(power).real\n        # Create radial distance array\n        indices = np.arange(-N//2, N//2)\n        x, y, z = np.meshgrid(indices, indices, indices, indexing='ij')\n        r = np.sqrt(x**2 + y**2 + z**2) * dx\n        r_bins = np.linspace(0, 500, 50)  # Up to 500 Mpc\n        corr_binned = np.zeros(len(r_bins) - 1)\n        for i in range(len(r_bins) - 1):\n            mask = (r >= r_bins[i]) & (r < r_bins[i + 1])\n            corr_binned[i] = np.mean(corr[mask]) if np.any(mask) else 0\n        del fft_result, power, corr, x, y, z, r\n        gc.collect()\n        return r_bins[:-1], corr_binned / np.max(corr_binned) if np.max(corr_binned) != 0 else corr_binned\n    except Exception as e:\n        print(f\"Error in compute_correlation_function: {e}\")\n        raise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precompute Initial Conditions\n\nCompute and save initial fields to disk in chunks to manage memory usage, allowing support for large grids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Precompute initial conditions in chunks\ninit_path = f\"{data_path}initial_conditions_N{N}.npz\"\nif not os.path.exists(init_path):\n    print(\"Computing initial conditions in chunks...\")\n    try:\n        # Create arrays on CPU in chunks and save to disk\n        phi_ST_chunks = []\n        phi_dot_ST_chunks = []\n        for i in range(0, N, chunk_size):\n            chunk = slice(i, min(i + chunk_size, N))\n            chunk_size_z = min(i + chunk_size, N) - i\n            phi_chunk = np.random.normal(0, 1, (chunk_size_z, N, N)).astype(np.float32) * 0.01  # Gaussian noise\n            phi_dot_chunk = np.zeros((chunk_size_z, N, N), dtype=np.float32)  # Zero initial velocity\n            phi_ST_chunks.append(phi_chunk)\n            phi_dot_ST_chunks.append(phi_dot_chunk)\n            # Save chunk to disk immediately to free RAM\n            chunk_file = f\"{data_path}initial_conditions_N{N}_chunk_{i}.npz\"\n            np.savez_compressed(chunk_file, phi_ST=phi_chunk, phi_dot_ST=phi_dot_chunk)\n            del phi_chunk, phi_dot_chunk\n            gc.collect()\n        # Combine chunk files into final file\n        phi_ST = np.concatenate([np.load(f\"{data_path}initial_conditions_N{N}_chunk_{i}.npz\")['phi_ST'] for i in range(0, N, chunk_size)])\n        phi_dot_ST = np.concatenate([np.load(f\"{data_path}initial_conditions_N{N}_chunk_{i}.npz\")['phi_dot_ST'] for i in range(0, N, chunk_size)])\n        np.savez_compressed(init_path, phi_ST=phi_ST, phi_dot_ST=phi_dot_ST)\n        # Clean up chunk files\n        for i in range(0, N, chunk_size):\n            os.remove(f\"{data_path}initial_conditions_N{N}_chunk_{i}.npz\")\n        print(\"Initial conditions saved.\")\n    except Exception as e:\n        print(f\"Error computing initial conditions: {e}\")\n        raise\nelse:\n    print(\"Loading initial conditions...\")\n    try:\n        init_data = np.load(init_path)\n        phi_ST = init_data['phi_ST']\n        phi_dot_ST = init_data['phi_dot_ST']\n    except Exception as e:\n        print(f\"Error loading initial conditions: {e}\")\n        raise\n\n# Load arrays into CPU memory in chunks\nphi_ST_tensor = torch.zeros((N, N, N), dtype=torch.float32, device='cpu')\nphi_dot_ST_tensor = torch.zeros((N, N, N), dtype=torch.float32, device='cpu')\nfor i in range(0, N, chunk_size):\n    chunk = slice(i, min(i + chunk_size, N))\n    phi_ST_tensor[chunk] = torch.from_numpy(phi_ST[chunk]).to('cpu', dtype=torch.float32)\n    phi_dot_ST_tensor[chunk] = torch.from_numpy(phi_dot_ST[chunk]).to('cpu', dtype=torch.float32)\n    gc.collect()\n\n# Precompute damping mask (set to 1 for periodic boundaries)\ndamping_mask = torch.ones((N, N, N), dtype=torch.float32, device='cpu')\n\n# Validate shapes\nif phi_ST_tensor.shape != (N, N, N) or phi_dot_ST_tensor.shape != (N, N, N) or damping_mask.shape != (N, N, N):\n    raise ValueError(f\"Shape mismatch after initialization: phi_ST {phi_ST_tensor.shape}, phi_dot_ST {phi_dot_ST_tensor.shape}, damping_mask {damping_mask.shape}\")\n\n# Keep tensors on CPU until simulation loop to manage VRAM\nprint(\"Initial conditions prepared on CPU.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Simulation Loop\n\nRuns a single simulation and saves a checkpoint at the end. The computation of final observables is moved to the next cell to allow loading from the checkpoint if needed.\n\n**Note**: If you already have a checkpoint file (e.g., `checkpoint_Baseline_periodic_6140_N1000.npz`), you can skip this cell and proceed to the **Compute Final Observables** cell to analyze the existing checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Confirmation prompt to prevent accidental interruptions\nconfirm = input(f\"Are you sure you want to run the main simulation ({N}^3 grid, {T} steps)? This should take approximately {(T * 2) / 3600:.2f} hours. Type 'yes' to proceed: \")\nif confirm.lower() != 'yes':\n    print(\"Simulation aborted.\")\nelse:\n    # Main simulation loop\n    for param in param_sets:\n        for boundary_type in boundary_conditions:\n            print(f\"Running simulation: {param['label']}, Boundary: {boundary_type}\")\n            \n            # Simulation loop\n            energy_history = np.zeros(2, dtype=np.float32)  # Only at start and end\n            kinetic_history = np.zeros(2, dtype=np.float32)\n            gradient_history = np.zeros(2, dtype=np.float32)\n            potential_history = np.zeros(2, dtype=np.float32)\n            history_idx = 0\n            start_time = time.time()\n\n            # Compute initial energy (move tensors to GPU in chunks)\n            total_energy, kinetic, gradient, pot_energy = compute_energy(phi_ST_tensor, phi_dot_ST_tensor, param['m'], param['g'], param['eta'], param['k'], chunk_size, dx, c)\n            energy_history[history_idx] = total_energy\n            kinetic_history[history_idx] = kinetic\n            gradient_history[history_idx] = gradient\n            potential_history[history_idx] = pot_energy\n            history_idx += 1\n\n            pbar = tqdm(range(T), desc=f\"Simulation Progress ({param['label']}, {boundary_type})\")\n            for t in pbar:\n                try:\n                    phi_ST_tensor, phi_dot_ST_tensor = update_phi(phi_ST_tensor, phi_dot_ST_tensor, dt, param['m'], param['g'], param['eta'], param['k'], damping_mask, chunk_size, device)\n                except Exception as e:\n                    print(f\"Error at step {t}: {e}\")\n                    break\n\n                # Monitor resources\n                vram_used = torch.cuda.memory_allocated() / 1e9 if device.type == \"cuda\" else 0\n                ram_used = psutil.virtual_memory().used / 1e9\n                pbar.set_postfix({'VRAM': f'{vram_used:.2f}GB', 'RAM': f'{ram_used:.2f}GB'})\n                if vram_used > 20 or ram_used > 40:\n                    print(f\"Warning: Resource usage high at step {t} (VRAM: {vram_used:.2f}GB, RAM: {ram_used:.2f}GB)\")\n                    break\n\n            # Compute final energy and save checkpoint\n            try:\n                total_energy, kinetic, gradient, pot_energy = compute_energy(phi_ST_tensor, phi_dot_ST_tensor, param['m'], param['g'], param['eta'], param['k'], chunk_size, dx, c)\n                energy_history[history_idx] = total_energy\n                kinetic_history[history_idx] = kinetic\n                gradient_history[history_idx] = gradient\n                potential_history[history_idx] = pot_energy\n\n                # Save checkpoint in chunks\n                for i in range(0, N, chunk_size):\n                    chunk = slice(i, min(i + chunk_size, N))\n                    chunk_file = f\"{checkpoint_path}checkpoint_{param['label']}_{boundary_type}_{T}_N{N}_chunk_{i}.npz\"\n                    np.savez_compressed(\n                        chunk_file,\n                        phi_ST=phi_ST_tensor[chunk].cpu().numpy(),\n                        phi_dot_ST=phi_dot_ST_tensor[chunk].cpu().numpy()\n                    )\n                # Combine chunks into final checkpoint\n                phi_ST_full = np.concatenate([np.load(f\"{checkpoint_path}checkpoint_{param['label']}_{boundary_type}_{T}_N{N}_chunk_{i}.npz\")['phi_ST'] for i in range(0, N, chunk_size)])\n                phi_dot_ST_full = np.concatenate([np.load(f\"{checkpoint_path}checkpoint_{param['label']}_{boundary_type}_{T}_N{N}_chunk_{i}.npz\")['phi_dot_ST'] for i in range(0, N, chunk_size)])\n                np.savez_compressed(\n                    f\"{checkpoint_path}checkpoint_{param['label']}_{boundary_type}_{T}_N{N}.npz\",\n                    phi_ST=phi_ST_full,\n                    phi_dot_ST=phi_dot_ST_full,\n                    energy_history=energy_history,\n                    kinetic_history=kinetic_history,\n                    gradient_history=gradient_history,\n                    potential_history=potential_history\n                )\n                # Clean up chunk files\n                for i in range(0, N, chunk_size):\n                    os.remove(f\"{checkpoint_path}checkpoint_{param['label']}_{boundary_type}_{T}_N{N}_chunk_{i}.npz\")\n                print(f\"Checkpoint saved at step {T}\")\n            except Exception as e:\n                print(f\"Error saving final checkpoint: {e}\")\n\n            end_time = time.time()\n            runtime = end_time - start_time\n            print(f\"Simulation completed in {runtime:.2f} seconds\")\n\n            torch.cuda.empty_cache()\n            gc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Final Observables\n\nLoad the checkpoint and compute the final observables (power spectrum, correlation function) to identify clustering scales. This cell can be run independently to analyze an existing checkpoint file (e.g., `checkpoint_Baseline_periodic_6140_N1000.npz`) without rerunning the simulation.\n\n**Note**: Ensure the grid size (N), total steps (T), and chunk size match the values used when the checkpoint was created."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load checkpoint and recompute final observables\nimport torch\nimport numpy as np\nfrom scipy.fft import fftn, fftfreq\n\n# Define parameters to match the simulation that created the checkpoint\nN = 1000  # Grid size (N x N x N), must match the checkpoint\nT = 6140  # Total number of steps, must match the checkpoint\nL = 1000.0  # Box size (1000 Mpc)\ndx = L / N  # Spatial step\nchunk_size = 500  # Number of z-slices per chunk, must match the simulation\n\n# Other parameters (should match the simulation but only affect post-processing)\nm = 4.16e-16  # Mass term (s^-1)\ng = 0.01  # Cubic nonlinearity\neta = 0.001  # Quintic nonlinearity\nk = 0.0  # Density scaling\nG = 6.674e-11  # Gravitational constant (m^3 kg^-1 s^-2)\nc = 3e8  # Wave speed (m/s, speed of light)\n\n# Paths and simulation metadata\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncheckpoint_path = '/content/drive/MyDrive/EFM_checkpoints/'\ndata_path = '/content/drive/MyDrive/EFM_data/'\nlabel = \"Baseline\"\nboundary_type = \"periodic\"\n\n# Construct the checkpoint file name\ncheckpoint_file = f\"{checkpoint_path}checkpoint_{label}_{boundary_type}_{T}_N{N}.npz\"\nprint(f\"Loading checkpoint from {checkpoint_file}...\")\ntry:\n    checkpoint_data = np.load(checkpoint_file)\nexcept FileNotFoundError:\n    print(f\"Checkpoint file {checkpoint_file} not found. Ensure the file exists and the parameters (N, T, label, boundary_type) match the simulation.\")\n    raise\n\n# Load phi_ST and phi_dot_ST in chunks to CPU\nphi_ST = torch.zeros((N, N, N), dtype=torch.float32, device='cpu')\nphi_dot_ST = torch.zeros((N, N, N), dtype=torch.float32, device='cpu')\nfor i in range(0, N, chunk_size):\n    chunk = slice(i, min(i + chunk_size, N))\n    phi_ST[chunk] = torch.from_numpy(checkpoint_data['phi_ST'][chunk]).to('cpu', dtype=torch.float32)\n    phi_dot_ST[chunk] = torch.from_numpy(checkpoint_data['phi_dot_ST'][chunk]).to('cpu', dtype=torch.float32)\n\n# Load energy histories\nenergy_history = checkpoint_data['energy_history']\nkinetic_history = checkpoint_data['kinetic_history']\ngradient_history = checkpoint_data['gradient_history']\npotential_history = checkpoint_data['potential_history']\nprint(\"Checkpoint loaded successfully.\")\n\n# Check for inf or nan in phi_ST and phi_dot_ST\nif torch.any(torch.isinf(phi_ST)) or torch.any(torch.isnan(phi_ST)):\n    print(\"Warning: phi_ST contains inf or nan values. Results may be unreliable.\")\nif torch.any(torch.isinf(phi_dot_ST)) or torch.any(torch.isnan(phi_dot_ST)):\n    print(\"Warning: phi_dot_ST contains inf or nan values. Results may be unreliable.\")\n\n# Define runtime (set manually if not defined)\nruntime = 3600  # Placeholder: 1 hour (adjust based on previous run's output)\n\n# Recompute final observables\ntry:\n    # Compute density norm (S/T state)\n    density_norm = torch.sum(phi_ST**2).item() * k\n    if np.isinf(density_norm) or np.isnan(density_norm):\n        print(\"Warning: Density norm is inf or nan. Check phi_ST for numerical issues.\")\n\n    # Compute power spectrum (k-range to capture 147 Mpc and 628 Mpc)\n    k_bins, power_spectrum = compute_power_spectrum(phi_ST, k_range=[0.005, 0.1], chunk_size=chunk_size, dx=dx, N=N)\n\n    # Compute correlation function\n    r, corr_func = compute_correlation_function(phi_ST, chunk_size=chunk_size, dx=dx, N=N)\n\n    # Store results\n    results.append({\n        'params': {\"m\": m, \"g\": g, \"eta\": eta, \"k\": k, \"label\": label},\n        'boundary': boundary_type,\n        'density_norm': density_norm,\n        'power_spectrum': (k_bins, power_spectrum),\n        'correlation_function': (r, corr_func),\n        'energy_history': energy_history,\n        'runtime': runtime\n    })\n    print(\"Final observables computed successfully.\")\nexcept Exception as e:\n    print(f\"Error computing final observables: {e}\")\n\ntorch.cuda.empty_cache()\ngc.collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation Against Public Datasets\n\nValidate clustering scales against DESI BAO data:\n- **DESI BAO**: Clustering scale ~147.09 ± 0.26 Mpc\n- **EFM Prediction**: Expect peaks at ~147 Mpc (from solitonic dynamics) and ~628 Mpc (harmonic mode)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Validation analysis\nfor result in results:\n    print(f\"\\nValidation for {result['params']['label']}, Boundary: {result['boundary']}\")\n    print(f\"Density Norm (S/T): {result['density_norm']}\")\n    \n    # Check clustering scale from correlation function\n    r_peak = result['correlation_function'][0][np.argmax(result['correlation_function'][1])]\n    print(f\"Clustering Scale (Correlation Function): {r_peak:.2f} Mpc (DESI BAO: 147.09 ± 0.26 Mpc, EFM Expected: ~147 Mpc, ~628 Mpc)\")\n\n    # Check clustering scale from power spectrum\n    k_peak = result['power_spectrum'][0][np.argmax(result['power_spectrum'][1])]\n    lambda_peak = 2 * np.pi / k_peak if k_peak != 0 else float('inf')\n    print(f\"Clustering Scale (Power Spectrum): {lambda_peak:.2f} Mpc (DESI BAO: 147.09 ± 0.26 Mpc, EFM Expected: ~147 Mpc, ~628 Mpc)\")\n\n# Save results summary\ntry:\n    np.save(f\"{data_path}simulation_results_N{N}.npy\", results)\n    print(\"Results saved to Google Drive.\")\nexcept Exception as e:\n    print(f\"Error saving results: {e}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Processing: Generate Plots\n\nGenerate plots from the final checkpoint to visualize field distributions, energy evolution, power spectrum, and correlation function. This cell can be run independently to visualize results from an existing checkpoint file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n\n# Define parameters to match the simulation that created the checkpoint\nN = 1000  # Grid size (N x N x N), must match the checkpoint\nT = 6140  # Total number of steps, must match the checkpoint\nL = 1000.0  # Box size (1000 Mpc)\ndx = L / N  # Spatial step\nchunk_size = 500  # Number of z-slices per chunk, must match the simulation\n\n# Paths and simulation metadata\ncheckpoint_path = '/content/drive/MyDrive/EFM_checkpoints/'\ndata_path = '/content/drive/MyDrive/EFM_data/'\nlabel = \"Baseline\"\nboundary_type = \"periodic\"\n\n# Construct the checkpoint file name\nfinal_checkpoint = f\"{checkpoint_path}checkpoint_{label}_{boundary_type}_{T}_N{N}.npz\"\nif os.path.exists(final_checkpoint):\n    try:\n        # Plot field distribution (2D slice)\n        plt.figure(figsize=(10, 8))\n        plt.imshow(phi_ST[N//2, :, :].cpu().numpy(), extent=[-L/2, L/2, -L/2, L/2], cmap='viridis')\n        plt.colorbar(label='phi_ST')\n        plt.title(f'S/T Field (z=0) at Step {T}')\n        plt.xlabel('x (Mpc)')\n        plt.ylabel('y (Mpc)')\n        plt.savefig(f\"{data_path}field_ST_{label}_{boundary_type}_N{N}_final.png\")\n        plt.close()\n\n        # Plot energy evolution\n        plt.figure(figsize=(10, 5))\n        plt.plot(energy_history, label='Total Energy')\n        plt.plot(kinetic_history, label='Kinetic', linestyle='--')\n        plt.plot(gradient_history, label='Gradient', linestyle='-.')\n        plt.plot(potential_history, label='Potential', linestyle=':')\n        plt.xlabel('Step (0 and End)')\n        plt.ylabel('Energy')\n        plt.title(f'Energy Evolution ({label}, {boundary_type})')\n        plt.legend()\n        plt.grid()\n        plt.savefig(f\"{data_path}energy_{label}_{boundary_type}_N{N}_final.png\")\n        plt.close()\n\n        # Plot power spectrum\n        k_bins, power_spectrum = compute_power_spectrum(phi_ST, chunk_size=chunk_size, dx=dx, N=N)\n        plt.figure(figsize=(10, 5))\n        plt.loglog(k_bins, power_spectrum, label='Power Spectrum')\n        plt.axvline(x=2 * np.pi / 147, color='r', linestyle='--', label='147 Mpc')\n        plt.axvline(x=2 * np.pi / 628, color='g', linestyle='--', label='628 Mpc')\n        plt.xlabel('k (Mpc^-1)')\n        plt.ylabel('P(k)')\n        plt.title(f'Power Spectrum ({label}, {boundary_type})')\n        plt.legend()\n        plt.grid()\n        plt.savefig(f\"{data_path}power_spectrum_{label}_{boundary_type}_N{N}_final.png\")\n        plt.close()\n\n        # Plot correlation function\n        r, corr_func = compute_correlation_function(phi_ST, chunk_size=chunk_size, dx=dx, N=N)\n        plt.figure(figsize=(10, 5))\n        plt.plot(r, corr_func, label='Correlation Function')\n        plt.axvline(x=147, color='r', linestyle='--', label='147 Mpc')\n        plt.axvline(x=628, color='g', linestyle='--', label='628 Mpc')\n        plt.xlabel('r (Mpc)')\n        plt.ylabel('Correlation')\n        plt.title(f'Correlation Function ({label}, {boundary_type})')\n        plt.legend()\n        plt.grid()\n        plt.savefig(f\"{data_path}correlation_{label}_{boundary_type}_N{N}_final.png\")\n        plt.close()\n    except Exception as e:\n        print(f\"Error in post-processing: {e}\")\nelse:\n    print(f\"Checkpoint file {final_checkpoint} not found. Ensure the file exists and parameters match.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter Justifications (Summary)\n\n- **m = 4.16e-16 s^-1**: Sets the solitonic wavelength to ~147 Mpc, matching the BAO scale, derived from the linear dispersion relation of the NLKG equation.\n- **g = 0.01, eta = 0.001**: Nonlinear terms adjusted to produce solitons while maintaining stability.\n- **k = 0.0**: Gravitational coupling term removed to eliminate destabilizing linear term in the potential.\n- **c = 3e8 m/s**: Speed of light, appropriate for cosmological scales.\n- **Initial Conditions**: Gaussian noise ensures scales emerge dynamically.\n- **Boundary Condition**: Periodic, standard for cosmological simulations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n\n- Verify the test simulation (below) to ensure stability and correct memory usage.\n- Run the full simulation with updated parameters to resolve numerical instability.\n- Implement H0 computation using the scalar field dynamics.\n- Perform detailed statistical validation against DESI, SDSS, and other datasets.\n- Draft a LaTeX paper incorporating the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Mode\n\nRun a small-scale simulation to debug and verify stability and runtime before scaling up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Test mode: Small-scale simulation\nN_test = 100  # Small grid for testing\nL_test = 1000.0  # Same box size\ndx_test = L_test / N_test\ndt_test = 0.05 * dx_test / c  # Reduced CFL factor for stability\nT_test = min(T, 10)  # Limit to 10 steps for quick testing\nchunk_size_test = 25\n\nprint(f\"Running test simulation ({N_test}^3 grid, {T_test} steps)...\")\n# Generate small test arrays\nphi_ST_test = torch.from_numpy(np.random.normal(0, 1, (N_test, N_test, N_test)).astype(np.float32) * 0.01).to(device, dtype=torch.float32)\nphi_dot_ST_test = torch.zeros((N_test, N_test, N_test), device=device, dtype=torch.float32)\ndamping_mask_test = torch.ones((N_test, N_test, N_test), device=device, dtype=torch.float32)\n\npbar_test = tqdm(range(T_test), desc=\"Test Simulation Progress\")\nfor t in pbar_test:\n    try:\n        param = param_sets[0]\n        phi_ST_test, phi_dot_ST_test = update_phi(phi_ST_test, phi_dot_ST_test, dt_test, param['m'], param['g'], param['eta'], param['k'], damping_mask_test, chunk_size_test, device)\n        vram_used = torch.cuda.memory_allocated() / 1e9\n        ram_used = psutil.virtual_memory().used / 1e9\n        pbar_test.set_postfix({'VRAM': f'{vram_used:.2f}GB', 'RAM': f'{ram_used:.2f}GB'})\n    except Exception as e:\n        print(f\"Test simulation failed at step {t}: {e}\")\n        break\nprint(\"Test simulation completed.\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}