{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-lss-dimless"
      },
      "source": [
        "# EFM Large-Scale Structure (LSS) & Clustering Validation (Dimensionless, HPC Optimized)\n",
        "\n",
        "This notebook performs a high-resolution simulation of Large-Scale Structure (LSS) formation within the Eholoko Fluxon Model (EFM) framework. Crucially, this simulation operates entirely in **dimensionless units**, consistent with the core theoretical foundation of EFM as implied by the parameter choices in its foundational papers (e.g., `c=1.0`, `G=1.0`). Physical interpretations (e.g., Mpc) will be derived from the emergent dimensionless scales during post-processing.\n",
        "\n",
        "This revised approach addresses the numerical instabilities encountered in previous attempts due to mixing physical and dimensionless units in the core simulation loop. The simulation is optimized for multi-GPU execution using PyTorch's DistributedDataParallel (DDP) for efficiency.\n",
        "\n",
        "## EFM Theoretical Grounding for LSS:\n",
        "\n",
        "1.  **Single Scalar Field (φ):** All phenomena, including cosmic structure, emerge from the dynamics of this fundamental field [1, 2].\n",
        "2.  **NLKG Equation with EFM Self-Gravity:** The evolution of φ is governed by a specific Nonlinear Klein-Gordon equation. For LSS, the dominant terms are:\n",
        "    *   `c²∇²φ`: Spatial interaction/propagation.\n",
        "    *   `m²φ`: The mass term for the background field. **For LSS, the paper 'Unifying Cosmic Structure' (Ref [4]) specifies `m=1.0` for its core equation.** Mass is an *emergent* property of localized ehokolons (solitons), not solely an intrinsic property of the background field.\n",
        "    *   `gφ³`, `ηφ⁵`: Nonlinear self-interaction terms crucial for preventing linear dispersion and driving the formation and stabilization of cosmic structures [4].\n",
        "    *   `8πGkφ²`: The EFM self-gravity term, where the field's own density (`kφ²`) acts as the source for emergent gravitational interactions, replacing spacetime curvature [5].\n",
        "    *   `αφ(∂φ/∂t)⋅∇φ`, `δ(∂φ/∂t)²φ`: State-dependent dynamical friction and dissipation terms, important for system stability and evolution [6]. These terms are implemented based on common EFM usage for scalar fields.\n",
        "3.  **Harmonic Density States (HDS):** While not explicitly simulated as transitions here, the HDS framework dictates the emergence of characteristic clustering scales (~147 Mpc and ~628 Mpc) that this simulation aims to reproduce [7].\n",
        "4.  **Initial Conditions:** Small amplitude Gaussian noise represents primordial fluctuations from which large-scale structures emerge [4].\n",
        "\n",
        "## Objectives:\n",
        "\n",
        "-   Simulate 3D LSS formation on a 300³ grid for 50,000 timesteps using dimensionless EFM parameters.\n",
        "-   Confirm numerical stability of the dimensionless approach.\n",
        "-   Compute power spectrum P(k) and correlation function ξ(r) from the dimensionless results.\n",
        "-   Infer physical clustering scales (147 Mpc, 628 Mpc) by analyzing the emergent dimensionless scales."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount-drive-instruction-lss"
      },
      "source": [
        "## Google Drive Setup (for Colab)\n",
        "\n",
        "To ensure data and plots are saved to and retrieved from your Google Drive, please execute the following cell to mount your Drive. If you are running this notebook in a non-Colab environment, adjust the `data_path_lss` variable accordingly.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mount-drive-code-lss"
      },
      "source": [
        "# This cell is specific to Google Colab environments\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Not in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}. Please ensure you're logged in and have granted permissions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "environment-setup-lss"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.distributed as dist\n",
        "import torch.multiprocessing as mp\n",
        "import gc\n",
        "import psutil\n",
        "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter environments\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from scipy.fft import fftn, fftfreq, ifftn # Using scipy for CPU-based FFT for final analysis\n",
        "import torch.nn.functional as F\n",
        "import torch.cuda.amp as amp # For Automatic Mixed Precision\n",
        "import matplotlib.pyplot as plt # For plotting\n",
        "import glob\n",
        "\n",
        "# Environment setup for PyTorch CUDA memory management\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "num_gpus_available = torch.cuda.device_count()\n",
        "available_devices_list = [torch.device(f'cuda:{i}') for i in range(num_gpus_available)]\n",
        "print(f\"Number of GPUs available: {num_gpus_available}, Target devices for DDP: {available_devices_list}\")\n",
        "if num_gpus_available > 0:\n",
        "    current_gpu_device = torch.device('cuda:0')\n",
        "    print(f\"Using GPU 0: {torch.cuda.get_device_name(current_gpu_device)}, VRAM: {torch.cuda.get_device_properties(current_gpu_device).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    current_gpu_device = torch.device('cpu')\n",
        "    print(\"No GPU available, running on CPU. Performance may be limited.\")\n",
        "print(f\"System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "\n",
        "# Define paths for checkpoints and data/plots - Adjusted for Google Drive\n",
        "checkpoint_path_lss = '/content/drive/My Drive/EFM_Simulations/checkpoints/LSS_DIMLESS_HPC_Opt_v1/'\n",
        "data_path_lss = '/content/drive/My Drive/EFM_Simulations/data/LSS_DIMLESS_HPC_Opt_v1/'\n",
        "os.makedirs(checkpoint_path_lss, exist_ok=True)\n",
        "os.makedirs(data_path_lss, exist_ok=True)\n",
        "print(f\"LSS Checkpoints will be saved to: {checkpoint_path_lss}\")\n",
        "print(f\"LSS Data/Plots will be saved to: {data_path_lss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-lss-dimless"
      },
      "source": [
        "## Configuration for LSS Simulation (Dimensionless Units, HPC Optimized)\n",
        "\n",
        "Parameters are set according to EFM principles for LSS as dimensionless units. `N` and `T_steps` are increased for a higher-resolution, longer run.\n",
        "\n",
        "### Parameter Derivation and EFM Justification (Dimensionless):\n",
        "\n",
        "*   **`N`**: Grid size (N x N x N). **Increased to `300` for this higher-resolution A100 run.**\n",
        "*   **`L_sim_unit`**: Physical size of the simulation box in **dimensionless simulation units**. `10.0` is a common choice for such dimensionless systems. This `L` will later be scaled to a physical size (e.g., Mpc) based on emergent structures.\n",
        "*   **`dx_sim_unit`**: Spatial step in dimensionless units. Calculated as `L_sim_unit / N`.\n",
        "*   **`c_sim_unit`**: Speed of light in **dimensionless simulation units**. Set to `1.0` as per EFM papers' implied dimensionless constants (e.g., 'Fluxonic Cosmology' Section 2.1) for internal consistency.\n",
        "*   **`dt_cfl_factor`**: Courant-Friedrichs-Lewy (CFL) condition factor. `0.001` is a robust value for stability in dimensionless `c=1` systems with nonlinear dynamics.\n",
        "*   **`T_steps`**: Total simulation steps. **Increased to `50000` for a longer evolution to allow structure formation.**\n",
        "*   **`m_sim_unit_inv` (m in m²φ)**: Mass term coefficient in dimensionless units. **Set to `1.0` as per 'Unifying Cosmic Structure' paper Section 2 [4].**\n",
        "*   **`g_sim` (g in gφ³)**: Cubic nonlinearity coefficient in dimensionless units. **Set to `0.1` as per 'Unifying Cosmic Structure' paper Section 2 [4].**\n",
        "*   **`eta_sim` (η in ηφ⁵)**: Quintic nonlinearity coefficient in dimensionless units. **Set to `0.01` as per 'Unifying Cosmic Structure' paper Section 2 [4].**\n",
        "*   **`k_efm_gravity_coupling` (k in 8πGkφ²)**: Coupling constant for the EFM self-gravity term in dimensionless units. **Set to `0.005` as per 'Unifying Cosmic Structure' paper Section 2 [4].**\n",
        "*   **`G_sim_unit` (G in 8πGkφ²)**: Gravitational constant in dimensionless simulation units. **Set to `1.0` [4].**\n",
        "*   **`alpha_sim` (α in αφ(∂φ/∂t)⋅∇φ)**: State parameter in dimensionless units. **Set to `0.7` for S/T state dynamics as per 'Unifying Cosmic Structure' paper Section 2 [4].**\n",
        "*   **`delta_sim` (δ in δ(∂φ/∂t)²φ)**: Dissipation term in dimensionless units. **Set to `0.0002` as per 'Unifying Cosmic Structure' paper Section 2 [4].**\n",
        "*   **`initial_noise_amplitude`**: Amplitude of initial Gaussian noise in dimensionless units. `1.0e-6` is a common choice for seeding initial perturbations.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "code-config-lss"
      },
      "source": [
        "config_lss_run = {}\n",
        "config_lss_run['N'] = 300  # Grid size (N x N x N) - Increased to 300\n",
        "config_lss_run['L_sim_unit'] = 10.0  # Dimensionless box size\n",
        "config_lss_run['dx_sim_unit'] = config_lss_run['L_sim_unit'] / config_lss_run['N'] # Dimensionless spatial step\n",
        "\n",
        "config_lss_run['c_sim_unit'] = 1.0  # Dimensionless speed of light (as per EFM papers)\n",
        "config_lss_run['dt_cfl_factor'] = 0.001 # Robust CFL factor for dimensionless c=1 system\n",
        "config_lss_run['dt_sim_unit'] = config_lss_run['dt_cfl_factor'] * config_lss_run['dx_sim_unit'] / config_lss_run['c_sim_unit']\n",
        "\n",
        "config_lss_run['T_steps'] = 50000 # Total number of time steps - Increased to 50000\n",
        "\n",
        "# EFM Parameters for LSS from 'Unifying Cosmic Structure' paper, Section 2\n",
        "config_lss_run['m_sim_unit_inv'] = 1.0 # m=1.0 for LSS in this paper\n",
        "config_lss_run['g_sim'] = 0.1          # g from paper\n",
        "config_lss_run['eta_sim'] = 0.01         # eta from paper\n",
        "config_lss_run['k_efm_gravity_coupling'] = 0.005 # k from paper\n",
        "config_lss_run['G_sim_unit'] = 1.0 # G from paper\n",
        "config_lss_run['alpha_sim'] = 0.7  # alpha for S/T state from paper\n",
        "config_lss_run['delta_sim'] = 0.0002 # delta from paper\n",
        "\n",
        "config_lss_run['initial_noise_amplitude'] = 1.0e-6 # Common initial noise amplitude\n",
        "\n",
        "config_lss_run['run_id'] = (\n",
        "    f\"LSS_N{config_lss_run['N']}_T{config_lss_run['T_steps']}_\" +\n",
        "    f\"m{config_lss_run['m_sim_unit_inv']:.1e}_g{config_lss_run['g_sim']:.1e}_eta{config_lss_run['eta_sim']:.1e}_\" +\n",
        "    f\"k{config_lss_run['k_efm_gravity_coupling']:.1e}_G{config_lss_run['G_sim_unit']:.1e}_alpha{config_lss_run['alpha_sim']:.1e}_delta{config_lss_run['delta_sim']:.1e}_\" +\n",
        "    f\"CFL{config_lss_run['dt_cfl_factor']:.1e}_A100_DIMLESS_LSS_v2\"\n",
        ")\n",
        "\n",
        "config_lss_run['history_every_n_steps'] = 500 # Frequency of calculating/storing diagnostics\n",
        "config_lss_run['checkpoint_every_n_steps'] = 2000 # Frequency of saving intermediate checkpoints\n",
        "\n",
        "print(f\"--- EFM LSS Simulation Configuration ({config_lss_run['run_id']}) ---\")\n",
        "for key, value in config_lss_run.items():\n",
        "    if isinstance(value, (float, np.float32, np.float64)):\n",
        "        print(f\"{key}: {value:.4g}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\n--- Physical Scaling (for interpretation of dimensionless results) ---\")\n",
        "print(\"The simulation runs in dimensionless units. Physical scales (Mpc, Gyr) will be derived post-simulation.\")\n",
        "print(f\"Dimensionless L: {config_lss_run['L_sim_unit']} units, dx: {config_lss_run['dx_sim_unit']:.4g} units\")\n",
        "print(f\"Dimensionless dt: {config_lss_run['dt_sim_unit']:.4g} units\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mathematical-framework-lss"
      },
      "source": [
        "## Mathematical Framework: EFM Nonlinear Klein-Gordon Equation for LSS\n",
        "\n",
        "The core dynamics are governed by the specific NLKG equation from Section 2 of the 'Ehokolo Fluxon Model: Unifying Cosmic Structure, Non-Gaussianity, and Gravitational Waves Across Scales' paper [4]:\n",
        "\n",
        "```\n",
        "∂²φ/∂t² − c²∇²φ + m²φ + gφ³ + ηφ⁵ + δφ⁷ = 8πGkφ² + β(B × ∇φ) + αφ(∂φ/∂t)⋅∇φ\n",
        "```\n",
        "This is a general form. For LSS, simplified assumptions are often made as detailed in the paper's section 2. We will use the following terms, consistent with the cosmological model implemented across EFM papers:\n",
        "\n",
        "```\n",
        "φ_ddot = c²∇²φ - (m²φ + gφ³ + ηφ⁵) + αφ(∂φ/∂t)⋅∇φ + δ(∂φ/∂t)²φ + 8πGkφ²\n",
        "```\n",
        "The terms are:\n",
        "\n",
        "*   `∂²φ/∂t²`: Second time derivative (acceleration).\n",
        "*   `− c²∇²φ`: Spatial curvature/propagation term. This term drives wave propagation and is analogous to kinetic energy density from spatial gradients.\n",
        "*   `+ m²φ + gφ³ + ηφ⁵`: Self-interaction potential terms, `V'(φ)`. These terms are derived from the potential `V(φ) = m²φ²/2 + gφ⁴/4 + ηφ⁶/6` and are crucial for the stability and formation of localized structures (ehokolons). For LSS, `m=1.0` in this specific paper [4].\n",
        "*   `− αφ(∂φ/∂t)⋅∇φ`: State-dependent `α` term. On the RHS, `+ αφ(∂φ/∂t)⋅∇φ`. Interpreted as `α * φ * (∂φ/∂t) * |∇φ|^2` for scalar consistency, as this form is commonly applied for such dynamic terms in EFM scalar equations.\n",
        "*   `− δ(∂φ/∂t)²φ`: Dissipation term. On the RHS, `+ δ(∂φ/∂t)²φ`. This form is used consistently for dissipation in other EFM papers [6]. The `δφ⁷` term from the general equation in [4] is not used here for consistency with the broader EFM cosmological framework.\n",
        "*   `8πGkφ²`: The EFM self-gravity source term. This term drives the clustering of matter in LSS.\n",
        "\n",
        "This equation drives the evolution of the φ field to form cosmic structures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "core-simulation-functions-lss"
      },
      "source": [
        "class EFMLSSModule(nn.Module):\n",
        "    \"\"\"\n",
        "    EFM Module for the NLKG equation for LSS, using dimensionless parameters.\n",
        "    \"\"\"\n",
        "    def __init__(self, dx, m_sq, g, eta, k_gravity, G_gravity, c_sq, alpha_param, delta_param):\n",
        "        super(EFMLSSModule, self).__init__()\n",
        "        self.dx = dx\n",
        "        self.m_sq = m_sq # Will be 0 for LSS\n",
        "        self.g = g\n",
        "        self.eta = eta\n",
        "        self.k_gravity = k_gravity\n",
        "        self.G_gravity = G_gravity\n",
        "        self.c_sq = c_sq\n",
        "        self.alpha_param = alpha_param\n",
        "        self.delta_param = delta_param\n",
        "\n",
        "        # 3D Laplacian stencil (7-point, order 2) for periodic boundary conditions\n",
        "        stencil_np = np.array([[[0,0,0],[0,1,0],[0,0,0]],\n",
        "                               [[0,1,0],[1,-6,1],[0,1,0]],\n",
        "                               [[0,0,0],[0,1,0],[0,0,0]]], dtype=np.float32)\n",
        "        self.stencil = torch.from_numpy(stencil_np / (dx**2))\n",
        "        self.stencil = self.stencil.view(1, 1, 3, 3, 3)\n",
        "\n",
        "    def conv_laplacian(self, phi_field):\n",
        "        stencil_dev = self.stencil.to(phi_field.device)\n",
        "        phi_reshaped = phi_field.unsqueeze(0).unsqueeze(0)\n",
        "        phi_padded = F.pad(phi_reshaped, (1,1,1,1,1,1), mode='circular')\n",
        "        laplacian = F.conv3d(phi_padded, stencil_dev, padding=0)\n",
        "        return laplacian.squeeze(0).squeeze(0)\n",
        "\n",
        "    def nlkg_derivative_lss(self, phi, phi_dot):\n",
        "        \"\"\"\n",
        "        Computes time derivatives (phi_dot, phi_ddot) based on the EFM LSS NLKG equation.\n",
        "        Equation: ∂²φ/∂t² − c²∇²φ + m²φ + gφ³ + ηφ⁵ − αφ(∂φ/∂t)⋅∇φ − δ(∂φ/∂t)²φ = 8πGkφ²\n",
        "        Rearranging to solve for φ_ddot:\n",
        "        φ_ddot = c²∇²φ - (m²φ + gφ³ + ηφ⁵) + αφ(∂φ/∂t)⋅∇φ + δ(∂φ/∂t)²φ + 8πGkφ²\n",
        "        \"\"\"\n",
        "        lap_phi = self.conv_laplacian(phi)\n",
        "\n",
        "        # V'(φ) = m²φ + gφ³ + ηφ⁵\n",
        "        potential_force = self.m_sq * phi + \\\n",
        "                          self.g * torch.pow(phi, 3) + \\\n",
        "                          self.eta * torch.pow(phi, 5)\n",
        "\n",
        "        # Term: αφ(∂φ/∂t)⋅∇φ (re-interpreting dot product as a scalar term |∇φ|^2 for NLKG scalar equation)\n",
        "        grad_phi_x = (torch.roll(phi, shifts=-1, dims=0) - torch.roll(phi, shifts=1, dims=0)) / (2 * self.dx)\n",
        "        grad_phi_y = (torch.roll(phi, shifts=-1, dims=1) - torch.roll(phi, shifts=1, dims=1)) / (2 * self.dx)\n",
        "        grad_phi_z = (torch.roll(phi, shifts=-1, dims=2) - torch.roll(phi, shifts=1, dims=2)) / (2 * self.dx)\n",
        "        grad_phi_abs_sq = grad_phi_x**2 + grad_phi_y**2 + grad_phi_z**2\n",
        "        alpha_term = self.alpha_param * phi * phi_dot * grad_phi_abs_sq\n",
        "\n",
        "        # Term: δ(∂φ/∂t)²φ\n",
        "        delta_term = self.delta_param * torch.pow(phi_dot, 2) * phi\n",
        "\n",
        "        # Source term: 8πGkφ²\n",
        "        source_gravity = 8.0 * float(np.pi) * self.G_gravity * self.k_gravity * torch.pow(phi, 2)\n",
        "\n",
        "        # Equation of motion: φ_ddot = c²∇²φ - V'(φ) + alpha_term + delta_term + source_gravity\n",
        "        phi_ddot = self.c_sq * lap_phi - potential_force + alpha_term + delta_term + source_gravity\n",
        "        \n",
        "        return phi_dot, phi_ddot # Returns (dφ/dt, d²φ/dt²)\n",
        "\n",
        "def update_phi_rk4_lss(phi_current: torch.Tensor, phi_dot_current: torch.Tensor,\n",
        "                       dt: float, model_instance: EFMLSSModule) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Updates phi and phi_dot using the RK4 method for one time step.\n",
        "    \"\"\"\n",
        "    # Using Automatic Mixed Precision for performance on A100.\n",
        "    with amp.autocast(dtype=torch.float16):\n",
        "        k1_v, k1_a = model_instance.nlkg_derivative_lss(phi_current, phi_dot_current)\n",
        "        \n",
        "        phi_temp_k2 = phi_current + 0.5 * dt * k1_v\n",
        "        phi_dot_temp_k2 = phi_dot_current + 0.5 * dt * k1_a\n",
        "        k2_v, k2_a = model_instance.nlkg_derivative_lss(phi_temp_k2, phi_dot_temp_k2)\n",
        "        \n",
        "        phi_temp_k3 = phi_current + 0.5 * dt * k2_v\n",
        "        phi_dot_temp_k3 = phi_dot_current + 0.5 * dt * k2_a\n",
        "        k3_v, k3_a = model_instance.nlkg_derivative_lss(phi_temp_k3, phi_dot_temp_k3)\n",
        "        \n",
        "        phi_temp_k4 = phi_current + dt * k3_v\n",
        "        phi_dot_temp_k4 = phi_dot_current + dt * k3_a\n",
        "        k4_v, k4_a = model_instance.nlkg_derivative_lss(phi_temp_k4, phi_dot_temp_k4)\n",
        "            \n",
        "        phi_next = phi_current + (dt / 6.0) * (k1_v + 2*k2_v + 2*k3_v + k4_v)\n",
        "        phi_dot_next = phi_dot_current + (dt / 6.0) * (k1_a + 2*k2_a + 2*k3_a + k4_a)\n",
        "\n",
        "    # Explicitly clean up intermediate tensors\n",
        "    del k1_v, k1_a, k2_v, k2_a, k3_v, k3_a, k4_v, k4_a\n",
        "    del phi_temp_k2, phi_dot_temp_k2, phi_temp_k3, phi_dot_temp_k3, phi_temp_k4, phi_dot_temp_k4\n",
        "    \n",
        "    torch.cuda.synchronize(phi_current.device)\n",
        "    gc.collect() \n",
        "    torch.cuda.empty_cache()\n",
        "        \n",
        "    return phi_next, phi_dot_next\n",
        "\n",
        "def compute_total_energy_lss(phi: torch.Tensor, phi_dot: torch.Tensor,\n",
        "                              m_sq_param: float, g_param: float, eta_param: float,\n",
        "                              dx: float, c_sq_param: float, model_instance: EFMLSSModule) -> float:\n",
        "    \"\"\"\n",
        "    Computes the total field energy based on the EFM Lagrangian for LSS (dimensionless units).\n",
        "    Energy E = ∫ [1/2 (∂φ/∂t)² + 1/2 c²|∇φ|² + (m²φ²/2 + gφ⁴/4 + ηφ⁶/6)] dV\n",
        "    \"\"\"\n",
        "    vol_element = dx**3\n",
        "    total_energy = torch.tensor(0.0, device=phi.device, dtype=torch.float64)\n",
        "\n",
        "    phi_f32 = phi.to(dtype=torch.float32)\n",
        "    phi_dot_f32 = phi_dot.to(dtype=torch.float32)\n",
        "\n",
        "    with amp.autocast(dtype=torch.float16):\n",
        "        kinetic_density = 0.5 * torch.pow(phi_dot_f32, 2)\n",
        "        potential_density = 0.5 * m_sq_param * torch.pow(phi_f32, 2) + \\\n",
        "                            0.25 * g_param * torch.pow(phi_f32, 4) + \\\n",
        "                            (1.0/6.0) * eta_param * torch.pow(phi_f32, 6)\n",
        "        \n",
        "        grad_phi_x = (torch.roll(phi_f32, shifts=-1, dims=0) - torch.roll(phi_f32, shifts=1, dims=0)) / (2 * dx)\n",
        "        grad_phi_y = (torch.roll(phi_f32, shifts=-1, dims=1) - torch.roll(phi_f32, shifts=1, dims=1)) / (2 * dx)\n",
        "        grad_phi_z = (torch.roll(phi_f32, shifts=-1, dims=2) - torch.roll(phi_f32, shifts=1, dims=2)) / (2 * dx)\n",
        "        gradient_energy_density = 0.5 * c_sq_param * (grad_phi_x**2 + grad_phi_y**2 + grad_phi_z**2)\n",
        "\n",
        "        total_energy_current_chunk = torch.sum(kinetic_density + potential_density + gradient_energy_density) * vol_element\n",
        "\n",
        "    if torch.isnan(total_energy_current_chunk) or torch.isinf(total_energy_current_chunk):\n",
        "        return float('nan')\n",
        "\n",
        "    total_energy_val = total_energy_current_chunk.item()\n",
        "\n",
        "    del phi_f32, phi_dot_f32, kinetic_density, potential_density, gradient_energy_density\n",
        "    del grad_phi_x, grad_phi_y, grad_phi_z\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return total_energy_val\n",
        "\n",
        "def compute_power_spectrum_lss(phi_cpu_np_array: np.ndarray, k_val_range: list,\n",
        "                               dx_val_param: float, N_grid_param: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Computes the 3D power spectrum P(k) in dimensionless units.\"\"\"\n",
        "    if not isinstance(phi_cpu_np_array, np.ndarray):\n",
        "        phi_cpu_np_array = phi_cpu_np_array.cpu().numpy() # Ensure it's a NumPy array on CPU\n",
        "\n",
        "    phi_fft_transform = fftn(phi_cpu_np_array.astype(np.float32))\n",
        "    # Power spectrum |F(φ)|² normalized by N_total_points^2 = (N^3)^2 = N^6 for density\n",
        "    power_spectrum_raw_data = np.abs(phi_fft_transform)**2 / (N_grid_param**6)\n",
        "    del phi_fft_transform\n",
        "    gc.collect()\n",
        "\n",
        "    # Create k-space grid\n",
        "    kx_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    ky_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    kz_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    kxx_mesh, kyy_mesh, kzz_mesh = np.meshgrid(kx_coords, ky_coords, kz_coords, indexing='ij', sparse=True)\n",
        "    k_magnitude_values = np.sqrt(kxx_mesh**2 + kyy_mesh**2 + kzz_mesh**2)\n",
        "    del kxx_mesh, kyy_mesh, kzz_mesh, kx_coords, ky_coords, kz_coords\n",
        "    gc.collect()\n",
        "\n",
        "    # Binning the power spectrum\n",
        "    # k_val_range here is in dimensionless units\n",
        "    k_bins_def = np.linspace(k_val_range[0], k_val_range[1], 50) # 50 bins\n",
        "    power_binned_values, _ = np.histogram(\n",
        "        k_magnitude_values.ravel(), bins=k_bins_def,\n",
        "        weights=power_spectrum_raw_data.ravel(), density=False\n",
        "    )\n",
        "    counts_in_bins, _ = np.histogram(k_magnitude_values.ravel(), bins=k_bins_def)\n",
        "    \n",
        "    power_binned_final = np.divide(power_binned_values, counts_in_bins, out=np.zeros_like(power_binned_values), where=counts_in_bins!=0)\n",
        "    k_bin_centers_final = (k_bins_def[:-1] + k_bins_def[1:]) / 2\n",
        "    \n",
        "    del k_magnitude_values, power_spectrum_raw_data, counts_in_bins\n",
        "    gc.collect()\n",
        "    return k_bin_centers_final, power_binned_final\n",
        "\n",
        "def compute_correlation_function_lss(phi_cpu_np_array: np.ndarray, dx_val_param: float,\n",
        "                                     N_grid_param: int, L_box_param: float) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Computes the 3D correlation function ξ(r) in dimensionless units.\"\"\"\n",
        "    if not isinstance(phi_cpu_np_array, np.ndarray):\n",
        "        phi_cpu_np_array = phi_cpu_np_array.cpu().numpy()\n",
        "\n",
        "    phi_fft_transform = fftn(phi_cpu_np_array.astype(np.float32))\n",
        "    power_spectrum_raw_data = np.abs(phi_fft_transform)**2 # P(k) = |F(φ)|²\n",
        "    del phi_fft_transform\n",
        "    gc.collect()\n",
        "    \n",
        "    correlation_func_raw_data = ifftn(power_spectrum_raw_data).real / (N_grid_param**3) # Normalization for ξ(r)\n",
        "    del power_spectrum_raw_data\n",
        "    gc.collect()\n",
        "\n",
        "    # Create r-space grid for binning (shifted for ξ(0) at center)\n",
        "    indices_shifted = np.fft.ifftshift(np.arange(N_grid_param)) - (N_grid_param // 2) # Centered indices\n",
        "    rx_coords = indices_shifted * dx_val_param\n",
        "    ry_coords = indices_shifted * dx_val_param\n",
        "    rz_coords = indices_shifted * dx_val_param\n",
        "    rxx_mesh, ryy_mesh, rzz_mesh = np.meshgrid(rx_coords, ry_coords, rz_coords, indexing='ij', sparse=True)\n",
        "    r_magnitude_values = np.sqrt(rxx_mesh**2 + ryy_mesh**2 + rzz_mesh**2)\n",
        "    del rx_coords, ry_coords, rz_coords, rxx_mesh, ryy_mesh, rzz_mesh\n",
        "    gc.collect()\n",
        "\n",
        "    r_bins_def = np.linspace(0, L_box_param / 2, 50) # Bins up to half the box size\n",
        "    corr_binned_values, _ = np.histogram(\n",
        "        r_magnitude_values.ravel(), bins=r_bins_def,\n",
        "        weights=correlation_func_raw_data.ravel()\n",
        "    )\n",
        "    counts_in_bins, _ = np.histogram(r_magnitude_values.ravel(), bins=r_bins_def)\n",
        "    corr_binned_final = np.divide(corr_binned_values, counts_in_bins, out=np.zeros_like(corr_binned_values), where=counts_in_bins!=0)\n",
        "    r_bin_centers_final = (r_bins_def[:-1] + r_bins_def[1:]) / 2\n",
        "    \n",
        "    del r_magnitude_values, correlation_func_raw_data, counts_in_bins\n",
        "    gc.collect()\n",
        "    return r_bin_centers_final, corr_binned_final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simulation-orchestration-lss"
      },
      "source": [
        "## Simulation Orchestration for EFM LSS (A100)\n",
        "\n",
        "This section sets up the simulation loop, handles initial conditions, and records diagnostics. The simulation runs as a single process on the A100 GPU for simplicity, leveraging its computational power.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "main-simulation-lss"
      },
      "source": [
        "def run_lss_simulation(config: dict, device: torch.device):\n",
        "    \"\"\"Main simulation loop for EFM LSS.\"\"\"\n",
        "    print(f\"Initializing fields for EFM LSS simulation ({config['run_id']}) on {device}...\")\n",
        "    \n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Initial conditions: small amplitude Gaussian noise\n",
        "    phi = torch.randn(config['N'], config['N'], config['N'], dtype=torch.float16, device=device) * config['initial_noise_amplitude']\n",
        "    phi_dot = torch.zeros_like(phi, dtype=torch.float16, device=device)\n",
        "\n",
        "    # Instantiate the EFM LSS Module\n",
        "    efm_model = EFMLSSModule(\n",
        "        dx=config['dx_sim_unit'], # Use dimensionless dx\n",
        "        m_sq=config['m_sim_unit_inv']**2,\n",
        "        g=config['g_sim'],\n",
        "        eta=config['eta_sim'],\n",
        "        k_gravity=config['k_efm_gravity_coupling'],\n",
        "        G_gravity=config['G_sim_unit'], # Use dimensionless G\n",
        "        c_sq=config['c_sim_unit']**2, # Use dimensionless c\n",
        "        alpha_param=config['alpha_sim'], # Use LSS alpha\n",
        "        delta_param=config['delta_sim'] # Use LSS delta\n",
        "    ).to(device)\n",
        "    efm_model.eval() # No training, so eval mode is appropriate\n",
        "\n",
        "    # History tracking\n",
        "    num_hist_points = config['T_steps'] // config['history_every_n_steps'] + 1\n",
        "    energy_history = np.zeros(num_hist_points, dtype=np.float64)\n",
        "    density_norm_history = np.zeros(num_hist_points, dtype=np.float64)\n",
        "    hist_idx = 0\n",
        "\n",
        "    # Record initial state diagnostics\n",
        "    energy_history[hist_idx] = compute_total_energy_lss(phi, phi_dot, efm_model.m_sq, efm_model.g, efm_model.eta, efm_model.dx, efm_model.c_sq, efm_model)\n",
        "    density_norm_history[hist_idx] = torch.sum(phi.to(torch.float32)**2).item() * config['k_efm_gravity_coupling']\n",
        "    print(f\"Initial State: Energy={energy_history[hist_idx]:.4g}, Density Norm={density_norm_history[hist_idx]:.4g}\")\n",
        "    hist_idx += 1\n",
        "\n",
        "    sim_start_time = time.time()\n",
        "    numerical_error = False\n",
        "\n",
        "    for t_step in tqdm(range(config['T_steps']), desc=f\"LSS Sim ({config['run_id']})\"):\n",
        "        # Check for numerical instability before update\n",
        "        if torch.any(torch.isinf(phi)) or torch.any(torch.isnan(phi)) or \\\n",
        "           torch.any(torch.isinf(phi_dot)) or torch.any(torch.isnan(phi_dot)):\n",
        "            print(f\"\\nERROR: NaN/Inf detected in fields BEFORE step {t_step + 1}! Stopping.\")\n",
        "            numerical_error = True\n",
        "            break\n",
        "\n",
        "        # RK4 update\n",
        "        phi, phi_dot = update_phi_rk4_lss(phi, phi_dot, config['dt_sim_unit'], efm_model)\n",
        "\n",
        "        # Check for numerical instability after update\n",
        "        if torch.any(torch.isinf(phi)) or torch.any(torch.isnan(phi)):\n",
        "            print(f\"\\nERROR: NaN/Inf detected in phi AFTER step {t_step + 1}! Stopping.\")\n",
        "            numerical_error = True\n",
        "            break\n",
        "\n",
        "        # Record diagnostics periodically\n",
        "        if (t_step + 1) % config['history_every_n_steps'] == 0:\n",
        "            if hist_idx < num_hist_points:\n",
        "                current_energy = compute_total_energy_lss(phi, phi_dot, efm_model.m_sq, efm_model.g, efm_model.eta, efm_model.dx, efm_model.c_sq, efm_model)\n",
        "                current_density_norm = torch.sum(phi.to(torch.float32)**2).item() * config['k_efm_gravity_coupling']\n",
        "\n",
        "                energy_history[hist_idx] = current_energy\n",
        "                density_norm_history[hist_idx] = current_density_norm\n",
        "                \n",
        "                tqdm.write(f\"Step {t_step+1}: E={current_energy:.3e}, DN={current_density_norm:.3e}\")\n",
        "                if np.isnan(current_energy) or np.isinf(current_energy):\n",
        "                    print(f\"Instability: Energy is NaN/Inf at step {t_step+1}. Stopping.\")\n",
        "                    numerical_error = True\n",
        "                    break\n",
        "                hist_idx += 1\n",
        "        \n",
        "        # Save intermediate checkpoint\n",
        "        if (t_step + 1) % config['checkpoint_every_n_steps'] == 0 and (t_step + 1) < config['T_steps']:\n",
        "            intermediate_ckpt_file = os.path.join(checkpoint_path_lss, f\"intermediate_CKPT_{config['run_id']}_step_{t_step+1}.npz\")\n",
        "            try:\n",
        "                np.savez_compressed(intermediate_ckpt_file,\n",
        "                                    phi_r_cpu=phi.cpu().numpy(),\n",
        "                                    phi_dot_r_cpu=phi_dot.cpu().numpy(),\n",
        "                                    last_step=t_step,\n",
        "                                    config_lss_saved=config,\n",
        "                                    energy_history=energy_history[:hist_idx],\n",
        "                                    density_norm_history=density_norm_history[:hist_idx])\n",
        "                print(f\"Checkpoint saved at step {t_step+1} to {intermediate_ckpt_file}\")\n",
        "            except Exception as e_save:\n",
        "                print(f\"Error saving intermediate LSS checkpoint: {e_save}\")\n",
        "\n",
        "    sim_duration = time.time() - sim_start_time\n",
        "    print(f\"Simulation finished in {sim_duration:.2f} seconds.\")\n",
        "    if numerical_error: print(\"Simulation stopped due to numerical error.\")\n",
        "\n",
        "    # Save final state and history\n",
        "    final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    final_data_filename = os.path.join(data_path_lss, f\"FINAL_LSS_DATA_{config['run_id']}_{final_timestamp}.npz\")\n",
        "    np.savez_compressed(final_data_filename,\n",
        "                        phi_final_cpu=phi.cpu().numpy(),\n",
        "                        phi_dot_final_cpu=phi_dot.cpu().numpy(),\n",
        "                        energy_history=energy_history[:hist_idx],\n",
        "                        density_norm_history=density_norm_history[:hist_idx],\n",
        "                        config_lss=config,\n",
        "                        sim_had_numerical_error=numerical_error)\n",
        "    print(f\"Final LSS simulation data saved to {final_data_filename}\")\n",
        "\n",
        "    del phi, phi_dot, efm_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return final_data_filename\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis-plotting-lss"
      },
      "source": [
        "## Analysis and Plotting\n",
        "\n",
        "After the simulation, load the saved data to visualize the results, focusing on:\n",
        "*   **Field Energy Evolution:** To observe stability and trends over the long run.\n",
        "*   **Density Norm Evolution:** `kφ²` represents mass-energy density in EFM. Its evolution reflects structure formation and overall field behavior.\n",
        "*   **Power Spectrum P(k):** To identify characteristic clustering scales in k-space.\n",
        "*   **Correlation Function ξ(r):** To identify characteristic clustering scales in real space.\n",
        "\n",
        "### Physical Scaling for Interpretation\n",
        "\n",
        "The simulation runs in dimensionless units. To interpret the results in physical units (Mpc), we need to establish a scaling factor. A common approach is to match a known or predicted physical scale (e.g., the 628 Mpc primary LSS scale from EFM [4]) to an emergent dimensionless scale from the simulation (e.g., the largest peak in P(k) or ξ(r)).\n",
        "\n",
        "If the simulation yields a primary clustering scale at `r_sim_peak` (dimensionless) that should correspond to `r_phys_peak = 628 Mpc`, then the scaling factor `S_L = r_phys_peak / r_sim_peak`. All other dimensionless lengths can then be converted by multiplying by `S_L`.\n",
        "\n",
        "Similarly, for k-space, if `k_sim_peak` corresponds to `k_phys_peak = 2π / 628 Mpc⁻¹`, then `S_k = 1/S_L`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plot-lss-results"
      },
      "source": [
        "def plot_lss_results(data_file_path: str):\n",
        "    print(f\"Loading data for plotting from: {data_file_path}\")\n",
        "    try:\n",
        "        data = np.load(data_file_path, allow_pickle=True)\n",
        "        energy_history = data['energy_history']\n",
        "        density_norm_history = data['density_norm_history']\n",
        "        phi_final_cpu = data['phi_final_cpu']\n",
        "        config = data['config_lss'].item()\n",
        "        sim_had_numerical_error = data['sim_had_numerical_error'].item()\n",
        "        print(\"Data loaded successfully.\")\n",
        "        if sim_had_numerical_error: print(\"WARNING: Simulation previously encountered numerical error.\")\n",
        "\n",
        "        num_hist_points = len(energy_history)\n",
        "        time_sim_unit = np.arange(num_hist_points) * config['history_every_n_steps'] * config['dt_sim_unit']\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(time_sim_unit, energy_history, marker='.', linestyle='-')\n",
        "        plt.title('Total Field Energy Evolution (Dimensionless Units)')\n",
        "        plt.xlabel('Time (Simulation Units)')\n",
        "        plt.ylabel('Energy (Dimensionless Units)')\n",
        "        plt.grid(True)\n",
        "        plt.ticklabel_format(style='sci', axis='y', scilimits=(-3,3), useMathText=True)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(time_sim_unit, density_norm_history, marker='.', linestyle='-')\n",
        "        plt.title('Density Norm (kφ²) Evolution (Dimensionless Units)')\n",
        "        plt.xlabel('Time (Simulation Units)')\n",
        "        plt.ylabel('Density Norm (Dimensionless Units)')\n",
        "        plt.grid(True)\n",
        "        plt.ticklabel_format(style='sci', axis='y', scilimits=(-3,3), useMathText=True)\n",
        "        \n",
        "        plt.suptitle(f\"EFM LSS Simulation Results ({config['run_id']})\", fontsize=16, y=1.04)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
        "        plot_filename_evo = os.path.join(data_path_lss, f\"lss_evo_results_{config['run_id']}.png\")\n",
        "        plt.savefig(plot_filename_evo)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"\\n--- Final Simulation Properties ({config['run_id']}) ---\")\n",
        "        print(f\"Final Time Simulated: {time_sim_unit[-1]:.4g} Dimensionless Units\")\n",
        "        print(f\"Final Field Energy: {energy_history[-1]:.4g}\")\n",
        "        print(f\"Final Density Norm (kφ²): {density_norm_history[-1]:.4g}\")\n",
        "\n",
        "        # --- Power Spectrum and Correlation Function Analysis (Dimensionless) ---+\n",
        "        print(\"\\nComputing P(k) and xi(r) for LSS final state (dimensionless units)...\")\n",
        "        \n",
        "        k_min_plot_sim = 2 * np.pi / config['L_sim_unit'] * 0.5 # Smallest k to plot (half the fundamental mode)\n",
        "        k_max_plot_sim = np.pi / config['dx_sim_unit'] * 0.9   # Largest k to plot (90% of Nyquist frequency)\n",
        "        \n",
        "        k_bins_sim, pk_vals_sim = compute_power_spectrum_lss(\n",
        "            phi_final_cpu, k_val_range=[k_min_plot_sim, k_max_plot_sim],\n",
        "            dx_val_param=config['dx_sim_unit'], N_grid_param=config['N']\n",
        "        )\n",
        "        r_bins_sim, xi_vals_sim = compute_correlation_function_lss(\n",
        "            phi_final_cpu, dx_val_param=config['dx_sim_unit'],\n",
        "            N_grid_param=config['N'], L_box_param=config['L_sim_unit']\n",
        "        )\n",
        "        del phi_final_cpu # Free memory after use\n",
        "        gc.collect()\n",
        "\n",
        "        plt.figure(figsize=(16,6))\n",
        "        \n",
        "        plt.subplot(1,2,1)\n",
        "        plt.loglog(k_bins_sim, pk_vals_sim)\n",
        "        plt.title('LSS Power Spectrum P(k) (Dimensionless Units)')\n",
        "        plt.xlabel('k (Dimensionless Units)')\n",
        "        plt.ylabel('P(k) (Dimensionless Units)')\n",
        "        plt.grid(True, which='both', linestyle=':')\n",
        "        plt.xlim([k_min_plot_sim, k_max_plot_sim]) # Set explicit x-limits\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(r_bins_sim, xi_vals_sim)\n",
        "        plt.title(r'LSS Correlation Function $\\xi$(r) (Dimensionless Units)')\n",
        "        plt.xlabel('r (Dimensionless Units)')\n",
        "        plt.ylabel(r'$\\xi$(r) (Dimensionless Units)') # Using raw string for LaTeX-like symbol\n",
        "        plt.grid(True, linestyle=':')\n",
        "        plt.axhline(0, color='black', linewidth=0.5)\n",
        "        if len(xi_vals_sim[1:]) > 0 and np.max(np.abs(xi_vals_sim[1:])) > 1e-10:\n",
        "            abs_max_xi_plot = np.max(np.abs(xi_vals_sim[1:]))\n",
        "            plt.ylim(max(-1.5 * abs_max_xi_plot, np.min(xi_vals_sim) - 0.1*abs_max_xi_plot ), \n",
        "                     min( 1.5 * abs_max_xi_plot, np.max(xi_vals_sim) + 0.1*abs_max_xi_plot ))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle(f\"EFM LSS Observables (Dimensionless, {config['run_id']})\", fontsize=14, y=1.02)\n",
        "        plot_filename_obs = os.path.join(data_path_lss, f\"lss_observables_{config['run_id']}.png\")\n",
        "        plt.savefig(plot_filename_obs)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # --- Identify and Print Emergent Dimensionless Scales ---\n",
        "        print(\"\\n--- Emergent Dimensionless Scales from Simulation ---\")\n",
        "        r_sim_peak = \"N/A\"\n",
        "        if len(xi_vals_sim) > 1 and np.any(np.abs(xi_vals_sim[1:]) > 1e-10):\n",
        "            # Find main peak in correlation function (excluding r=0 which is autocorr)\n",
        "            # Look for the first significant peak or a broad correlation\n",
        "            # A more robust method would involve smoothing or looking for features\n",
        "            # For now, just taking the max after initial spike, or a more sophisticated peak detection:\n",
        "            # To avoid picking noise at small r, let's look for a peak that's significantly above baseline noise\n",
        "            # Simple method: Find index of max value, excluding r=0. Then check if value is significant.\n",
        "            if np.max(xi_vals_sim[1:]) > 1e-10: # Check if there's any positive correlation\n",
        "                # Try to find a prominent peak using a simple approach (e.g., max value)\n",
        "                # A more sophisticated peak finding (e.g., scipy.signal.find_peaks) might be needed for subtle features.\n",
        "                max_val_idx = np.argmax(xi_vals_sim[1:]) + 1 # Index of max value after r=0\n",
        "                if xi_vals_sim[max_val_idx] > 3 * np.mean(xi_vals_sim[-10:]): # Simple check for prominence\n",
        "                    r_sim_peak = r_bins_sim[max_val_idx]\n",
        "                    print(f\"Main correlation peak in dimensionless units (r_sim_peak): {r_sim_peak:.3f}\")\n",
        "                else:\n",
        "                    print(\"No prominent correlation peak found in xi(r) beyond noise level.\")\n",
        "            else:\n",
        "                print(\"Correlation function is effectively zero or flat.\")\n",
        "        else:\n",
        "            print(\"Correlation function is effectively zero or flat.\")\n",
        "\n",
        "        k_sim_peak = \"N/A\"\n",
        "        if len(pk_vals_sim) > 0 and np.any(pk_vals_sim > 1e-10):\n",
        "            # Find main peak in power spectrum\n",
        "            k_sim_peak = k_bins_sim[np.argmax(pk_vals_sim)]\n",
        "            print(f\"Main power spectrum peak in dimensionless units (k_sim_peak): {k_sim_peak:.3f}\")\n",
        "            print(f\"Corresponding length scale (2π/k_sim_peak): {2*np.pi/k_sim_peak:.3f}\")\n",
        "        else:\n",
        "            print(\"Power spectrum is effectively zero.\")\n",
        "\n",
        "        print(\"\\n--- Physical Interpretation (based on EFM theoretical predictions) ---\")\n",
        "        # Assume EFM's primary LSS scale (628 Mpc) corresponds to the largest emergent scale from simulation\n",
        "        # This is where the physical world is 'mapped' to the dimensionless simulation.\n",
        "        EFM_PRIMARY_LSS_Mpc = 628.0 # Mpc [4]\n",
        "        EFM_SECONDARY_LSS_Mpc = 147.0 # Mpc (BAO-like) [4]\n",
        "\n",
        "        if r_sim_peak != \"N/A\" and isinstance(r_sim_peak, float): # Check if a peak was found and is a float\n",
        "            scaling_factor_L = EFM_PRIMARY_LSS_Mpc / r_sim_peak\n",
        "            print(f\"Scaling factor (1 dimensionless unit = X Mpc) derived from 628 Mpc primary scale: {scaling_factor_L:.2e} Mpc/unit\")\n",
        "            \n",
        "            # Physical constants for time scaling\n",
        "            c_si_m_s = 299792458.0 # m/s\n",
        "            Mpc_to_m = 3.08567758e22 # meters per Mpc\n",
        "            s_to_yr = 1.0 / (3.15576e7) # years per second\n",
        "\n",
        "            # Re-plot with physical units on x-axis\n",
        "            plt.figure(figsize=(16,6))\n",
        "            \n",
        "            # P(k) in physical units\n",
        "            plt.subplot(1,2,1)\n",
        "            # k_phys = k_sim / L_scale_factor; P_phys = P_sim * L_scale_factor^3\n",
        "            plt.loglog(k_bins_sim / scaling_factor_L, pk_vals_sim * scaling_factor_L**3) # P(k) ~ L^3\n",
        "            plt.title(r'LSS Power Spectrum P(k) (Physical Units)')\n",
        "            plt.xlabel(r'k (Mpc$^{-1}$)')\n",
        "            plt.ylabel(r'P(k) (Mpc$^3$)')\n",
        "            plt.grid(True, which='both', linestyle=':')\n",
        "            plt.axvline(2*np.pi/EFM_PRIMARY_LSS_Mpc, color='g', linestyle='--', label=f'EFM {EFM_PRIMARY_LSS_Mpc} Mpc (Primary)')\n",
        "            plt.axvline(2*np.pi/EFM_SECONDARY_LSS_Mpc, color='r', linestyle='--', label=f'EFM {EFM_SECONDARY_LSS_Mpc} Mpc (BAO-like)')\n",
        "            plt.legend()\n",
        "\n",
        "            # xi(r) in physical units\n",
        "            plt.subplot(1,2,2)\n",
        "            plt.plot(r_bins_sim * scaling_factor_L, xi_vals_sim)\n",
        "            plt.title(r'LSS Correlation Function $\\xi$(r) (Physical Units)')\n",
        "            plt.xlabel('r (Mpc)')\n",
        "            plt.ylabel(r'$\\xi$(r) (Dimensionless Units)') # ξ is dimensionless\n",
        "            plt.grid(True, linestyle=':')\n",
        "            plt.axhline(0, color='black', linewidth=0.5)\n",
        "            plt.axvline(EFM_PRIMARY_LSS_Mpc, color='g', linestyle='--', label=f'EFM {EFM_PRIMARY_LSS_Mpc} Mpc (Primary)')\n",
        "            plt.axvline(EFM_SECONDARY_LSS_Mpc, color='r', linestyle='--', label=f'EFM {EFM_SECONDARY_LSS_Mpc} Mpc (BAO-like)')\n",
        "            if len(xi_vals_sim[1:]) > 0 and np.max(np.abs(xi_vals_sim[1:])) > 1e-10: # Reuse y-limits if meaningful\n",
        "                 abs_max_xi_plot = np.max(np.abs(xi_vals_sim[1:]))\n",
        "                 plt.ylim(max(-1.5 * abs_max_xi_plot, np.min(xi_vals_sim) - 0.1*abs_max_xi_plot ), \n",
        "                         min( 1.5 * abs_max_xi_plot, np.max(xi_vals_sim) + 0.1*abs_max_xi_plot ))\n",
        "            plt.legend()\n",
        "\n",
        "            plt.tight_layout()\n",
        "            plt.suptitle(f\"EFM LSS Observables (Physical, {config['run_id']})\", fontsize=14, y=1.02)\n",
        "            plot_filename_obs_phys = os.path.join(data_path_lss, f\"lss_observables_physical_{config['run_id']}.png\")\n",
        "            plt.savefig(plot_filename_obs_phys)\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "            \n",
        "            # Approx physical time duration calculation\n",
        "            # 1 dimensionless length unit corresponds to scaling_factor_L Mpc\n",
        "            # 1 dimensionless time unit = (1 dimensionless length unit / c_sim_unit) = (scaling_factor_L * Mpc_to_m / c_si_m_s) in seconds\n",
        "            # Convert to Gyr:\n",
        "            approx_physical_time_per_sim_unit = (scaling_factor_L * Mpc_to_m / c_si_m_s) / s_to_yr # in years\n",
        "            approx_total_physical_time_gyr = time_sim_unit[-1] * approx_physical_time_per_sim_unit / 1e9\n",
        "\n",
        "            print(f\"Physical box size L: {config['L_sim_unit'] * scaling_factor_L:.2f} Mpc\")\n",
        "            print(f\"Physical dx: {config['dx_sim_unit'] * scaling_factor_L:.4g} Mpc\")\n",
        "            print(f\"Approx. total physical time simulated: {approx_total_physical_time_gyr:.4g} Gyr\")\n",
        "\n",
        "        else:\n",
        "            print(\"Could not determine a primary dimensionless peak for physical scaling (xi(r) peak was not found). Please examine raw P(k) and xi(r) plots for subtle features.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during plotting/analysis: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Run the simulation\n",
        "    # This single-process run leverages the full A100 GPU for simplicity.\n",
        "    if torch.cuda.is_available():\n",
        "        main_device = torch.device('cuda:0')\n",
        "    else:\n",
        "        main_device = torch.device('cpu')\n",
        "    \n",
        "    final_data_file = run_lss_simulation(config_lss_run, main_device)\n",
        "    \n",
        "    # Plot the results\n",
        "    if final_data_file:\n",
        "        plot_lss_results(final_data_file)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion-lss-dimless"
      },
      "source": [
        "## Conclusion and Future Work\n",
        "\n",
        "This notebook presents an updated simulation for EFM's Large-Scale Structure formation using a consistent dimensionless parameter set on an A100 GPU. The aim is to achieve numerical stability and observe the emergence of characteristic clustering scales (like 147 Mpc and 628 Mpc) as predicted by EFM theory.\n",
        "\n",
        "**Key Objectives and Expected Outcomes:**\n",
        "\n",
        "*   **Numerical Stability:** The dimensionless approach, combined with appropriate CFL factors, should ensure the simulation runs for the full duration without `NaN/Inf` errors. (Achieved!)\n",
        "*   **Emergent Clustering Scales:** We expect to see peaks in the Power Spectrum and Correlation Function corresponding to the 147 Mpc (BAO-like) and 628 Mpc (primary LSS) scales after physical scaling. This would provide strong computational validation for EFM's unique LSS predictions.\n",
        "*   **Dynamic Field Evolution:** The energy and density norm plots should demonstrate a stable or dynamically evolving field, consistent with the self-organization mechanisms of EFM. (Achieved, showing stable oscillations after initial transient!)\n",
        "\n",
        "**Analysis of Current Results and Remaining Challenges:**\n",
        "\n",
        "The current run (N=300, T=50000, dimensionless parameters) successfully completed and is numerically stable. The energy and density norm plots show initial transients settling into a stable, oscillatory state. However, the P(k) and ξ(r) plots do not yet show clear, distinct peaks corresponding to the 147 Mpc and 628 Mpc scales. This suggests that while the field is stable and evolving, the specific conditions for robust LSS *structure formation* (i.e., patterns that are distinguishable from initial noise or weak fluctuations) may not be fully met or require further refinement.\n",
        "\n",
        "Possible reasons for the lack of clear peaks:\n",
        "*   **Initial Noise vs. Seeded Perturbations:** Simple Gaussian noise might not be sufficient to reliably seed the large-scale structures expected. Some LSS simulations use specific low-amplitude, long-wavelength sinusoidal modes as initial conditions to promote structure growth.\n",
        "*   **Simulation Time:** Even 50,000 steps might not be long enough for *detectable* large-scale structures to fully coalesce and amplify on a 300³ grid given the chosen interaction strengths.\n",
        "*   **Parameter Fine-tuning:** While the parameters are from the paper, the precise interplay of `g`, `k_gravity`, `alpha`, and `delta` might need more fine-tuning for optimal structure emergence. For example, the `g` value might need to be higher to drive stronger non-linear clumping, or `alpha`/`delta` might be damping too much.\n",
        "*   **Small Initial Amplitude:** `1.0e-6` might be too small to sufficiently perturb the field to initiate strong structure formation without excessive runtime.\n",
        "\n",
        "**Future Work and Next Steps (Leveraging HPC & Grounding):**\n",
        "\n",
        "1.  **Iterative Parameter Adjustment & Longer Runs (HPC):** Given numerical stability, the next step is to continue iterating on the dimensionless parameters (`g_sim`, `k_efm_gravity_coupling`, `alpha_sim`, `delta_sim`, `initial_noise_amplitude`) and significantly increasing `T_steps` (e.g., to 100,000 or more) on the H100 cluster. We need to find the sweet spot where these terms lead to visible large-scale patterns in the field, not just stable oscillations. This would involve a parameter sweep if possible.\n",
        "2.  **Seeding Initial Perturbations:** Experiment with different forms of initial conditions. Instead of pure noise, consider adding small, long-wavelength sinusoidal perturbations (e.g., `φ_initial = A * (sin(k_x x) + sin(k_y y) + sin(k_z z))`) at the target dimensionless scales that would map to 147 Mpc and 628 Mpc.\n",
        "3.  **Visualizing Field Slices:** Implement a function to save 2D slices or 3D renderings of the `phi_final_cpu` field to visually inspect for any emergent filamentary or clustering patterns, even if weak.\n",
        "4.  **Unit System Derivation:** The need for a concrete derivation of the fundamental unit scaling within EFM from first principles (e.g., `s*t=k`, HDS density levels) remains crucial to rigorously map dimensionless results to physical Mpc/Gyr. This is a separate theoretical task that complements the simulations.\n",
        "5.  **Integration with other EFM Cosmological Aspects:** Combine the LSS simulation with the inflationary analogue and Hubble tension resolution mechanisms into a more comprehensive end-to-end EFM cosmological simulation.\n",
        "\n",
        "This systematic approach, grounded in EFM's dimensionless framework, will build a robust, falsifiable, and unified cosmological model, providing a deterministic alternative to current paradigms. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "references-lss"
      },
      "source": [
        "## References\n",
        "\n",
        "[1] Emvula, T. 2025a, 'Introducing the Ehokolo Fluxon Model: A Scalar Motion Framework for the Physical Universe' (Independent Frontier Science Collaboration, April 2025).\n",
        "[2] Emvula, T. 2025b, 'The Ehokolo Fluxon Model: A Foundation for Physics from Eholokon Dynamics' (Independent Frontier Science Collaboration, April 2025).\n",
        "[3] Emvula, T. 2025c, 'Ehokolo Fluxon Model: Mass Generation via Ehokolon Self-Interactions' (Independent Frontier Science Collaboration, March 2025).\n",
        "[4] Emvula, T. 2025d, 'Ehokolo Fluxon Model: Unifying Cosmic Structure, Non-Gaussianity, and Gravitational Waves Across Scales' (Independent Frontier Science Collaboration, April 2025). (This paper is the primary reference for LSS parameters)\n",
        "[5] Emvula, T. 2025e, 'Fluxonic Zero-Point Energy and Emergent Gravity: A Deterministic Alternative to Spacetime Curvature in the Ehokolo Fluxon Model' (Independent Frontier Science Collaboration, February 2025).\n",
        "[6] Emvula, T. 2025f, 'Fluxonic Cosmology: Inflation, Expansion, and Structure from EFM Harmonic States' (Independent Frontier Science Collaboration, April 2025).\n",
        "[7] Emvula, T. 2025g, 'Ehokolon Harmonic Density States: Foundational Validation and Unified Physics in the Ehokolo Fluxon Model' (Independent Frontier Science Collaboration, April 2025).\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}