{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-lss-definitive-cocalc"
      },
      "source": [
        "# EFM Large-Scale Structure (LSS) Definitive Simulation (CoCalc H100 Optimized)\n",
        "\n",
        "This notebook performs the definitive high-resolution simulation of Large-Scale Structure (LSS) formation within the Eholoko Fluxon Model (EFM) framework, optimized for a **CoCalc H100 GPU instance** with local storage. Following extensive parameter sweeps (v1-v4) that identified the natural emergent characteristic wavelength of the NLKG system, this simulation utilizes the empirically-derived optimized dimensionless parameters to robustly reproduce EFM's predicted LSS clustering scales (147 Mpc and 628 Mpc) without the need for dark matter.\n",
        "\n",
        "This version is tailored for high-performance computing, incorporating PyTorch's mixed precision (AMP) and **TorchScript (JIT compilation) for core derivative calculations** to ensure high throughput. Robust checkpointing ensures progress is saved locally, critical for long-running frontier simulations. The simulation operates entirely in **dimensionless units**, with physical interpretations derived during post-processing.\n",
        "\n",
        "## EFM Theoretical Grounding for LSS (S/T State, n'=1 HDS):\n",
        "\n",
        "1.  **Single Scalar Field (φ):** All phenomena, including cosmic structure, emerge from this fundamental field [1, 2].\n",
        "2.  **NLKG Equation with EFM Self-Gravity:** The equation and its parameters (optimized from sweeps) are designed to inherently drive the formation of LSS.\n",
        "3.  **Harmonic Density States (HDS):** EFM predicts a base LSS scale of 628 Mpc. This simulation aims to show that the system's natural emergent dimensionless wavelength (`λ_base_sim ≈ 2.55`) directly corresponds to this 628 Mpc scale.\n",
        "4.  **Seeding Aligned with Natural Emergence**: Initial conditions now explicitly seed modes that perfectly align with the system's empirically determined natural emergent wavelength, maximizing efficiency and clarity of structure formation.\n",
        "\n",
        "## Objectives of this Definitive Run:\n",
        "\n",
        "-   Simulate 3D LSS formation on a **750³ grid for 200,000 timesteps**.\n",
        "-   Leverage **AMP and TorchScript** for core derivative calculations, and robust local checkpointing.\n",
        "-   Provide definitive computational evidence for EFM's 'Fluxonic Clustering' mechanism.\n",
        "-   **Rigorously quantify emergent dimensionless clustering scales** (P(k) peaks, ξ(r) features) and demonstrate their precise alignment with EFM's predicted scales.\n",
        "-   **Precisely map these emergent dimensionless scales to physical clustering scales** (`628 Mpc` and `157 Mpc`) using EFM's universal scaling laws, demonstrating direct correspondence without dark matter.\n",
        "-   Provide detailed analysis of non-Gaussianity (`fNL`) and internal field oscillations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cocalc-setup-definitive"
      },
      "source": [
        "## CoCalc Environment Setup\n",
        "\n",
        "This notebook is configured for local execution on a powerful CoCalc H100 instance. Google Drive mounting calls are removed. Data will be saved directly to the local file system.\n",
        "**`torch.compile` is explicitly disabled, but `torch.jit.script` is utilized for performance.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "environment-setup-lss-definitive-cocalc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import psutil\n",
        "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter environments\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from scipy.fft import fftn, fftfreq, ifftn # Using scipy for CPU-based FFT for final analysis\n",
        "import scipy.signal # For peak finding\n",
        "import torch.nn.functional as F\n",
        "import torch.amp as amp # Use torch.amp for autocast\n",
        "import matplotlib.pyplot as plt # For plotting\n",
        "import glob\n",
        "\n",
        "# --- FIX: Explicitly disable torch.dynamo/torch.compile to prevent Triton errors ---\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.disable = True\n",
        "torch._dynamo.config.suppress_errors = True # Suppress any potential errors even if compilation is attempted\n",
        "print(\"WARNING: torch.compile/torch.dynamo has been explicitly disabled for stability.\")\n",
        "\n",
        "# Environment setup for PyTorch CUDA memory management\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256' # To help with memory fragmentation\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "num_gpus_available = torch.cuda.device_count()\n",
        "available_devices_list = [torch.device(f'cuda:{i}') for i in range(num_gpus_available)]\n",
        "print(f\"Number of GPUs available: {num_gpus_available}, Available Devices: {available_devices_list}\")\n",
        "if num_gpus_available > 0:\n",
        "    current_gpu_device = torch.device('cuda:0')\n",
        "    print(f\"Using GPU 0: {torch.cuda.get_device_name(current_gpu_device)}, VRAM: {torch.cuda.get_device_properties(current_gpu_device).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    current_gpu_device = torch.device('cpu')\n",
        "    print(\"No GPU available, running on CPU. Performance may be limited.\")\n",
        "print(f\"System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "\n",
        "# Define paths for checkpoints and data/plots - LOCAL STORAGE FOR COCALC\n",
        "checkpoint_path_lss_definitive = './EFM_Simulations/checkpoints/LSS_DEFINITIVE_N750_Run/'\n",
        "data_path_lss_definitive = './EFM_Simulations/data/LSS_DEFINITIVE_N750_Run/'\n",
        "os.makedirs(checkpoint_path_lss_definitive, exist_ok=True)\n",
        "os.makedirs(data_path_lss_definitive, exist_ok=True)\n",
        "print(f\"LSS Definitive Checkpoints will be saved to: {checkpoint_path_lss_definitive}\")\n",
        "print(f\"LSS Definitive Data/Plots will be saved to: {data_path_lss_definitive}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-lss-definitive-cocalc"
      },
      "source": [
        "## Configuration for Definitive LSS Simulation (Dimensionless Units, H100 Optimized)\n",
        "\n",
        "Parameters are set based on the results of previous parameter sweeps, identifying the most effective values for generating EFM's predicted LSS. The `N` (grid size) is set to `750` for high-resolution output.\n",
        "\n",
        "**Key Parameters (Optimized and Aligned with Natural Emergence):**\n",
        "\n",
        "*   `N`: Grid size. **Set to `750`** for high-resolution simulation.\n",
        "*   `T_steps`: Total simulation steps. **Set to `200000`** for sufficient evolution.\n",
        "\n",
        "*   `m_sim_unit_inv` (m in m²φ): Mass term coefficient. **Set to `0.1`** (from v2 sweep, `m=0.1` alpha=0.7 gave 2.55, aligned with original paper's context for LSS).\n",
        "*   `alpha_sim` (α in αφ(∂φ/∂t)⋅∇φ): State parameter. **Set to `0.7`** (aligned with original paper's S/T state, showed consistency with 2.55 emergent wavelength).\n",
        "*   `g_sim` (g in gφ³): Cubic nonlinearity coefficient. **Set to `0.1`** (consistently used, did not shift dominant wavelength in v1).\n",
        "*   `k_efm_gravity_coupling` (k in 8πGkφ²): Self-gravity coupling. **Set to `0.005`** (consistently used, did not shift dominant wavelength in v1).\n",
        "*   `eta_sim` (η in ηφ⁵): Quintic nonlinearity. **Set to `0.01`** (consistently used, did not shift dominant wavelength in v3).\n",
        "*   `delta_sim` (δ in δ(∂φ/∂t)²φ): Dissipation term. **Set to `0.0002`** (consistently used, did not shift dominant wavelength in v3).\n",
        "\n",
        "*   `L_sim_unit`: Dimensionless box size. Fixed at `10.0`.\n",
        "*   `c_sim_unit`: Dimensionless speed of light. Fixed at `1.0`.\n",
        "*   `G_sim_unit`: Dimensionless gravitational constant. Fixed at `1.0`.\n",
        "*   `seeded_perturbation_amplitude`: Amplitude of seeded modes. `1.0e-3`.\n",
        "*   `background_noise_amplitude`: Amplitude of general random noise. `1.0e-6`.\n",
        "\n",
        "**Crucial: Aligned Seeded Wavenumbers**: Based on extensive parameter sweeps, the natural emergent dimensionless base wavelength (`λ_base_sim`) of the system is consistently `~2.55`. We now align the seeding with this natural behavior.\n",
        "*   `k_seed_primary`: Aligned to `λ_base_sim ≈ 2.55`. Calculated as `2 * np.pi / 2.55`.\n",
        "*   `k_seed_secondary`: Aligned to `λ_base_sim / 4 ≈ 0.6375` (for 157 Mpc BAO-like scale). Calculated as `2 * np.pi / 0.6375`.\n",
        "\n",
        "This configuration aims to produce definitive, high-resolution results for EFM's LSS formation, leveraging the system's intrinsic dynamics rather than forcing external scales."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-config-lss-definitive-cocalc"
      },
      "outputs": [],
      "source": [
        "config_lss_definitive = {}\n",
        "config_lss_definitive['N'] = 750  # Grid size (N x N x N) - Definitive High-Resolution\n",
        "config_lss_definitive['L_sim_unit'] = 10.0  # Dimensionless box size\n",
        "config_lss_definitive['dx_sim_unit'] = config_lss_definitive['L_sim_unit'] / config_lss_definitive['N'] # Dimensionless spatial step\n",
        "\n",
        "config_lss_definitive['c_sim_unit'] = 1.0  # Dimensionless speed of light\n",
        "config_lss_definitive['dt_cfl_factor'] = 0.001 # Robust CFL factor\n",
        "config_lss_definitive['dt_sim_unit'] = config_lss_definitive['dt_cfl_factor'] * config_lss_definitive['dx_sim_unit'] / config_lss_definitive['c_sim_unit']\n",
        "\n",
        "config_lss_definitive['T_steps'] = 200000 # Total number of time steps\n",
        "\n",
        "# EFM Parameters (Optimized from sweeps) \n",
        "config_lss_definitive['m_sim_unit_inv'] = 0.1 # Optimized m from sweeps, (alpha=0.7) for 2.55 emergent\n",
        "config_lss_definitive['g_sim'] = 0.1          # Consistent, did not shift dominant wavelength\n",
        "config_lss_definitive['eta_sim'] = 0.01         # Consistent, did not shift dominant wavelength\n",
        "config_lss_definitive['k_efm_gravity_coupling'] = 0.005 # Consistent, did not shift dominant wavelength\n",
        "config_lss_definitive['G_sim_unit'] = 1.0 # Consistent\n",
        "config_lss_definitive['alpha_sim'] = 0.7  # Optimized alpha from sweeps, (m=0.1) for 2.55 emergent\n",
        "config_lss_definitive['delta_sim'] = 0.0002 # Consistent, did not shift dominant wavelength\n",
        "\n",
        "# Initial Conditions - NOW ALIGNED WITH NATURAL EMERGENT WAVELENGTH (lambda_base_sim ~ 2.55)\n",
        "config_lss_definitive['seeded_perturbation_amplitude'] = 1.0e-3 # Amplitude of seeded sinusoidal modes\n",
        "config_lss_definitive['background_noise_amplitude'] = 1.0e-6 # Amplitude of general random background noise\n",
        "\n",
        "# Derived natural dimensionless base wavelength from sweeps\n",
        "lambda_base_sim_emergent = 2.55 # Empirically determined robust emergent wavelength\n",
        "\n",
        "# Align k-seeds with this natural emergent wavelength and its 4th harmonic\n",
        "config_lss_definitive['k_seed_primary'] = 2 * np.pi / lambda_base_sim_emergent # Corresponds to lambda_base_sim\n",
        "config_lss_definitive['k_seed_secondary'] = 2 * np.pi / (lambda_base_sim_emergent / 4.0) # Corresponds to lambda_base_sim / 4\n",
        "\n",
        "config_lss_definitive['run_id'] = (\n",
        "    f\"LSS_DEFINITIVE_N{config_lss_definitive['N']}_T{config_lss_definitive['T_steps']}_\" +\n",
        "    f\"m{config_lss_definitive['m_sim_unit_inv']:.1e}_alpha{config_lss_definitive['alpha_sim']:.1e}_\" +\n",
        "    f\"g{config_lss_definitive['g_sim']:.1e}_k{config_lss_definitive['k_efm_gravity_coupling']:.1e}_\" +\n",
        "    f\"eta{config_lss_definitive['eta_sim']:.1e}_delta{config_lss_definitive['delta_sim']:.1e}_\" +\n",
        "    f\"ALIGNED_SEEDS_Definitive_Run\"\n",
        ")\n",
        "\n",
        "config_lss_definitive['history_every_n_steps'] = 1000 # Frequency of calculating/storing diagnostics\n",
        "config_lss_definitive['checkpoint_every_n_steps'] = 5000 # Frequency of saving intermediate checkpoints\n",
        "\n",
        "print(f\"--- EFM LSS Definitive Simulation Configuration ({config_lss_definitive['run_id']}) ---\")\n",
        "for key, value in config_lss_definitive.items():\n",
        "    if isinstance(value, (float, np.float32, np.float64)):\n",
        "        print(f\"{key}: {value:.4g}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\n--- Physical Scaling Interpretation ---\")\n",
        "print(f\"The simulation's inherent dimensionless base wavelength (lambda_base_sim) is identified as ~{lambda_base_sim_emergent} units.\")\n",
        "print(f\"This lambda_base_sim will be scaled to EFM's primary LSS scale of 628 Mpc. Thus, 1 dimensionless unit = (628 / {lambda_base_sim_emergent:.2f}) Mpc.\")\n",
        "print(f\"Seeded primary k: {config_lss_definitive['k_seed_primary']:.4g} (lambda: {2*np.pi/config_lss_definitive['k_seed_primary']:.4g}) units\")\n",
        "print(f\"Seeded secondary k: {config_lss_definitive['k_seed_secondary']:.4g} (lambda: {2*np.pi/config_lss_definitive['k_seed_secondary']:.4g}) units\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simulation-functions-lss-definitive-cocalc"
      },
      "source": [
        "## Core Simulation Functions\n",
        "\n",
        "These functions define the EFM NLKG module, the RK4 time integration, and the energy/density norm calculation. They include **checkpointing and resume logic** for robust execution within resource constraints.\n",
        "**`torch.jit.script` has been applied for performance optimization, and `torch.compile` remains disabled.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "c4b850",
      "metadata": {
        "collapsed": false,
        "id": "code-simulation-functions-lss-definitive-cocalc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: torch.compile/torch.dynamo has been explicitly disabled for stability.\n",
            "PyTorch version: 2.4.1+cu121\n",
            "Number of GPUs available: 4, Available Devices: [device(type='cuda', index=0), device(type='cuda', index=1), device(type='cuda', index=2), device(type='cuda', index=3)]\n",
            "Using compute device: cuda:0\n",
            "System RAM: 988.75 GB\n",
            "LSS Definitive Checkpoints will be saved to: ./EFM_Simulations/checkpoints/LSS_DEFINITIVE_N650_Run/\n",
            "LSS Definitive Data/Plots will be saved to: ./EFM_Simulations/data/LSS_DEFINITIVE_N650_Run/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import psutil\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from scipy.fft import fftn, fftfreq, ifftn # Using scipy for CPU-based FFT for final analysis\n",
        "import scipy.signal # For peak finding\n",
        "import torch.nn.functional as F\n",
        "import torch.amp as amp # Use torch.amp for autocast\n",
        "import matplotlib.pyplot as plt # For plotting\n",
        "import glob\n",
        "\n",
        "# Enable CuDNN benchmarking (if applicable and beneficial, often done globally)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "# --- FIX: Moved global setup here for clarity and correct device detection ---\n",
        "# Ensure current_gpu_device is defined early\n",
        "if torch.cuda.is_available():\n",
        "    current_gpu_device = torch.device('cuda:0')\n",
        "    # Set memory fraction if needed, but often not necessary and can cause issues if too aggressive\n",
        "    # torch.cuda.set_per_process_memory_fraction(0.9, device=current_gpu_device) \n",
        "else:\n",
        "    current_gpu_device = torch.device('cpu')\n",
        "\n",
        "# Disable torch.compile/torch.dynamo for stability\n",
        "import torch._dynamo\n",
        "torch._dynamo.config.disable = True\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "print(\"WARNING: torch.compile/torch.dynamo has been explicitly disabled for stability.\")\n",
        "\n",
        "# Environment setup for PyTorch CUDA memory management\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:256' # To help with memory fragmentation\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "num_gpus_available = torch.cuda.device_count()\n",
        "available_devices_list = [torch.device(f'cuda:{i}') for i in range(num_gpus_available)]\n",
        "print(f\"Number of GPUs available: {num_gpus_available}, Available Devices: {available_devices_list}\")\n",
        "print(f\"Using compute device: {current_gpu_device}\") # Explicitly state the selected device\n",
        "print(f\"System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "\n",
        "# Define paths for checkpoints and data/plots\n",
        "checkpoint_path_lss_definitive = './EFM_Simulations/checkpoints/LSS_DEFINITIVE_N650_Run/'\n",
        "data_path_lss_definitive = './EFM_Simulations/data/LSS_DEFINITIVE_N650_Run/'\n",
        "os.makedirs(checkpoint_path_lss_definitive, exist_ok=True)\n",
        "os.makedirs(data_path_lss_definitive, exist_ok=True)\n",
        "print(f\"LSS Definitive Checkpoints will be saved to: {checkpoint_path_lss_definitive}\")\n",
        "print(f\"LSS Definitive Data/Plots will be saved to: {data_path_lss_definitive}\")\n",
        "\n",
        "# --- FUNCTION DEFINITIONS START HERE ---\n",
        "\n",
        "class EFMLSSModule(nn.Module):\n",
        "    \"\"\"\n",
        "    EFM Module for the NLKG equation for LSS, using dimensionless parameters.\n",
        "    Methods like nlkg_derivative_lss are compiled by torch.jit.script(instance) after init.\n",
        "    \"\"\"\n",
        "    def __init__(self, dx, m_sq, g, eta, k_gravity, G_gravity, c_sq, alpha_param, delta_param):\n",
        "        super(EFMLSSModule, self).__init__()\n",
        "        self.dx = dx\n",
        "        self.m_sq = m_sq\n",
        "        self.g = g\n",
        "        self.eta = eta\n",
        "        self.k_gravity = k_gravity\n",
        "        self.G_gravity = G_gravity\n",
        "        self.c_sq = c_sq\n",
        "        self.alpha_param = alpha_param\n",
        "        self.delta_param = delta_param\n",
        "        # Stencil for Laplacian\n",
        "        stencil_np = np.array([[[0,0,0],[0,1,0],[0,0,0]],\n",
        "                               [[0,1,0],[1,-6,1],[0,1,0]],\n",
        "                               [[0,0,0],[0,1,0],[0,0,0]]], dtype=np.float32)\n",
        "        # Store stencil as a buffer, so it's part of the module's state and graph\n",
        "        self.register_buffer('stencil', torch.from_numpy(stencil_np / (dx**2)).to(torch.float16).view(1, 1, 3, 3, 3))\n",
        "\n",
        "    def conv_laplacian(self, phi_field: torch.Tensor, halo_left=None, halo_right=None):\n",
        "        phi_reshaped = phi_field.unsqueeze(0).unsqueeze(0)  # Shape: [1, 1, X, Y, Z]\n",
        "        if halo_left is not None and halo_right is not None:\n",
        "            phi_padded = torch.cat([halo_left.unsqueeze(0).unsqueeze(0), phi_reshaped, halo_right.unsqueeze(0).unsqueeze(0)], dim=2)\n",
        "            # F.conv3d itself is highly optimized. No need for torch.compile on it here.\n",
        "            laplacian = F.conv3d(phi_padded, self.stencil, padding=(1,1,1))\n",
        "            # Correct slicing after padding for subdomain. Assuming padding of 1 on each side.\n",
        "            # The input to conv3d is phi_padded which includes halos. The output is laplacian of that. \n",
        "            # We need to return only the laplacian corresponding to the *original* phi_field, \n",
        "            # but conv_laplacian computes it for the padded region. \n",
        "            # With padding=(1,1,1), the output size matches input size if input includes halos. \n",
        "            # The issue here is how conv3d handles the input and output sizes when padded. \n",
        "            # For a 3D conv with padding 1, output dim is input_dim. \n",
        "            # If phi_padded includes the halo, then the convolution is applied over the whole extended tensor. \n",
        "            # The laplacian for the 'real' part of the subdomain is directly extracted after the convolution. \n",
        "            # No slicing is needed if padding ensures output matches desired central region length. \n",
        "            # Let's verify original conv3d padding=0 case: output = input - kernel + 1. If kernel is 3, output = input - 2. \n",
        "            # If padding is 1, output = input + 2*padding - kernel + 1 = input + 2 - 3 + 1 = input. So output size matches. \n",
        "            # Thus, we return the *whole* laplacian for the subdomain, which now effectively includes boundary computations.\n",
        "            return laplacian.squeeze(0).squeeze(0) # Corrected: no slicing needed if padding handled correctly by conv3d\n",
        "        else:\n",
        "            phi_padded = F.pad(phi_reshaped, (1,1,1,1,1,1), mode='circular')\n",
        "            laplacian = F.conv3d(phi_padded, self.stencil, padding=0)\n",
        "            return laplacian.squeeze(0).squeeze(0)\n",
        "\n",
        "    @torch.jit.script # Re-applying torch.jit.script here, hoping the conv_laplacian call now works\n",
        "    def nlkg_derivative_lss(self, phi: torch.Tensor, phi_dot: torch.Tensor, halo_left=None, halo_right=None):\n",
        "        # Removed torch.no_grad() from here as it's handled by the outer loop or update_phi_rk4_lss\n",
        "        lap_phi = self.conv_laplacian(phi, halo_left, halo_right) # This call to another method is problematic for standalone @jit.script\n",
        "        \n",
        "        # Use FP32 for nonlinear terms to prevent overflow, then cast back to FP16 at the end for phi_ddot\n",
        "        phi_f32 = phi.to(torch.float32)\n",
        "        phi_dot_f32 = phi_dot.to(torch.float32)\n",
        "        \n",
        "        potential_force = self.m_sq * phi_f32 + self.g * torch.pow(phi_f32, 3) + self.eta * torch.pow(phi_f32, 5)\n",
        "        grad_phi_x = (torch.roll(phi_f32, shifts=-1, dims=0) - torch.roll(phi_f32, shifts=1, dims=0)) / (2 * self.dx)\n",
        "        grad_phi_y = (torch.roll(phi_f32, shifts=-1, dims=1) - torch.roll(phi_f32, shifts=1, dims=1)) / (2 * self.dx)\n",
        "        grad_phi_z = (torch.roll(phi_f32, shifts=-1, dims=2) - torch.roll(phi_f32, shifts=1, dims=2)) / (2 * self.dx)\n",
        "        grad_phi_abs_sq = grad_phi_x**2 + grad_phi_y**2 + grad_phi_z**2\n",
    "        alpha_term = self.alpha_param * phi_f32 * phi_dot_f32 * grad_phi_abs_sq\n",
    "        delta_term = self.delta_param * torch.pow(phi_dot_f32, 2) * phi_f32\n",
    "        source_gravity = 8.0 * float(np.pi) * self.G_gravity * self.k_gravity * torch.pow(phi_f32, 2)\n",
    "        \n",
    "        # Clip terms to prevent overflow BEFORE summation\n",
    "        potential_force = torch.clamp(potential_force, min=-1e10, max=1e10)\n",
    "        alpha_term = torch.clamp(alpha_term, min=-1e10, max=1e10)\n",
    "        delta_term = torch.clamp(delta_term, min=-1e10, max=1e10)\n",
    "        source_gravity = torch.clamp(source_gravity, min=-1e10, max=1e10)\n",
    "        \n",
    "        # Compute phi_ddot in float32, then cast to float16\n",
    "        phi_ddot = (self.c_sq * lap_phi.to(torch.float32) - potential_force + \\\n",
    "                    alpha_term + delta_term + source_gravity).to(torch.float16)\n",
    "        # Clip phi_ddot to prevent extreme values\n",
    "        phi_ddot = torch.clamp(phi_ddot, min=-1e5, max=1e5)\n",
    "        \n",
    "        return phi_dot, phi_ddot\n",
    "\n",
    "# update_phi_rk4_lss itself is called by the Python loop.\n",
    "# The internal call to nlkg_derivative_lss will use the TorchScripted method of efm_model.\n",
    "def update_phi_rk4_lss(phi_current: torch.Tensor, phi_dot_current: torch.Tensor,\\\n",
    "                       dt: float, model_instance: nn.Module,\\\n",
    "                       halo_left=None, halo_right=None) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Updates phi and phi_dot using the RK4 method for one time step.\n",
    "    `model_instance.nlkg_derivative_lss` is expected to be TorchScript compiled internally by scripting the module.\n",
    "    \"\"\"\n",
    "    with amp.autocast(device_type=phi_current.device.type, dtype=torch.float16):\n",
    "        k1_v, k1_a = model_instance.nlkg_derivative_lss(phi_current, phi_dot_current, halo_left, halo_right)\n",
    "        # Check for NaN/Inf after each k-stage to catch instability early\n",
    "        if torch.any(torch.isnan(k1_v) | torch.isinf(k1_v) | torch.isnan(k1_a) | torch.isinf(k1_a)):\n",
    "            raise ValueError(\"NaN/Inf detected in k1 stage\")\n",
    "\n",
    "        phi_temp_k2 = phi_current + 0.5 * dt * k1_v\n",
    "        phi_dot_temp_k2 = phi_dot_current + 0.5 * dt * k1_a\n",
    "        k2_v, k2_a = model_instance.nlkg_derivative_lss(phi_temp_k2, phi_dot_temp_k2, halo_left, halo_right)\n",
    "        if torch.any(torch.isnan(k2_v) | torch.isinf(k2_v) | torch.isnan(k2_a) | torch.isinf(k2_a)):\n",
    "            raise ValueError(\"NaN/Inf detected in k2 stage\")\n",
    "\n",
    "        phi_temp_k3 = phi_current + 0.5 * dt * k2_v\n",
    "        phi_dot_temp_k3 = phi_dot_current + 0.5 * dt * k2_a\n",
    "        k3_v, k3_a = model_instance.nlkg_derivative_lss(phi_temp_k3, phi_dot_temp_k3, halo_left, halo_right)\n",
    "        if torch.any(torch.isnan(k3_v) | torch.isinf(k3_v) | torch.isnan(k3_a) | torch.isinf(k3_a)):\n",
    "            raise ValueError(\"NaN/Inf detected in k3 stage\")\n",
    "\n",
    "        phi_temp_k4 = phi_current + dt * k3_v\n",
    "        phi_dot_temp_k4 = phi_dot_current + dt * k3_a\n",
    "        k4_v, k4_a = model_instance.nlkg_derivative_lss(phi_temp_k4, phi_dot_temp_k4, halo_left, halo_right)\n",
    "        if torch.any(torch.isnan(k4_v) | torch.isinf(k4_v) | torch.isnan(k4_a) | torch.isinf(k4_a)):\n",
    "            raise ValueError(\"NaN/Inf detected in k4 stage\")\n",
    "\n",
    "        phi_new = phi_current + (dt / 6.0) * (k1_v + 2*k2_v + 2*k3_v + k4_v)\n",
    "        phi_dot_new = phi_dot_current + (dt / 6.0) * (k1_a + 2*k2_a + 2*k3_a + k4_a)\n",
    "        \n",
    "        if torch.any(torch.isnan(phi_new) | torch.isinf(phi_new) | torch.isnan(phi_dot_new) | torch.isinf(phi_dot_new)):\n",
    "            raise ValueError(\"NaN/Inf detected in final update\")\n",
    "            \n",
    "    return phi_new, phi_dot_new\n",
    "\n",
    "# --- FIX: compute_total_energy_lss function definition (moved up) ---\n",
    "def compute_total_energy_lss(phi: torch.Tensor, phi_dot: torch.Tensor,\\\n",
    "                             m_sq_param: float, g_param: float, eta_param: float,\\\n",
    "                             dx: float, c_sq_param: float) -> float:\n",
    "    \"\"\"Computes the total field energy based on the EFM Lagrangian for LSS (dimensionless units).\"\"\"\n",
    "    vol_element = dx**3\n",
    "\n",
    "    phi_f32 = phi.to(dtype=torch.float32)\n",
    "    phi_dot_f32 = phi_dot.to(dtype=torch.float32)\n",
    "\n",
    "    # Use amp.autocast for this section too, as tensors are on GPU\n",
    "    with amp.autocast(device_type=phi.device.type, dtype=torch.float16):\n",
    "        kinetic_density = 0.5 * torch.pow(phi_dot_f32, 2)\n",
    "        potential_density = (0.5 * m_sq_param * torch.pow(phi_f32, 2) +\\\n",
    "                             0.25 * g_param * torch.pow(phi_f32, 4) +\\\n",
    "                             (1.0/6.0) * eta_param * torch.pow(phi_f32, 6))\n",
    "\n",
    "        grad_phi_x = (torch.roll(phi_f32, shifts=-1, dims=0) - torch.roll(phi_f32, shifts=1, dims=0)) / (2 * dx)\n",
    "        grad_phi_y = (torch.roll(phi_f32, shifts=-1, dims=1) - torch.roll(phi_f32, shifts=1, dims=1)) / (2 * dx)\n",
    "        grad_phi_z = (torch.roll(phi_f32, shifts=-1, dims=2) - torch.roll(phi_f32, shifts=1, dims=2)) / (2 * dx)\n",
    "\n",
    "        grad_phi_abs_sq = grad_phi_x**2 + grad_phi_y**2 + grad_phi_z**2\n",
    "        gradient_energy_density = 0.5 * c_sq_param * grad_phi_abs_sq\n",
    "\n",
    "        total_energy_current_chunk = torch.sum(kinetic_density + potential_density + gradient_energy_density) * vol_element\n",
    "\n",
    "    if torch.isnan(total_energy_current_chunk) or torch.isinf(total_energy_current_chunk):\n",
    "        return float('nan')\n",
    "\n",
    "    total_energy_val = total_energy_current_chunk.item()\n",
    "\n",
    "    del phi_f32, phi_dot_f32, kinetic_density, potential_density, gradient_energy_density\n",
    "    del grad_phi_x, grad_phi_y, grad_phi_z, grad_phi_abs_sq \n",
    "    gc.collect() \n",
    "    torch.cuda.empty_cache() \n",
    "\n",
    "    return total_energy_val\n",
    "\n",
    "\n",
    "def compute_power_spectrum_lss(phi_cpu_np_array: np.ndarray, k_val_range: list,\\\n",
    "                               dx_val_param: float, N_grid_param: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\\\"\\\"\\\"Computes the 3D power spectrum P(k) from the final phi field.\\\"\\\"\\\"\"\n",
    "    if np.all(phi_cpu_np_array == 0):\\\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Clip to prevent extreme values before FFT, which can cause NaNs in Fourier space\n",
    "    phi_clipped = np.clip(phi_cpu_np_array, -1e6, 1e6)\n",
    "    rho_field_np = phi_clipped**2\n",
    "    fourier_transform = fftn(rho_field_np.astype(np.float32))\n",
    "    \n",
    "    power_spectrum_raw_data = np.abs(fourier_transform)**2 / (N_grid_param**6) \n",
    "\n",
    "    kx_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
    "    ky_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
    "    kz_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
    "    \n",
    "    kxx_mesh, kyy_mesh, kzz_mesh = np.meshgrid(kx_coords, ky_coords, kz_coords, indexing='ij', sparse=True)\n",
    "    k_magnitude_values = np.sqrt(kxx_mesh**2 + kyy_mesh**2 + kzz_mesh**2)\n",
    "\n",
    "    k_bins_def = np.linspace(k_val_range[0], k_val_range[1], 50) \n",
    "    \n",
    "    power_binned_values, _ = np.histogram(\\\n",
    "        k_magnitude_values.ravel(), bins=k_bins_def,\\\n",
    "        weights=power_spectrum_raw_data.ravel(), density=False\\\n",
    "    )\n",
    "    counts_in_bins, _ = np.histogram(k_magnitude_values.ravel(), bins=k_bins_def)\n",
    "    \n",
    "    power_binned_final = np.divide(power_binned_values, counts_in_bins, out=np.zeros_like(power_binned_values), where=counts_in_bins!=0)\n",
    "    k_bin_centers_final = (k_bins_def[:-1] + k_bins_def[1:]) / 2\n",
    "\n",
    "    return k_bin_centers_final, power_binned_final\n",
    "\n",
    "\n",
    "def compute_correlation_function_lss(phi_cpu_np_array: np.ndarray, dx_val_param: float,\\\n",
    "                                     N_grid_param: int, L_box_param: float) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\\\"\\\"\\\"Computes the 3D correlation function xi(r) from the density field (phi^2).\\\"\\\"\\\"\"\n",
    "    # Clip to prevent extreme values before FFT\n",
    "    phi_clipped = np.clip(phi_cpu_np_array, -1e6, 1e6).astype(np.float32)\n",
    "    rho_field_np = phi_clipped**2\n",
    "    \n",
    "    rho_mean = np.mean(rho_field_np)\n",
    "    rho_k = fftn(rho_field_np - rho_mean)\n",
    "    power_spectrum_rho = np.abs(rho_k)**2\n",
    "\n",
    "    correlation_func_raw_data = ifftn(power_spectrum_rho).real \n",
    "\n",
    "    if rho_mean**2 > 1e-15:\n",
    "        xi_normalized = correlation_func_raw_data / (N_grid_param**3 * rho_mean**2)\n",
    "    else:\n",
    "        xi_normalized = np.zeros_like(correlation_func_raw_data)\n",
    "\n",
    "    indices_shifted = np.fft.ifftshift(np.arange(N_grid_param)) - (N_grid_param // 2)\n",
    "    rx_coords = indices_shifted * dx_val_param\n",
    "    ry_coords = indices_shifted * dx_val_param\n",
    "    rz_coords = indices_shifted * dx_val_param\n",
    "    rxx_mesh, ryy_mesh, rzz_mesh = np.meshgrid(rx_coords, ry_coords, rz_coords, indexing='ij', sparse=True)\n",
    "    r_magnitude_values = np.sqrt(rxx_mesh**2 + ryy_mesh**2 + rzz_mesh**2)\n",
    "\n",
    "    r_bins_def = np.linspace(0, L_box_param / 2, 50) \n",
    "    \n",
    "    corr_binned_values, _ = np.histogram(\\\n",
    "        r_magnitude_values.ravel(), bins=r_bins_def,\\\n",
    "        weights=xi_normalized.ravel()\\\n",
    "    )\n",
    "    counts_in_bins, _ = np.histogram(r_magnitude_values.ravel(), bins=r_bins_def)\n",
    "    \n",
    "    corr_binned_final = np.divide(corr_binned_values, counts_in_bins, out=np.zeros_like(corr_binned_values), where=counts_in_bins!=0)\n",
    "    r_bin_centers_final = (r_bins_def[:-1] + r_bins_def[1:]) / 2\n",
    "\n",
    "    return r_bin_centers_final, corr_binned_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8500",
   "metadata": {
    "collapsed": false,
    "id": "simulation-orchestration-lss-definitive-cocalc"
   },
   "source": [
    "## Simulation Orchestration for Definitive Run\n",
    "\n",
    "This section sets up the definitive high-resolution LSS simulation. It includes robust checkpointing and resume logic to ensure progress is saved and can be continued, which is crucial for managing computational resources.\n",
    "The `run_lss_simulation` function directly calls `update_phi_rk4_lss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "id": "d41c60",
   "metadata": {
    "collapsed": false,
    "id": "code-main-orchestration-lss-definitive-cocalc",
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "def run_lss_simulation(config: dict, device: torch.device, checkpoint_dir: str, data_dir: str):\n",
    "    \"\"\"Main simulation loop for EFM LSS with domain decomposition and multi-GPU support.\"\"\"\n",
    "    print(f\"Initializing fields for EFM LSS simulation ({config['run_id']}) on {device} with {torch.cuda.device_count()} GPUs...\")\n",
    "\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    devices = [torch.device(f'cuda:{i}') for i in range(num_gpus)] if num_gpus > 0 else [torch.device('cpu')]\n",
    "    phi_subdomains = [None] * num_gpus\n",
    "    phi_dot_subdomains = [None] * num_gpus\n",
    "    start_step = 0\n",
    "    energy_history = []\n",
    "    # density_norm_history will be computed at the end in plot_lss_results from phi_final_cpu for simplicity.\n",
    "\n",
    "    N = config['N']\n",
    "    base_size = N // num_gpus\n",
    "    remainder = N % num_gpus\n",
    "    subdomain_sizes = [base_size + 1 if i < remainder else base_size for i in range(num_gpus)]\n",
    "    subdomain_starts = [sum(subdomain_sizes[:i]) for i in range(num_gpus)]\n",
    "    subdomain_ends = [start + size for start, size in zip(subdomain_starts, subdomain_sizes)]\n",
    "    print(f\"Grid size N: {N}, Subdomain sizes: {subdomain_sizes}\")\n",
    "\n",
    "    # Check for existing checkpoint to resume\n",
    "    checkpoint_file_pattern = os.path.join(checkpoint_dir, f\"CKPT_{config['run_id']}_step_*.npz\")\n",
    "    existing_checkpoints = sorted(glob.glob(checkpoint_file_pattern),\\\n",
    "                                 key=lambda f: int(os.path.basename(f).split('_step_')[1].split('.npz')[0]), reverse=True)\n",
    "    \n",
    "    if existing_checkpoints: \n",
    "        latest_checkpoint_file = existing_checkpoints[0]\n",
    "        print(f\"Resuming from checkpoint: {latest_checkpoint_file}\")\n",
    "        try: \n",
    "            checkpoint = np.load(latest_checkpoint_file, allow_pickle=True)\n",
    "            phi_full = torch.from_numpy(checkpoint['phi_r_cpu']).to(dtype=torch.float16)\n",
    "            phi_dot_full = torch.from_numpy(checkpoint['phi_dot_r_cpu']).to(dtype=torch.float16)\n",
    "            start_step = checkpoint['last_step'].item() + 1\n",
    "            if 'energy_history_saved' in checkpoint.files:\n",
    "                energy_history.extend(checkpoint['energy_history_saved'].tolist())\n",
    "            \n",
    "            # Distribute loaded full field to subdomains\n",
    "            for i in range(num_gpus):\n",
    "                phi_subdomains[i] = torch.empty((subdomain_sizes[i], N, N), dtype=torch.float16, device=devices[i])\n",
    "                phi_dot_subdomains[i] = torch.empty((subdomain_sizes[i], N, N), dtype=torch.float16, device=devices[i])\n",
    "                # Use copy_() for non-blocking copy from CPU to GPU\n",
    "                phi_subdomains[i].copy_(phi_full[subdomain_starts[i]:subdomain_ends[i]].to(devices[i], non_blocking=True))\n",
    "                phi_dot_subdomains[i].copy_(phi_dot_full[subdomain_starts[i]:subdomain_ends[i]].to(devices[i], non_blocking=True))\n",
    "            \n",
    "            print(f\"Resumed from step {start_step}. Last recorded energy: {energy_history[-1]:.4g}\" if energy_history else f\"Resumed from step {start_step}.\")\n",
    "            del checkpoint, phi_full, phi_dot_full\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint {latest_checkpoint_file}: {e}. Starting from scratch.\")\n",
    "            phi_subdomains = [None] * num_gpus # Reset to trigger new initialization if loading fails\n",
    "\n",
    "    if all(p is None for p in phi_subdomains): \n",
    "        print(\"No valid checkpoint found or error loading. Starting simulation from scratch.\")\n",
    "        x_coords = np.linspace(-config['L_sim_unit']/2, config['L_sim_unit']/2, config['N'], dtype=np.float32)\n",
    "        X, Y, Z = np.meshgrid(x_coords, x_coords, x_coords, indexing='ij')\n",
    "        \n",
    "        seeded_modes_field = config['seeded_perturbation_amplitude'] * (\\\n",
    "            np.sin(config['k_seed_primary'] * X) +\\\n",
    "            np.sin(config['k_seed_secondary'] * Y) +\\\n",
    "            np.cos(config['k_seed_primary'] * Z)\\\n",
    "        )\n",
    "        random_background_noise = config['background_noise_amplitude'] * (np.random.rand(config['N'], config['N'], config['N']) - 0.5)\n",
    "        initial_phi_np = seeded_modes_field + random_background_noise\n",
    "        if np.all(initial_phi_np == 0): \n",
    "            initial_phi_np = config['background_noise_amplitude'] * (np.random.rand(config['N'], config['N'], config['N']) - 0.5)\n",
    "        \n",
    "        initial_phi_tensor = torch.from_numpy(initial_phi_np.astype(np.float16)).pin_memory()\n",
    "        \n",
    "        for i in range(num_gpus):\n",
    "            phi_subdomains[i] = initial_phi_tensor[subdomain_starts[i]:subdomain_ends[i]].to(devices[i], non_blocking=True)\n",
    "            phi_dot_subdomains[i] = torch.zeros((subdomain_sizes[i], N, N), dtype=torch.float16, device=devices[i])\n",
    "        \n",
    "        if start_step == 0:\n",
    "            with torch.no_grad():\n",
    "                phi_full_cpu = torch.cat([p.cpu() for p in phi_subdomains], dim=0) \n",
    "                phi_dot_full_cpu = torch.cat([p.cpu() for p in phi_dot_subdomains], dim=0)\n",
    "                current_energy = compute_total_energy_lss(phi=phi_full_cpu, phi_dot=phi_dot_full_cpu, \\\n",
    "                                                       m_sq_param=config['m_sim_unit_inv']**2, \\\n",
    "                                                       g_param=config['g_sim'], eta_param=config['eta_sim'], \\\n",
    "                                                       dx=config['dx_sim_unit'], c_sq_param=config['c_sim_unit']**2)\n",
    "                current_density_norm = torch.sum(phi_full_cpu.to(torch.float32)**2).item() * config['k_efm_gravity_coupling']\n",
    "                del phi_full_cpu, phi_dot_full_cpu \n",
    "            energy_history.append(current_energy) \n",
    "            print(f\"Initial State: Energy={current_energy:.4g}, Density Norm={current_density_norm:.4g}\")\n",
    "\n",
    "    efm_models = []\n",
    "    streams = [torch.cuda.Stream(device=devices[i]) for i in range(num_gpus)] \n",
    "    for i in range(num_gpus):\n",
    "        model = EFMLSSModule(\\\n",
    "            dx=config['dx_sim_unit'], m_sq=config['m_sim_unit_inv']**2, g=config['g_sim'], eta=config['eta_sim'],\\\n",
    "            k_gravity=config['k_efm_gravity_coupling'], G_gravity=config['G_sim_unit'], c_sq=config['c_sim_unit']**2,\\\n",
    "            alpha_param=config['alpha_sim'], delta_param=config['delta_sim']\\\n",
    "        ).to(devices[i])\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            dummy_phi = torch.zeros((subdomain_sizes[i], N, N), dtype=torch.float16, device=devices[i])\n",
    "            dummy_halo = torch.zeros((1, N, N), dtype=torch.float16, device=devices[i]) \n",
    "            model.conv_laplacian(dummy_phi, dummy_halo, dummy_halo)\n",
    "        efm_models.append(model)\n",
    "    \n",
    "    sim_start_time = time.time()\n",
    "    numerical_error = False\n",
    "\n",
    "    halo_buffer_cpu_left = [torch.empty((1, N, N), dtype=torch.float16, pin_memory=True) for _ in range(num_gpus)]\n",
    "    halo_buffer_cpu_right = [torch.empty((1, N, N), dtype=torch.float16, pin_memory=True) for _ in range(num_gpus)]\n",
    "    \n",
    "    def exchange_halos(phi_subs, phi_dot_subs): \n",
    "        # Send data to CPU buffers (for neighbor GPUs)\n",
    "        for i in range(num_gpus):\n",
    "            target_gpu_idx = (i + 1) % num_gpus\n",
    "            source_gpu_idx = (i - 1 + num_gpus) % num_gpus \n",
    "            with torch.cuda.stream(streams[i]): \n",
    "                halo_buffer_cpu_right[target_gpu_idx].copy_(phi_subs[i][-1:, :, :].cpu(), non_blocking=True)\n",
    "                halo_buffer_cpu_left[source_gpu_idx].copy_(phi_subs[i][0:1, :, :].cpu(), non_blocking=True)\n",
    "        \n",
    "        torch.cuda.synchronize() \n",
    "\n",
    "        halos_left = [None] * num_gpus\n",
    "        halos_right = [None] * num_gpus\n",
    "        for i in range(num_gpus):\n",
    "            with torch.cuda.stream(streams[i]): \n",
    "                halos_right[i] = halo_buffer_cpu_right[i].to(devices[i], non_blocking=True)\n",
    "                halos_left[i] = halo_buffer_cpu_left[i].to(devices[i], non_blocking=True)\n",
    "        \n",
    "        torch.cuda.synchronize() \n",
    "        \n",
    "        return halos_left, halos_right\n",
    "\n",
    "    # Main simulation loop\n",
    "    for t_step in tqdm(range(start_step, config['T_steps']), desc=f\"LSS Sim ({config['run_id']})\"):\n",
    "        # Check for NaN/Inf/extreme values in subdomains (more frequent for diagnostics)\n",
    "        if (t_step + 1) % 1000 == 0 or t_step == start_step: \n",
    "            for i in range(num_gpus): \n",
    "                phi_max = torch.max(torch.abs(phi_subdomains[i])).item()\n",
    "                phi_dot_max = torch.max(torch.abs(phi_dot_subdomains[i])).item()\n",
    "                if phi_max > 1e4 or phi_dot_max > 1e4 or \\\n",
    "                   torch.any(torch.isinf(phi_subdomains[i])) or torch.any(torch.isnan(phi_subdomains[i])) or \\\n",
    "                   torch.any(torch.isinf(phi_dot_subdomains[i])) or torch.any(torch.isnan(phi_dot_subdomains[i])):\n",
    "                    print(f\"\\nError: NaN/Inf or extreme values in subdomain {i} at step {t_step + 1}: phi_max={phi_max:.4g}, phi_dot_max={phi_dot_max:.4g}\\n\")\n",
    "                    numerical_error = True # This line's indentation should be at the same level as the 'if' it belongs to\n",
    "                    break \n",
    "            if numerical_error: \n",
    "                break \n",
    "        \n",
    "        # Clip fields to prevent runaway growth (tighter clipping to maintain stability)\n",
    "        with torch.no_grad(): \n",
    "            for i in range(num_gpus): \n",
    "                phi_subdomains[i].clamp_(-1e3, 1e3)  \n",
    "                phi_dot_subdomains[i].clamp_(-1e3, 1e3)\n",
    "\n",
    "        # Halo exchange for phi (simplified to phi only for conv_laplacian, as before)\n",
    "        halos_left_phi, halos_right_phi = exchange_halos(phi_subdomains, phi_dot_subdomains) \n",
    "\n",
    "        with torch.no_grad(): \n",
    "            for i in range(num_gpus): \n",
    "                with torch.cuda.stream(streams[i]): \n",
    "                    phi_subdomains[i], phi_dot_subdomains[i] = update_phi_rk4_lss(\\\n",
    "                        phi_subdomains[i], phi_dot_subdomains[i], config['dt_sim_unit'], efm_models[i],\\\n",
    "                        halos_left_phi[i].squeeze(0), halos_right_phi[i].squeeze(0) \n",
    "                    )\n",
    "        torch.cuda.synchronize() \n",
    "\n",
    "        # Record energy diagnostics periodically\n",
    "        if (t_step + 1) % config['history_every_n_steps'] == 0: \n",
    "            with torch.no_grad():\n",
    "                phi_full_cpu_diag = torch.cat([p.cpu() for p in phi_subdomains], dim=0)\n",
    "                phi_dot_full_cpu_diag = torch.cat([p.cpu() for p in phi_dot_subdomains], dim=0)\n",
    "                current_energy = compute_total_energy_lss(phi=phi_full_cpu_diag, phi_dot=phi_dot_full_cpu_diag,\\\n",
    "                                                       m_sq_param=efm_models[0].m_sq, g_param=efm_models[0].g, eta_param=efm_models[0].eta,\\\n",
    "                                                       dx=efm_models[0].dx, c_sq_param=efm_models[0].c_sq)\n",
    "                current_density_norm = torch.sum(phi_full_cpu_diag.to(torch.float32)**2).item() * config['k_efm_gravity_coupling']\n",
    "                del phi_full_cpu_diag, phi_dot_full_cpu_diag \n",
    "            energy_history.append(current_energy) \n",
    "            tqdm.write(f\"Step {t_step+1}: E={current_energy:.3e}, DN={current_density_norm:.3e}\")\n",
    "            if np.isnan(current_energy) or np.isinf(current_energy): \n",
    "                print(f\"Instability: Energy is NaN/Inf at step {t_step+1}. Stopping.\")\n",
    "                numerical_error = True \n",
    "                break \n",
    "\n",
    "        # Save intermediate checkpoint\n",
    "        if (t_step + 1) % config['checkpoint_every_n_steps'] == 0 and (t_step + 1) < config['T_steps']: \n",
    "            intermediate_ckpt_file = os.path.join(checkpoint_dir, f\"CKPT_{config['run_id']}_step_{t_step+1}.npz\")\n",
    "            try: \n",
    "                with torch.no_grad():\n",
    "                    phi_full = torch.cat([p.cpu() for p in phi_subdomains], dim=0).numpy()\n",
    "                    phi_dot_full = torch.cat([p.cpu() for p in phi_dot_subdomains], dim=0).numpy()\n",
    "                np.savez_compressed(intermediate_ckpt_file,\\\n",
    "                                    phi_r_cpu=phi_full,\\\n",
    "                                    phi_dot_r_cpu=phi_dot_full,\\\n",
    "                                    last_step=t_step,\\\n",
    "                                    config_lss_saved=config,\\\n",
    "                                    energy_history_saved=np.array(energy_history))\\\n",
    "                print(f\"Checkpoint saved at step {t_step+1} to {intermediate_ckpt_file}\")\n",
    "                del phi_full, phi_dot_full\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception as e_save:\n",
    "                print(f\"Error saving intermediate checkpoint: {e_save}\")\n",
    "\n",
    "    sim_duration = time.time() - sim_start_time \n",
    "    print(f\"Simulation finished in {sim_duration:.2f} seconds.\")\n",
    "    if numerical_error: print(\"Simulation stopped due to numerical error.\") \n",
    "\n",
    "    # Save final state and history\n",
    "    final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") \n",
    "    final_data_filename = os.path.join(data_dir, f\"FINAL_LSS_DATA_{config['run_id']}_{final_timestamp}.npz\")\n",
    "    with torch.no_grad():\n",
    "        phi_final = torch.cat([p.cpu() for p in phi_subdomains], dim=0).numpy()\n",
    "        phi_dot_final = torch.cat([p.cpu() for p in phi_dot_subdomains], dim=0).numpy()\n",
    "    np.savez_compressed(final_data_filename,\\\n",
    "                        phi_final_cpu=phi_final,\\\n",
    "                        phi_dot_final_cpu=phi_dot_final,\\\n",
    "                        energy_history=np.array(energy_history),\\\n",
    "                        config_lss=config,\\\n",
    "                        sim_had_numerical_error=numerical_error)\\\n",
    "    print(f\"Final LSS simulation data saved to {final_data_filename}\")\n",
    "\n",
    "    # Clean up all tensors and models\n",
    "    del phi_subdomains, phi_dot_subdomains, efm_models, halo_buffer_cpu_left, halo_buffer_cpu_right, streams\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return final_data_filename\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "colab_kernel_launcher",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3",
   "env": {
   },
   "interrupt_mode": "signal",
   "language": "python",
   "metadata": {
    "colab": {
     "socketio_version": "2.1.1"
    }
   },
   "name": "python3",
   "resource_dir": "/usr/local/share/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}