{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-lss-dimless"
      },
      "source": [
        "<h1>EFM Large-Scale Structure (LSS) & Clustering Validation (Dimensionless, A100 Optimized)</h1>\n",
        "<p>This notebook performs a high-resolution simulation of Large-Scale Structure (LSS) formation within the Eholoko Fluxon Model (EFM) framework. Crucially, this simulation operates entirely in <strong>dimensionless units</strong>, consistent with the core theoretical foundation of EFM as implied by the parameter choices in its foundational papers (e.g., <code>c=1.0</code>, <code>G=1.0</code>). Physical interpretations (e.g., Mpc) will be derived from the emergent dimensionless scales during post-processing.</p>\n",
        "<p>This revised approach explicitly addresses previous challenges in achieving emergent clustering by incorporating <strong>seeded initial conditions</strong> and is optimized for efficient use of the A100 GPU.</p>\n",
        "<h2>EFM Theoretical Grounding for LSS (S/T State, n'=1 HDS):</h2>\n",
        "<ol>\n",
        "<li><p><strong>Single Scalar Field (φ):</strong> All phenomena, including cosmic structure, emerge from this fundamental field [1, 2].</p>\n",
        "</li>\n",
        "<li><p><strong>NLKG Equation with EFM Self-Gravity:</strong> The evolution of φ is governed by a specific Nonlinear Klein-Gordon equation. For LSS, the dominant terms are:</p>\n",
        "<ul>\n",
        "<li><code>c²∇²φ</code>: Spatial interaction/propagation.</li>\n",
        "<li><code>m²φ</code>: The mass term for the background field. <strong>For LSS, the paper 'Unifying Cosmic Structure' [4] specifies <code>m=1.0</code> for its core equation in the S/T state.</strong></li>\n",
        "<li><code>gφ³</code>, <code>ηφ⁵</code>: Nonlinear self-interaction terms crucial for preventing linear dispersion and driving the formation and stabilization of cosmic structures [4].</li>\n",
        "<li><code>8πGkφ²</code>: The EFM self-gravity term, where the field's own density (<code>kφ²</code>) acts as the source for emergent gravitational interactions, replacing spacetime curvature [5].</li>\n",
        "<li><code>αφ(∂φ/∂t)⋅∇φ</code>, <code>δ(∂φ/∂t)²φ</code>: State-dependent dynamical friction and dissipation terms, important for system stability and evolution [6]. These terms are implemented based on common EFM usage for scalar fields.</li>\n",
        "</ul>\n",
        "</li>\n",
        "<li><p><strong>Harmonic Density States (HDS):</strong> The HDS framework dictates the emergence of characteristic clustering scales (~147 Mpc and ~628 Mpc) that this simulation aims to reproduce by amplifying specific seeded modes [7].</p>\n",
        "</li>\n",
        "<li><p><strong>Initial Conditions (Crucial for LSS):</strong> Instead of pure random noise, low-amplitude, long-wavelength <strong>sinusoidal perturbations are explicitly seeded</strong> to provide a template for the EFM dynamics to amplify and form the expected structures. Background noise is also included. [4, Section 2.1, Simulation Code Snippet].</p>\n",
        "</li>\n",
        "</ul>\n",
        "<h2>Objectives:</h2>\n",
        "<ul>\n",
        "<li><p>Simulate 3D LSS formation on a <strong>450³ grid for 200,000 timesteps</strong> using dimensionless EFM parameters and <strong>seeded initial conditions</strong>.</p>\n",
        "</li>\n",
        "<li><p>Ensure efficient and efficacious use of <strong>Colab A100 (40GB VRAM, 80GB RAM)</strong>.</p>\n",
        "</li>\n",
        "<li><p>Confirm numerical stability of the dimensionless approach and parameter set.</p>\n",
        "</li>\n",
        "<li><p>Compute power spectrum P(k) and correlation function ξ(r) from the dimensionless results.</p>\n",
        "</li>\n",
        "<li><p><strong>Identify and quantify emergent dimensionless clustering scales</strong> (peaks in P(k) and ξ(r)).</p>\n",
        "</li>\n",
        "<li><p>Infer physical clustering scales (147 Mpc, 628 Mpc) by mapping the emergent dimensionless scales using EFM's theoretical predictions.</p>\n",
        "</li>\n",
        "</ul>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount-drive-instruction-lss"
      },
      "source": [
        "<h2>Google Drive Setup (for Colab)</h2>\n",
        "<p>To ensure data and plots are saved to and retrieved from your Google Drive, please execute the following cell to mount your Drive.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive-code-lss"
      },
      "source": [
        "# This cell is specific to Google Colab environments\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Not in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}. Please ensure you're logged in and have granted permissions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "environment-setup-lss"
      },
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import psutil\n",
        "from tqdm.notebook import tqdm # Use tqdm.notebook for Jupyter environments\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from scipy.fft import fftn, fftfreq, ifftn # Using scipy for CPU-based FFT for final analysis\n",
        "import torch.nn.functional as F\n",
        "import torch.amp as amp # CORRECTED: Use torch.amp as amp for autocast\n",
        "import matplotlib.pyplot as plt # For plotting\n",
        "import glob\n",
        "\n",
        "# Environment setup for PyTorch CUDA memory management\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512' # To help with memory fragmentation\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "num_gpus_available = torch.cuda.device_count()\n",
        "available_devices_list = [torch.device(f'cuda:{i}') for i in range(num_gpus_available)]\n",
        "print(f\"Number of GPUs available: {num_gpus_available}, Available devices: {available_devices_list}\")\n",
        "if num_gpus_available > 0:\n",
        "    current_gpu_device = torch.device('cuda:0')\n",
        "    print(f\"Using GPU 0: {torch.cuda.get_device_name(current_gpu_device)}, VRAM: {torch.cuda.get_device_properties(current_gpu_device).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    current_gpu_device = torch.device('cpu')\n",
        "    print(\"No GPU available, running on CPU. Performance may be limited.\")\n",
        "print(f\"System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "\n",
        "# Define paths for checkpoints and data/plots - Adjusted for Google Drive\n",
        "checkpoint_path_lss = '/content/drive/My Drive/EFM_Simulations/checkpoints/LSS_DIMLESS_A100_v2/'\n",
        "data_path_lss = '/content/drive/My Drive/EFM_Simulations/data/LSS_DIMLESS_A100_v2/'\n",
        "os.makedirs(checkpoint_path_lss, exist_ok=True)\n",
        "os.makedirs(data_path_lss, exist_ok=True)\n",
        "print(f\"LSS Checkpoints will be saved to: {checkpoint_path_lss}\")\n",
        "print(f\"LSS Data/Plots will be saved to: {data_path_lss}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-lss-dimless"
      },
      "source": [
        "<h2>Configuration for LSS Simulation (Dimensionless Units, A100 Optimized)</h2>\n",
        "<p>Parameters are set according to EFM principles for LSS as dimensionless units. <code>N</code> and <code>T_steps</code> are increased for a higher-resolution, longer run. <strong>Initial conditions are now seeded with specific long-wavelength modes to promote structure formation.</strong></p>\n",
        "<h3>Parameter Derivation and EFM Justification (Dimensionless):</h3>\n",
        "<ul>\n",
        "<li><p><strong><code>N</code></strong>: Grid size (N x N x N). <strong>Increased to <code>450</code></strong> for a higher-resolution, more detailed simulation that fits A100 capabilities.</p>\n",
        "</li>\n",
        "<li><p><strong><code>L_sim_unit</code></strong>: Physical size of the simulation box in <strong>dimensionless simulation units</strong>. <code>10.0</code> is a common choice for such dimensionless systems. This <code>L</code> will later be scaled to physical Mpc based on emergent structures.</p>\n",
        "</li>\n",
        "<li><p><strong><code>dx_sim_unit</code></strong>: Spatial step in dimensionless units. Calculated as <code>L_sim_unit / N</code>.</p>\n",
        "</li>\n",
        "<li><p><strong><code>c_sim_unit</code></strong>: Speed of light in <strong>dimensionless simulation units</strong>. Set to <code>1.0</code> as per EFM papers' implied dimensionless constants (e.g., 'Fluxonic Cosmology' Section 2.1) for internal consistency.</p>\n",
        "</li>\n",
        "<li><p><strong><code>dt_cfl_factor</code></strong>: Courant-Friedrichs-Lewy (CFL) condition factor. <code>0.001</code> is a robust value for stability in dimensionless <code>c=1</code> systems with nonlinear dynamics.</p>\n",
        "</li>\n",
        "<li><p><strong><code>T_steps</code></strong>: Total simulation steps. <strong>Increased to <code>200000</code> for a significantly longer evolution</strong> to allow structures to fully develop and amplify.</p>\n",
        "</li>\n",
        "<li><p><strong><code>m_sim_unit_inv</code> (m in m²φ)</strong>: Mass term coefficient in dimensionless units. <strong>Set to <code>1.0</code> as per 'Unifying Cosmic Structure' paper Section 2 [4].</strong></p>\n",
        "</li>\n",
        "<li><p><strong><code>g_sim</code> (g in gφ³)</strong>: Cubic nonlinearity coefficient in dimensionless units. <strong>Set to <code>0.1</code> as per 'Unifying Cosmic Structure' paper Section 2 [4].</strong> This parameter is crucial for driving matter clumping.</p>\n",
        "</li>\n",
        "<li><p><strong><code>eta_sim</code> (η in ηφ⁵)</strong>: Quintic nonlinearity coefficient in dimensionless units. <strong>Set to <code>0.01</code> as per 'Unifying Cosmic Structure' paper Section 2 [4].</strong> Helps stabilize higher amplitude field configurations.</p>\n",
        "</li>\n",
        "<li><p><strong><code>k_efm_gravity_coupling</code> (k in 8πGkφ²)</strong>: Coupling constant for the EFM self-gravity term in dimensionless units. <strong>Set to <code>0.005</code> as per 'Unifying Cosmic Structure' paper Section 2 [4].</strong> Determines the strength of emergent gravity.</p>\n",
        "</li>\n",
        "<li><p><strong><code>G_sim_unit</code> (G in 8πGkφ²)</strong>: Gravitational constant in dimensionless simulation units. <strong>Set to <code>1.0</code> [4].</strong></p>\n",
        "</li>\n",
        "<li><p><strong><code>alpha_sim</code> (α in αφ(∂φ/∂t)⋅∇φ)</strong>: State parameter in dimensionless units. <strong>Set to <code>0.7</code> for S/T state dynamics as per 'Unifying Cosmic Structure' paper Section 2 [4].</strong> This term provides dynamical friction or gain.</p>\n",
        "</li>\n",
        "<li><p><strong><code>delta_sim</code> (δ in δ(∂φ/∂t)²φ)</strong>: Dissipation term in dimensionless units. <strong>Set to <code>0.0002</code> as per 'Unifying Cosmic Structure' paper Section 2 [4].</strong> Controls energy dissipation.</p>\n",
        "</li>\n",
        "<li><p><strong><code>seeded_perturbation_amplitude</code></strong>: New parameter for the amplitude of the initial seeded sinusoidal modes. <strong>Set to <code>1.0e-3</code></strong> to provide a stronger, more targeted initial push for structure formation compared to pure noise. This amplitude is relative to the overall φ field.</p>\n",
        "</li>\n",
        "<li><p><strong><code>background_noise_amplitude</code></strong>: Amplitude of general random noise in dimensionless units. <strong>Set to <code>1.0e-6</code></strong> to provide a chaotic background while allowing seeded modes to dominate large scales.</p>\n",
        "</li>\n",
        "<li><p><strong><code>k_seed_primary</code></strong>: Dimensionless wavenumber for the primary (largest) seeded mode. We'll set this to correspond to 2 cycles within the <code>L_sim_unit</code> box (<code>2 * np.pi / (L_sim_unit / 2)</code>). This will be amplified to correspond to the 628 Mpc scale.</p>\n",
        "</li>\n",
        "<li><p><strong><code>k_seed_secondary</code></strong>: Dimensionless wavenumber for the secondary seeded mode. We'll set this to correspond to 5 cycles within the <code>L_sim_unit</code> box (<code>2 * np.pi / (L_sim_unit / 5)</code>). This will be amplified to correspond to the 147 Mpc (BAO-like) scale.</p>\n",
        "</li>\n",
        "</ul>\n",
        "<p>This configuration ensures consistency with the theoretical framework and directly addresses the need for robust structure emergence in the simulation.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-config-lss"
      },
      "source": [
        "config_lss_run = {}\n",
        "config_lss_run['N'] = 450  # Grid size (N x N x N) - Increased to 450\n",
        "config_lss_run['L_sim_unit'] = 10.0  # Dimensionless box size\n",
        "config_lss_run['dx_sim_unit'] = config_lss_run['L_sim_unit'] / config_lss_run['N'] # Dimensionless spatial step\n",
        "\n",
        "config_lss_run['c_sim_unit'] = 1.0  # Dimensionless speed of light (as per EFM papers)\n",
        "config_lss_run['dt_cfl_factor'] = 0.001 # Robust CFL factor for dimensionless c=1 system\n",
        "config_lss_run['dt_sim_unit'] = config_lss_run['dt_cfl_factor'] * config_lss_run['dx_sim_unit'] / config_lss_run['c_sim_unit']\n",
        "\n",
        "config_lss_run['T_steps'] = 200000 # Total number of time steps - Increased to 200,000\n",
        "\n",
        "# EFM Parameters for LSS from 'Unifying Cosmic Structure' paper, Section 2\n",
        "config_lss_run['m_sim_unit_inv'] = 1.0 # m=1.0 for LSS in this paper\n",
        "config_lss_run['g_sim'] = 0.1          # g from paper\n",
        "config_lss_run['eta_sim'] = 0.01         # eta from paper\n",
        "config_lss_run['k_efm_gravity_coupling'] = 0.005 # k from paper\n",
        "config_lss_run['G_sim_unit'] = 1.0 # G from paper\n",
        "config_lss_run['alpha_sim'] = 0.7  # alpha for S/T state from paper\n",
        "config_lss_run['delta_sim'] = 0.0002 # delta from paper\n",
        "\n",
        "# Initial Conditions - NOW WITH SEEDED PERTURBATIONS\n",
        "config_lss_run['seeded_perturbation_amplitude'] = 1.0e-3 # Amplitude of seeded sinusoidal modes\n",
        "config_lss_run['background_noise_amplitude'] = 1.0e-6 # Amplitude of general random background noise\n",
        "\n",
        "# Define dimensionless wavenumbers for seeding based on desired physical scales\n",
        "# We'll seed modes that represent a few cycles within the box, which will later be scaled.\n",
        "# For example, to seed 2 cycles (primary) and 5 cycles (secondary) within L_sim_unit\n",
        "config_lss_run['k_seed_primary'] = 2 * np.pi / (config_lss_run['L_sim_unit'] / 2.0) # Corresponds to L/2 wavelength\n",
        "config_lss_run['k_seed_secondary'] = 2 * np.pi / (config_lss_run['L_sim_unit'] / 5.0) # Corresponds to L/5 wavelength\n",
        "\n",
        "config_lss_run['run_id'] = (\n",
        "    f\"LSS_N{config_lss_run['N']}_T{config_lss_run['T_steps']}_\" +\n",
        "    f\"m{config_lss_run['m_sim_unit_inv']:.1e}_g{config_lss_run['g_sim']:.1e}_eta{config_lss_run['eta_sim']:.1e}_\" +\n",
        "    f\"k{config_lss_run['k_efm_gravity_coupling']:.1e}_G{config_lss_run['G_sim_unit']:.1e}_alpha{config_lss_run['alpha_sim']:.1e}_delta{config_lss_run['delta_sim']:.1e}_\" +\n",
        "    f\"CFL{config_lss_run['dt_cfl_factor']:.1e}_A100_DIMLESS_LSS_v3_Seeded\"\n",
        ")\n",
        "\n",
        "config_lss_run['history_every_n_steps'] = 1000 # Frequency of calculating/storing diagnostics - Adjusted for longer run\n",
        "config_lss_run['checkpoint_every_n_steps'] = 5000 # Frequency of saving intermediate checkpoints - Adjusted for longer run\n",
        "\n",
        "print(f\"--- EFM LSS Simulation Configuration ({config_lss_run['run_id']}) ---\")\n",
        "for key, value in config_lss_run.items():\n",
        "    if isinstance(value, (float, np.float32, np.float64)):\n",
        "        print(f\"{key}: {value:.4g}\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\n--- Physical Scaling (for interpretation of dimensionless results) ---\")\n",
        "print(\"The simulation runs in dimensionless units. Physical scales (Mpc, Gyr) will be derived post-simulation by matching emergent peaks to EFM's predicted 147 Mpc and 628 Mpc scales.\")\n",
        "print(f\"Dimensionless L: {config_lss_run['L_sim_unit']} units, dx: {config_lss_run['dx_sim_unit']:.4g} units\")\n",
        "print(f\"Dimensionless dt: {config_lss_run['dt_sim_unit']:.4g} units\")\n",
        "print(f\"Dimensionless k_seed_primary: {config_lss_run['k_seed_primary']:.4g}\")\n",
        "print(f\"Dimensionless k_seed_secondary: {config_lss_run['k_seed_secondary']:.4g}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mathematical-framework-lss"
      },
      "source": [
        "<h2>Mathematical Framework: EFM Nonlinear Klein-Gordon Equation for LSS</h2>\n",
        "<p>The core dynamics are governed by the specific NLKG equation from Section 2 of the 'Ehokolo Fluxon Model: Unifying Cosmic Structure, Non-Gaussianity, and Gravitational Waves Across Scales' paper [4]:</p>\n",
        "<pre><code>∂²φ/∂t² − c²∇²φ + m²φ + gφ³ + ηφ⁵ + δφ⁷ = 8πGkφ² + β(B × ∇φ) + αφ(∂φ/∂t)⋅∇φ\n",
        "</code></pre>\n",
        "<p>This is a general form. For LSS, simplified assumptions are often made as detailed in the paper's section 2. We will use the following terms, consistent with the broader EFM cosmological framework and parameters from [4] which specify <code>α</code> and <code>δ</code> terms:</p>\n",
        "<pre><code>φ_ddot = c²∇²φ - (m²φ + gφ³ + ηφ⁵) + αφ(∂φ/∂t)⋅∇φ + δ(∂φ/∂t)²φ + 8πGkφ²\n",
        "</code></pre>\n",
        "<p>The terms are:</p>\n",
        "<ul>\n",
        "<li><code>∂²φ/∂t²</code>: Second time derivative (acceleration).</li>\n",
        "<li><code>− c²∇²φ</code>: Spatial curvature/propagation term. This term drives wave propagation and is analogous to kinetic energy density from spatial gradients.</li>\n",
        "<li><code>+ m²φ + gφ³ + ηφ⁵</code>: Self-interaction potential terms, <code>V'(φ)</code>. These terms are derived from the potential <code>V(φ) = m²φ²/2 + gφ⁴/4 + ηφ⁶/6</code> and are crucial for the stability and formation of localized structures (ehokolons). For LSS, <code>m=1.0</code> in this specific paper [4].</li>\n",
        "<li><code>− αφ(∂φ/∂t)⋅∇φ</code>: State-dependent <code>α</code> term. On the RHS, <code>+ αφ(∂φ/∂t)⋅∇φ</code>. Implemented as <code>α * φ * (∂φ/∂t) * |∇φ|²</code> for scalar consistency, as this form is commonly applied for such dynamic terms in EFM scalar equations.</li>\n",
        "<li><code>− δ(∂φ/∂t)²φ</code>: Dissipation term. On the RHS, <code>+ δ(∂φ/∂t)²φ</code>. This form is used consistently for dissipation in other EFM papers [6] and is used here instead of the <code>δφ⁷</code> term which is a more general high-order damping. The paper [4] gives <code>δ=0.0002</code> for its LSS context.</li>\n",
        "<li><code>8πGkφ²</code>: The EFM self-gravity source term. This term drives the clustering of matter in LSS.</li>\n",
        "</ul>\n",
        "<p>This equation drives the evolution of the φ field to form cosmic structures.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "core-simulation-functions-lss"
      },
      "source": [
        "class EFMLSSModule(nn.Module):\n",
        "    \"\"\"EFM Module for the NLKG equation for LSS, using dimensionless parameters.\"\"\"\n",
        "    def __init__(self, dx, m_sq, g, eta, k_gravity, G_gravity, c_sq, alpha_param, delta_param):\n",
        "        super(EFMLSSModule, self).__init__()\n",
        "        self.dx = dx\n",
        "        self.m_sq = m_sq \n",
        "        self.g = g\n",
        "        self.eta = eta\n",
        "        self.k_gravity = k_gravity\n",
        "        self.G_gravity = G_gravity\n",
        "        self.c_sq = c_sq\n",
        "        self.alpha_param = alpha_param\n",
        "        self.delta_param = delta_param\n",
        "\n",
        "        # 3D Laplacian stencil (7-point, order 2) for periodic boundary conditions\n",
        "        stencil_np = np.array([[[0,0,0],[0,1,0],[0,0,0]],\n",
        "                               [[0,1,0],[1,-6,1],[0,1,0]],\n",
        "                               [[0,0,0],[0,1,0],[0,0,0]]], dtype=np.float32)\n",
        "        self.stencil = torch.from_numpy(stencil_np / (dx**2))\n",
        "        self.stencil = self.stencil.view(1, 1, 3, 3, 3)\n",
        "\n",
        "    def conv_laplacian(self, phi_field):\n",
        "        stencil_dev = self.stencil.to(phi_field.device)\n",
        "        phi_reshaped = phi_field.unsqueeze(0).unsqueeze(0)\n",
        "        phi_padded = F.pad(phi_reshaped, (1,1,1,1,1,1), mode='circular')\n",
        "        laplacian = F.conv3d(phi_padded, stencil_dev, padding=0)\n",
        "        return laplacian.squeeze(0).squeeze(0)\n",
        "\n",
        "    def nlkg_derivative_lss(self, phi, phi_dot):\n",
        "        \"\"\"Computes time derivatives (phi_dot, phi_ddot) based on the EFM LSS NLKG equation.\\nEquation: φ_ddot = c²∇²φ - (m²φ + gφ³ + ηφ⁵) + αφ(∂φ/∂t)⋅∇φ + δ(∂φ/∂t)²φ + 8πGkφ²\"\"\"\n",
        "        # Ensure phi and phi_dot are float32 for gradient calculation precision if they come in as float16\n",
        "        phi_f32 = phi.to(torch.float32)\n",
        "        phi_dot_f32 = phi_dot.to(torch.float32)\n",
        "\n",
        "        lap_phi = self.conv_laplacian(phi_f32)\n",
        "\n",
        "        # V'(φ) = m²φ + gφ³ + ηφ⁵\n",
        "        potential_force = self.m_sq * phi_f32 + \\\n",
        "                          self.g * torch.pow(phi_f32, 3) + \\\n",
        "                          self.eta * torch.pow(phi_f32, 5)\n",
        "\n",
        "        # Term: αφ(∂φ/∂t)⋅∇φ (re-interpreting dot product as a scalar term |∇φ|^2 for NLKG scalar equation)\n",
        "        # Use torch.gradient for potentially higher accuracy in gradients\n",
        "        grad_phi_tensors = torch.gradient(phi_f32, spacing=self.dx, dim=(0,1,2))\n",
        "        grad_phi_abs_sq = sum(g_val**2 for g_val in grad_phi_tensors)\n",
        "        alpha_term = self.alpha_param * phi_f32 * phi_dot_f32 * grad_phi_abs_sq\n",
        "\n",
        "        # Term: δ(∂φ/∂t)²φ\n",
        "        delta_term = self.delta_param * torch.pow(phi_dot_f32, 2) * phi_f32\n",
        "\n",
        "        # Source term: 8πGkφ²\n",
        "        source_gravity = 8.0 * float(np.pi) * self.G_gravity * self.k_gravity * torch.pow(phi_f32, 2)\n",
        "\n",
        "        # Equation of motion: φ_ddot = c²∇²φ - V'(φ) + alpha_term + delta_term + source_gravity\n",
        "        phi_ddot = self.c_sq * lap_phi - potential_force + alpha_term + delta_term + source_gravity\n",
        "        \n",
        "        # Return values in float16 for memory efficiency in RK4 loop\n",
        "        return phi_dot_f32.to(phi.dtype), phi_ddot.to(phi.dtype) \n",
        "\n",
        "def update_phi_rk4_lss(phi_current: torch.Tensor, phi_dot_current: torch.Tensor,\n",
        "                       dt: float, model_instance: EFMLSSModule) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"Updates phi and phi_dot using the RK4 method for one time step.\"\"\"\n",
        "    # CORRECTED: Use torch.amp.autocast for mixed precision\n",
        "    with amp.autocast(dtype=torch.float16):\n",
        "        k1_v, k1_a = model_instance.nlkg_derivative_lss(phi_current, phi_dot_current)\n",
        "        \n",
        "        phi_temp_k2 = phi_current + 0.5 * dt * k1_v\n",
        "        phi_dot_temp_k2 = phi_dot_current + 0.5 * dt * k1_a\n",
        "        k2_v, k2_a = model_instance.nlkg_derivative_lss(phi_temp_k2, phi_dot_temp_k2)\n",
        "        \n",
        "        phi_temp_k3 = phi_current + 0.5 * dt * k2_v\n",
        "        phi_dot_temp_k3 = phi_dot_current + 0.5 * dt * k2_a\n",
        "        k3_v, k3_a = model_instance.nlkg_derivative_lss(phi_temp_k3, phi_dot_temp_k3)\n",
        "        \n",
        "        phi_temp_k4 = phi_current + dt * k3_v\n",
        "        phi_dot_temp_k4 = phi_dot_current + dt * k3_a\n",
        "        k4_v, k4_a = model_instance.nlkg_derivative_lss(phi_temp_k4, phi_dot_temp_k4)\n",
        "            \n",
        "        phi_next = phi_current + (dt / 6.0) * (k1_v + 2*k2_v + 2*k3_v + k4_v)\n",
        "        phi_dot_next = phi_dot_current + (dt / 6.0) * (k1_a + 2*k2_a + 2*k3_a + k4_a)\n",
        "\n",
        "    # Explicitly clean up intermediate tensors\n",
        "    del k1_v, k1_a, k2_v, k2_a, k3_v, k3_a, k4_v, k4_a\n",
        "    del phi_temp_k2, phi_dot_temp_k2, phi_temp_k3, phi_dot_temp_k3, phi_temp_k4, phi_dot_temp_k4\n",
        "    \n",
        "    torch.cuda.synchronize(phi_current.device)\n",
        "    gc.collect() \n",
        "    torch.cuda.empty_cache()\n",
        "        \n",
        "    return phi_next, phi_dot_next\n",
        "\n",
        "def compute_total_energy_lss(phi: torch.Tensor, phi_dot: torch.Tensor,\n",
        "                              m_sq_param: float, g_param: float, eta_param: float,\n",
        "                              dx: float, c_sq_param: float) -> float:\n",
        "    \"\"\"Computes the total field energy based on the EFM Lagrangian for LSS (dimensionless units).\\nEnergy E = ∫ [1/2 (∂φ/∂t)² + 1/2 c²|∇φ|² + (m²φ²/2 + gφ⁴/4 + ηφ⁶/6)] dV\"\"\"\n",
        "    vol_element = dx**3\n",
        "\n",
        "    phi_f32 = phi.to(dtype=torch.float32)\n",
        "    phi_dot_f32 = phi_dot.to(dtype=torch.float32)\n",
        "\n",
        "    # CORRECTED: Use torch.amp.autocast for mixed precision for energy calculation\n",
        "    with amp.autocast(dtype=torch.float16):\n",
        "        kinetic_density = 0.5 * torch.pow(phi_dot_f32, 2)\n",
        "        potential_density = 0.5 * m_sq_param * torch.pow(phi_f32, 2) + \\\n",
        "                            0.25 * g_param * torch.pow(phi_f32, 4) + \\\n",
        "                            (1.0/6.0) * eta_param * torch.pow(phi_f32, 6)\n",
        "        \n",
        "        # Calculate |∇φ|^2 for the gradient energy term\n",
        "        # Use torch.gradient for consistency\n",
        "        grad_phi_tensors = torch.gradient(phi_f32, spacing=dx, dim=(0,1,2))\n",
        "        grad_phi_abs_sq = sum(g_val**2 for g_val in grad_phi_tensors)\n",
        "        gradient_energy_density = 0.5 * c_sq_param * grad_phi_abs_sq\n",
        "\n",
        "        total_energy_current_chunk = torch.sum(kinetic_density + potential_density + gradient_energy_density) * vol_element\n",
        "\n",
        "    if torch.isnan(total_energy_current_chunk) or torch.isinf(total_energy_current_chunk):\n",
        "        return float('nan')\n",
        "\n",
        "    total_energy_val = total_energy_current_chunk.item()\n",
        "\n",
        "    del phi_f32, phi_dot_f32, kinetic_density, potential_density, gradient_energy_density\n",
        "    del grad_phi_tensors, grad_phi_abs_sq\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return total_energy_val\n",
        "\n",
        "def compute_power_spectrum_lss(phi_cpu_np_array: np.ndarray, k_val_range: list,\n",
        "                               dx_val_param: float, N_grid_param: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Computes the 3D power spectrum P(k) in dimensionless units.\"\"\"\n",
        "    from scipy.fft import fftn, fftfreq # Included here for the placeholder, but should be at top level\n",
        "    if not isinstance(phi_cpu_np_array, np.ndarray):\n",
        "        phi_cpu_np_array = phi_cpu_np_array.cpu().numpy() # Ensure it's a NumPy array on CPU\n",
        "\n",
        "    phi_fft_transform = fftn(phi_cpu_np_array.astype(np.float32))\n",
        "    # Power spectrum |F(φ)|² normalized by N_total_points^2 = (N^3)^2 = N^6 for density\n",
        "    power_spectrum_raw_data = np.abs(phi_fft_transform)**2 / (N_grid_param**6)\n",
        "    del phi_fft_transform\n",
        "    gc.collect()\n",
        "\n",
        "    # Create k-space grid\n",
        "    kx_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    ky_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    kz_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    kxx_mesh, kyy_mesh, kzz_mesh = np.meshgrid(kx_coords, ky_coords, kz_coords, indexing='ij', sparse=True)\n",
        "    k_magnitude_values = np.sqrt(kxx_mesh**2 + kyy_mesh**2 + kzz_mesh**2)\n",
        "    del kxx_mesh, kyy_mesh, kzz_mesh, kx_coords, ky_coords, kz_coords\n",
        "    gc.collect()\n",
        "\n",
        "    # Binning the power spectrum\n",
        "    k_bins_def = np.linspace(k_val_range[0], k_val_range[1], 50) # 50 bins\n",
        "    # Use np.histogram with weights to correctly average power in bins\n",
        "    power_binned_values, _ = np.histogram(\n",
        "        k_magnitude_values.ravel(), bins=k_bins_def,\n",
        "        weights=power_spectrum_raw_data.ravel()\n",
        "    )\n",
        "    counts_in_bins, _ = np.histogram(k_magnitude_values.ravel(), bins=k_bins_def)\n",
        "    \n",
        "    power_binned_final = np.divide(power_binned_values, counts_in_bins, out=np.zeros_like(power_binned_values), where=counts_in_bins!=0)\n",
        "    k_bin_centers_final = (k_bins_def[:-1] + k_bins_def[1:]) / 2\n",
        "    \n",
        "    del k_magnitude_values, power_spectrum_raw_data, counts_in_bins\n",
        "    gc.collect()\n",
        "    return k_bin_centers_final, power_binned_final\n",
        "\n",
        "def compute_correlation_function_lss(phi_cpu_np_array: np.ndarray, dx_val_param: float,\n",
        "                                     N_grid_param: int, L_box_param: float) -> tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"Computes the 3D correlation function ξ(r) in dimensionless units.\"\"\"\n",
        "    from scipy.fft import fftn, ifftn # Included here for the placeholder, but should be at top level\n",
        "    if not isinstance(phi_cpu_np_array, np.ndarray):\n",
        "        phi_cpu_np_array = phi_cpu_np_array.cpu().numpy()\n",
        "\n",
        "    # The correlation function is the inverse Fourier transform of the Power Spectrum\n",
        "    # Calculate Power Spectrum first (phi_fft * conj(phi_fft))\n",
        "    phi_fft_transform = fftn(phi_cpu_np_array.astype(np.float32))\n",
        "    power_spectrum_raw_data = np.abs(phi_fft_transform)**2 \n",
        "    del phi_fft_transform\n",
        "    gc.collect()\n",
        "    \n",
        "    # For discrete FFT: if F = FFT(f), then IFFT(F) = f * N_total.\n",
        "    # If P(k) = |FFT(phi)|^2 / N^6, then IFFT(P(k)) should result in something scaled by N^3 for phi^2 quantities.\n",
        "    # The current approach `ifftn(power_spectrum_raw_data).real / (N_grid_param**3)` is standard for correlation function from *power spectrum density*.\n",
        "    correlation_func_raw_data = ifftn(power_spectrum_raw_data).real / (N_grid_param**3) \n",
        "    del power_spectrum_raw_data\n",
        "    gc.collect()\n",
        "\n",
        "    # Create r-space grid for binning (shifted for ξ(0) at center)\n",
        "    indices_shifted = np.fft.ifftshift(np.arange(N_grid_param)) - (N_grid_param // 2) # Centered indices\n",
        "    rx_coords = indices_shifted * dx_val_param\n",
        "    ry_coords = indices_shifted * dx_val_param\n",
        "    rz_coords = indices_shifted * dx_val_param\n",
        "    rxx_mesh, ryy_mesh, rzz_mesh = np.meshgrid(rx_coords, ry_coords, rz_coords, indexing='ij', sparse=True)\n",
        "    r_magnitude_values = np.sqrt(rxx_mesh**2 + ryy_mesh**2 + rzz_mesh**2)\n",
        "    del rx_coords, ry_coords, rz_coords, rxx_mesh, ryy_mesh, rzz_mesh\n",
        "    gc.collect()\n",
        "\n",
        "    r_bins_def = np.linspace(0, L_box_param / 2, 50) # Bins up to half the box size\n",
        "    corr_binned_values, _ = np.histogram(\n",
        "        r_magnitude_values.ravel(), bins=r_bins_def,\n",
        "        weights=correlation_func_raw_data.ravel()\n",
        "    )\n",
        "    counts_in_bins, _ = np.histogram(r_magnitude_values.ravel(), bins=r_bins_def)\n",
        "    corr_binned_final = np.divide(corr_binned_values, counts_in_bins, out=np.zeros_like(corr_binned_values), where=counts_in_bins!=0)\n",
        "    r_bin_centers_final = (r_bins_def[:-1] + r_bins_def[1:]) / 2\n",
        "    \n",
        "    del r_magnitude_values, correlation_func_raw_data, counts_in_bins\n",
        "    gc.collect()\n",
        "    return r_bin_centers_final, corr_binned_final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simulation-orchestration-lss"
      },
      "source": [
        "<h2>Simulation Orchestration for EFM LSS (A100)</h2>\n",
        "<p>This section sets up the simulation loop, handles initial conditions, and records diagnostics. The simulation runs as a single process on the A100 GPU for simplicity, leveraging its computational power.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main-simulation-lss"
      },
      "source": [
        "def run_lss_simulation(config: dict, device: torch.device, latest_checkpoint_file: str = None):\n",
        "    \"\"\"Main simulation loop for EFM LSS.\"\"\"\n",
        "    print(f\"Initializing fields for EFM LSS simulation ({config['run_id']}) on {device}...\")\n",
        "    \n",
        "    # Set seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    phi = None\n",
        "    phi_dot = None\n",
        "    hist_idx = 0\n",
        "    energy_history = np.zeros(config['T_steps'] // config['history_every_n_steps'] + 1, dtype=np.float64)\n",
        "    density_norm_history = np.zeros(config['T_steps'] // config['history_every_n_steps'] + 1, dtype=np.float64)\n",
        "    start_step = 0\n",
        "\n",
        "    if latest_checkpoint_file and os.path.exists(latest_checkpoint_file):\n",
        "        print(f\"Resuming from checkpoint: {latest_checkpoint_file}\")\n",
        "        try:\n",
        "            checkpoint = np.load(latest_checkpoint_file, allow_pickle=True)\n",
        "            phi = torch.from_numpy(checkpoint['phi_r_cpu']).to(device, dtype=torch.float16)\n",
        "            phi_dot = torch.from_numpy(checkpoint['phi_dot_r_cpu']).to(device, dtype=torch.float16)\n",
        "            start_step = checkpoint['last_step'].item() + 1\n",
        "            loaded_energy_hist = checkpoint['energy_history']\n",
        "            loaded_density_norm_hist = checkpoint['density_norm_history']\n",
        "\n",
        "            # Ensure history arrays are correctly sized if resuming from a partial run\n",
        "            if len(loaded_energy_hist) > len(energy_history):\n",
        "                energy_history = np.resize(energy_history, len(loaded_energy_hist) + (config['T_steps'] - start_step) // config['history_every_n_steps'] + 1)\n",
        "                density_norm_history = np.resize(density_norm_history, len(loaded_density_norm_hist) + (config['T_steps'] - start_step) // config['history_every_n_steps'] + 1)\n",
        "            \n",
        "            energy_history[:len(loaded_energy_hist)] = loaded_energy_hist\n",
        "            density_norm_history[:len(loaded_density_norm_hist)] = loaded_density_norm_hist\n",
        "            hist_idx = len(loaded_energy_hist) # Set hist_idx to the next available slot\n",
        "\n",
        "            print(f\"Resumed from step {start_step}. Last recorded energy: {energy_history[hist_idx-1] if hist_idx > 0 else 'N/A'}\")\n",
        "            del checkpoint, loaded_energy_hist, loaded_density_norm_hist\n",
        "            gc.collect()\n",
        "            torch.cuda.empty_cache()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}. Starting from scratch.\")\n",
        "            phi = None # Reset to trigger new initialization\n",
        "\n",
        "    if phi is None: # Initialize if not resumed from checkpoint\n",
        "        print(\"No valid checkpoint found or error loading. Starting simulation from scratch.\")\n",
        "        # Create spatial grid for initial conditions\n",
        "        x_coords = np.linspace(-config['L_sim_unit']/2, config['L_sim_unit']/2, config['N'], dtype=np.float32)\n",
        "        X, Y, Z = np.meshgrid(x_coords, x_coords, x_coords, indexing='ij')\n",
        "\n",
        "        # Initial conditions: Seeded sinusoidal perturbations + background noise\n",
        "        # φ_initial = A_seed * (sin(k_primary * X) + sin(k_secondary * Y) + cos(k_primary * Z)) + A_noise * random_noise\n",
        "        # We use different axes for primary/secondary seeds to make features distinct\n",
        "        seeded_modes_field = config['seeded_perturbation_amplitude'] * (\n",
        "            np.sin(config['k_seed_primary'] * X) + \n",
        "            np.sin(config['k_seed_secondary'] * Y) + \n",
        "            np.cos(config['k_seed_primary'] * Z) \n",
        "        )\n",
        "        # Add uniform random noise for the background\n",
        "        random_background_noise = config['background_noise_amplitude'] * (np.random.rand(config['N'], config['N'], config['N']) - 0.5)\n",
        "\n",
        "        initial_phi_np = seeded_modes_field + random_background_noise\n",
        "        phi = torch.from_numpy(initial_phi_np.astype(np.float16)).to(device, dtype=torch.float16)\n",
        "        phi_dot = torch.zeros_like(phi, dtype=torch.float16, device=device)\n",
        "\n",
        "        print(f\"Initial field created. Max amplitude: {phi.abs().max().item():.2e}\")\n",
        "\n",
        "        # Record initial state diagnostics (only if starting from scratch, otherwise loaded from checkpoint)\n",
        "        energy_history[hist_idx] = compute_total_energy_lss(phi, phi_dot, config['m_sim_unit_inv']**2, config['g_sim'], config['eta_sim'], config['dx_sim_unit'], config['c_sim_unit']**2)\n",
        "        density_norm_history[hist_idx] = torch.sum(phi.to(torch.float32)**2).item() * config['k_efm_gravity_coupling']\n",
        "        print(f\"Initial State: Energy={energy_history[hist_idx]:.4g}, Density Norm={density_norm_history[hist_idx]:.4g}\")\n",
        "        hist_idx += 1\n",
        "\n",
        "    # Instantiate the EFM LSS Module (always instantiate after phi/phi_dot are on device)\n",
        "    efm_model = EFMLSSModule(\n",
        "        dx=config['dx_sim_unit'], \n",
        "        m_sq=config['m_sim_unit_inv']**2,\n",
        "        g=config['g_sim'],\n",
        "        eta=config['eta_sim'],\n",
        "        k_gravity=config['k_efm_gravity_coupling'],\n",
        "        G_gravity=config['G_sim_unit'], \n",
        "        c_sq=config['c_sim_unit']**2, \n",
        "        alpha_param=config['alpha_sim'], \n",
        "        delta_param=config['delta_sim'] \n",
        "    ).to(device)\n",
        "    efm_model.eval() # No training, so eval mode is appropriate\n",
        "\n",
        "    sim_start_time = time.time()\n",
        "    numerical_error = False\n",
        "\n",
        "    for t_step in tqdm(range(start_step, config['T_steps']), desc=f\"LSS Sim ({config['run_id']})\"):\n",
        "        # Check for numerical instability before update\n",
        "        if torch.any(torch.isinf(phi)) or torch.any(torch.isnan(phi)) or \\\n",
        "           torch.any(torch.isinf(phi_dot)) or torch.any(torch.isnan(phi_dot)):\n",
        "            print(f\"\\nERROR: NaN/Inf detected in fields BEFORE step {t_step + 1}! Stopping.\")\n",
        "            numerical_error = True\n",
        "            break\n",
        "\n",
        "        # RK4 update\n",
        "        phi, phi_dot = update_phi_rk4_lss(phi, phi_dot, config['dt_sim_unit'], efm_model)\n",
        "\n",
        "        # Check for numerical instability after update\n",
        "        if torch.any(torch.isinf(phi)) or torch.any(torch.isnan(phi)):\n",
        "            print(f\"\\nERROR: NaN/Inf detected in phi AFTER step {t_step + 1}! Stopping.\")\n",
        "            numerical_error = True\n",
        "            break\n",
        "\n",
        "        # Record diagnostics periodically\n",
        "        if (t_step + 1) % config['history_every_n_steps'] == 0:\n",
        "            # Ensure history arrays are large enough to append new data\n",
        "            if hist_idx >= len(energy_history):\n",
        "                energy_history = np.resize(energy_history, hist_idx + 1)\n",
        "                density_norm_history = np.resize(density_norm_history, hist_idx + 1)\n",
        "\n",
        "            current_energy = compute_total_energy_lss(phi, phi_dot, efm_model.m_sq, efm_model.g, efm_model.eta, efm_model.dx, efm_model.c_sq)\n",
        "            current_density_norm = torch.sum(phi.to(torch.float32)**2).item() * config['k_efm_gravity_coupling']\n",
        "\n",
        "            energy_history[hist_idx] = current_energy\n",
        "            density_norm_history[hist_idx] = current_density_norm\n",
        "            \n",
        "            tqdm.write(f\"Step {t_step+1}: E={current_energy:.3e}, DN={current_density_norm:.3e}\")\n",
        "            if np.isnan(current_energy) or np.isinf(current_energy):\n",
        "                print(f\"Instability: Energy is NaN/Inf at step {t_step+1}. Stopping.\")\n",
        "                numerical_error = True\n",
        "                break\n",
        "            hist_idx += 1\n",
        "        \n",
        "        # Save intermediate checkpoint\n",
        "        if (t_step + 1) % config['checkpoint_every_n_steps'] == 0 and (t_step + 1) < config['T_steps']:\n",
        "            intermediate_ckpt_file = os.path.join(checkpoint_path_lss, f\"intermediate_CKPT_{config['run_id']}_step_{t_step+1}.npz\")\n",
        "            try:\n",
        "                np.savez_compressed(intermediate_ckpt_file,\n",
        "                                    phi_r_cpu=phi.cpu().numpy(),\n",
        "                                    phi_dot_r_cpu=phi_dot.cpu().numpy(),\n",
        "                                    last_step=t_step,\n",
        "                                    config_lss_saved=config,\n",
        "                                    energy_history=energy_history[:hist_idx],\n",
        "                                    density_norm_history=density_norm_history[:hist_idx])\n",
        "                print(f\"Checkpoint saved at step {t_step+1} to {intermediate_ckpt_file}\")\n",
        "            except Exception as e_save:\n",
        "                print(f\"Error saving intermediate LSS checkpoint: {e_save}\")\n",
        "\n",
        "    sim_duration = time.time() - sim_start_time\n",
        "    print(f\"Simulation finished in {sim_duration:.2f} seconds.\")\n",
        "    if numerical_error: print(\"Simulation stopped due to numerical error.\")\n",
        "\n",
        "    # Trim history arrays to actual recorded length before saving final checkpoint\n",
        "    energy_history_final = energy_history[:hist_idx]\n",
        "    density_norm_history_final = density_norm_history[:hist_idx]\n",
        "\n",
        "    # Save final state and history\n",
        "    final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    final_data_filename = os.path.join(data_path_lss, f\"FINAL_LSS_DATA_{config['run_id']}_{final_timestamp}.npz\")\n",
        "    np.savez_compressed(final_data_filename,\n",
        "                        phi_final_cpu=phi.cpu().numpy(),\n",
        "                        phi_dot_final_cpu=phi_dot.cpu().numpy(),\n",
        "                        energy_history=energy_history_final,\n",
        "                        density_norm_history=density_norm_history_final,\n",
        "                        config_lss=config,\n",
        "                        sim_had_numerical_error=numerical_error)\n",
        "    print(f\"Final LSS simulation data saved to {final_data_filename}\")\n",
        "\n",
        "    del phi, phi_dot, efm_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return final_data_filename\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis-plotting-lss"
      },
      "source": [
        "<h2>Analysis and Plotting</h2>\n",
        "<p>After the simulation, load the saved data to visualize the results, focusing on:</p>\n",
        "<ul>\n",
        "<li><strong>Field Energy Evolution:</strong> To observe stability and trends over the long run.</li>\n",
        "<li><strong>Density Norm Evolution:</strong> <code>kφ²</code> represents mass-energy density in EFM. Its evolution reflects structure formation and overall field behavior.</li>\n",
        "<li><strong>Power Spectrum P(k):</strong> To identify characteristic clustering scales in k-space.</li>\n",
        "<li><strong>Correlation Function ξ(r):</strong> To identify characteristic clustering scales in real space.</li>\n",
        "<li><strong>Visualization of Final Field Slice:</strong> To qualitatively observe emergent density perturbations.</li>\n",
        "</ul>\n",
        "<h3>Physical Scaling for Interpretation (Post-Simulation)</h3>\n",
        "<p>The simulation runs in dimensionless units. To interpret the results in physical units (Mpc), we establish a scaling factor by matching EFM's theoretically predicted primary LSS scale (628 Mpc) to the largest emergent dimensionless scale from the simulation (e.g., the prominent peak in P(k) or ξ(r)).</p>\n",
        "<p>If the simulation yields a primary clustering scale at <code>r_sim_peak</code> (dimensionless) that should correspond to <code>r_phys_peak = 628 Mpc</code> [4], then the length scaling factor <code>S_L = r_phys_peak / r_sim_peak</code>. All other dimensionless lengths can then be converted by multiplying by <code>S_L</code>.</p>\n",
        "<p>Similarly, for k-space, if <code>k_sim_peak</code> corresponds to <code>k_phys_peak = 2π / 628 Mpc⁻¹</code>, then <code>S_k = k_phys_peak / k_sim_peak</code>.</p>\n",
        "<p>This approach ensures that the physical interpretation is grounded in EFM's fundamental predictions for LSS scales.</p>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "plot-lss-results"
      },
      "source": [
        "def plot_lss_results(data_file_path: str):\n",
        "    print(f\"Loading data for plotting from: {data_file_path}\")\n",
        "    try:\n",
        "        data = np.load(data_file_path, allow_pickle=True)\n",
        "        energy_history = data['energy_history']\n",
        "        density_norm_history = data['density_norm_history']\n",
        "        phi_final_cpu = data['phi_final_cpu'] # Load the final phi field for visualization\n",
        "        config = data['config_lss'].item() # Ensure config is a dict\n",
        "        sim_had_numerical_error = data['sim_had_numerical_error'].item()\n",
        "        print(\"Data loaded successfully.\")\n",
        "        if sim_had_numerical_error: print(\"WARNING: Simulation previously encountered numerical error.\")\n",
        "\n",
        "        num_hist_points = len(energy_history)\n",
        "        time_sim_unit = np.arange(num_hist_points) * config['history_every_n_steps'] * config['dt_sim_unit']\n",
        "\n",
        "        plt.figure(figsize=(14, 6))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(time_sim_unit, energy_history, marker='.', linestyle='-')\n",
        "        plt.title('Total Field Energy Evolution (Dimensionless Units)')\n",
        "        plt.xlabel('Time (Simulation Units)')\n",
        "        plt.ylabel('Energy (Dimensionless Units)')\n",
        "        plt.grid(True)\n",
        "        plt.ticklabel_format(style='sci', axis='y', scilimits=(-3,3), useMathText=True)\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(time_sim_unit, density_norm_history, marker='.', linestyle='-')\n",
        "        plt.title('Density Norm (kφ²) Evolution (Dimensionless Units)')\n",
        "        plt.xlabel('Time (Simulation Units)')\n",
        "        plt.ylabel('Density Norm (Dimensionless Units)')\n",
        "        plt.grid(True)\n",
        "        plt.ticklabel_format(style='sci', axis='y', scilimits=(-3,3), useMathText=True)\n",
        "        \n",
        "        plt.suptitle(f\"EFM LSS Simulation Results ({config['run_id']})\", fontsize=16, y=1.04)\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.98])\n",
        "        plot_filename_evo = os.path.join(data_path_lss, f\"lss_evo_results_{config['run_id']}.png\")\n",
        "        plt.savefig(plot_filename_evo)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"\\n--- Final Simulation Properties ({config['run_id']}) ---\\n\" \\\n",
        "              f\"Final Time Simulated: {time_sim_unit[-1]:.4g} Dimensionless Units\\n\" \\\n",
        "              f\"Final Field Energy: {energy_history[-1]:.4g}\\n\" \\\n",
        "              f\"Final Density Norm (kφ²): {density_norm_history[-1]:.4g}\\n\")\n",
        "\n",
        "        # --- Power Spectrum and Correlation Function Analysis (Dimensionless) ---\n",
        "        # CORRECTED: Use raw string prefix r for print statements with LaTeX math\n",
        "        print(r\"\\nComputing P(k) and xi(r) for LSS final state (dimensionless units)...\\n\")\n",
        "        \n",
        "        k_min_plot_sim = 2 * np.pi / config['L_sim_unit'] * 0.5 # Smallest k to plot (half the fundamental mode)\n",
        "        k_max_plot_sim = np.pi / config['dx_sim_unit'] * 0.9   # Largest k to plot (90% of Nyquist frequency)\n",
        "        \n",
        "        k_bins_sim, pk_vals_sim = compute_power_spectrum_lss(\n",
        "            phi_final_cpu, k_val_range=[k_min_plot_sim, k_max_plot_sim],\n",
        "            dx_val_param=config['dx_sim_unit'], N_grid_param=config['N']\n",
        "        )\n",
        "        r_bins_sim, xi_vals_sim = compute_correlation_function_lss(\n",
        "            phi_final_cpu, dx_val_param=config['dx_sim_unit'],\n",
        "            N_grid_param=config['N'], L_box_param=config['L_sim_unit']\n",
        "        )\n",
        "\n",
        "        plt.figure(figsize=(16,6))\n",
        "        \n",
        "        plt.subplot(1,2,1)\n",
        "        plt.loglog(k_bins_sim, pk_vals_sim)\n",
        "        plt.title('LSS Power Spectrum P(k) (Dimensionless Units)')\n",
        "        plt.xlabel('k (Dimensionless Units)')\n",
        "        plt.ylabel('P(k) (Dimensionless Units)')\n",
        "        plt.grid(True, which='both', linestyle=':')\n",
        "        # Use r-string for label to correctly render LaTeX and prevent unicode escape errors\n",
        "        plt.axvline(config['k_seed_primary'], color='orange', linestyle='--', label=r\"Seeded k_primary ({:.2f})\".format(config['k_seed_primary']))\n",
        "        plt.axvline(config['k_seed_secondary'], color='purple', linestyle='--', label=r\"Seeded k_secondary ({:.2f})\".format(config['k_seed_secondary']))\n",
        "        plt.legend()\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(r_bins_sim, xi_vals_sim)\n",
        "        # Use r-string for title/labels to correctly render LaTeX\n",
        "        plt.title(r'LSS Correlation Function $\\xi$(r) (Dimensionless Units)')\n",
        "        plt.xlabel('r (Dimensionless Units)')\n",
        "        plt.ylabel(r'$\\xi$(r) (Dimensionless Units)') \n",
        "        plt.grid(True, linestyle=':')\n",
        "        plt.axhline(0, color='black', linewidth=0.5)\n",
        "        # Use r-string for labels to correctly render LaTeX\n",
        "        plt.axvline(2*np.pi/config['k_seed_primary'], color='orange', linestyle='--', label=r\"Seeded $\\lambda_1$ ({:.2f})\".format(2*np.pi/config['k_seed_primary']))\n",
        "        plt.axvline(2*np.pi/config['k_seed_secondary'], color='purple', linestyle='--', label=r\"Seeded $\\lambda_2$ ({:.2f})\".format(2*np.pi/config['k_seed_secondary']))\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.suptitle(f\"EFM LSS Observables (Dimensionless, {config['run_id']})\", fontsize=14, y=1.02)\n",
        "        plot_filename_obs = os.path.join(data_path_lss, f\"lss_observables_{config['run_id']}.png\")\n",
        "        plt.savefig(plot_filename_obs)\n",
        "        plt.show()\n",
        "        plt.close()\n",
        "\n",
        "        # --- Identify and Print Emergent Dimensionless Scales ---\n",
        "        # Use r-string for print statements with LaTeX math\n",
        "        print(r\"\\n--- Emergent Dimensionless Scales from Simulation ---\")\n",
        "        peak_k_idx = np.argmax(pk_vals_sim)\n",
        "        emergent_k_peak = k_bins_sim[peak_k_idx]\n",
        "        emergent_lambda_peak = 2*np.pi / emergent_k_peak if emergent_k_peak > 0 else np.nan\n",
        "        print(r\"Highest Power Spectrum peak at k_sim: {:.3f} (Dimless Units), corresponding $\\lambda_{{sim}}$: {:.3f} (Dimless Units)\".format(emergent_k_peak, emergent_lambda_peak))\n",
        "        \n",
        "        emergent_r_peak = \"N/A\"\n",
        "        if len(xi_vals_sim[1:]) > 0 and np.max(np.abs(xi_vals_sim[1:])) > 1e-10: # Check for positive correlation peak after r=0\n",
        "            max_val_idx = np.argmax(xi_vals_sim[1:]) + 1 # Index of max value after r=0\n",
        "            emergent_r_peak = r_bins_sim[max_val_idx]\n",
        "            print(r\"Highest Correlation Function peak at r_sim: {:.3f} (Dimless Units)\".format(emergent_r_peak))\n",
        "        else:\n",
        "            print(r\"No significant correlation peak found in $\\xi$(r).\")\n",
        "\n",
        "        # --- Physical Interpretation (based on EFM theoretical predictions) ---\n",
        "        # Use r-string for print statements with LaTeX math\n",
        "        print(r\"\\n--- Physical Interpretation (based on EFM theoretical predictions) ---\")\n",
        "        EFM_PRIMARY_LSS_Mpc = 628.0 # Mpc [4]\n",
        "        EFM_SECONDARY_LSS_Mpc = 147.0 # Mpc (BAO-like) [4]\n",
        "\n",
        "        # We assume the largest emergent scale corresponds to EFM_PRIMARY_LSS_Mpc\n",
        "        # If a prominent peak was found in xi(r):\n",
        "        if isinstance(emergent_r_peak, float) and not np.isnan(emergent_r_peak) and emergent_r_peak > 0:\n",
        "            scaling_factor_L = EFM_PRIMARY_LSS_Mpc / emergent_r_peak\n",
        "            print(f\"Scaling factor (1 dimensionless unit = X Mpc) derived from {EFM_PRIMARY_LSS_Mpc} Mpc primary scale: {scaling_factor_L:.2e} Mpc/unit\")\n",
        "            \n",
        "            # Convert seeded wavelengths to physical units using this factor\n",
        "            physical_primary_lambda_seeded = (config['L_sim_unit'] / 2.0) * scaling_factor_L\n",
        "            physical_secondary_lambda_seeded = (config['L_sim_unit'] / 5.0) * scaling_factor_L\n",
        "            print(f\"Seeded primary wavelength (dimless {config['L_sim_unit'] / 2.0:.2f}) -> Physical: {physical_primary_lambda_seeded:.2f} Mpc (Target: ~{EFM_PRIMARY_LSS_Mpc} Mpc)\")\n",
        "            print(f\"Seeded secondary wavelength (dimless {config['L_sim_unit'] / 5.0:.2f}) -> Physical: {physical_secondary_lambda_seeded:.2f} Mpc (Target: ~{EFM_SECONDARY_LSS_Mpc} Mpc)\")\n",
        "\n",
        "            # Approx physical time duration calculation\n",
        "            c_si_m_s = 299792458.0 # m/s\n",
        "            Mpc_to_m = 3.08567758e22 # meters per Mpc\n",
        "            s_to_yr = 1.0 / (3.15576e7) # years per second\n",
        "\n",
        "            approx_physical_time_per_sim_unit = (scaling_factor_L * Mpc_to_m / c_si_m_s) / s_to_yr # in years per dimensionless time unit\n",
        "            approx_total_physical_time_gyr = time_sim_unit[-1] * approx_physical_time_per_sim_unit / 1e9\n",
        "\n",
        "            print(f\"Physical box size L: {config['L_sim_unit'] * scaling_factor_L:.2f} Mpc\")\n",
        "            print(f\"Physical dx: {config['dx_sim_unit'] * scaling_factor_L:.4g} Mpc\")\n",
        "            print(f\"Approx. total physical time simulated: {approx_total_physical_time_gyr:.4g} Gyr\")\n",
        "\n",
        "            print(r\"\\n**Interpretation:** This simulation aims to provide quantitative evidence for EFM's LSS formation. If clear peaks are found at the seeded dimensionless scales, it means the EFM dynamics can amplify these initial perturbations into coherent structures. The derived scaling factor directly connects the simulated dimensionless universe to our physical universe, allowing EFM to predict the emergence of 147 Mpc and 628 Mpc scales without dark matter.\")\n",
        "            print(r\"This is a critical step towards resolving the Hubble tension and validating EFM's unified cosmological framework.\")\n",
        "\n",
        "        else:\n",
        "            print(r\"Could not determine a prominent primary dimensionless peak for physical scaling (xi(r) peak was not found). Please examine raw P(k) and xi(r) plots for subtle features.\")\n",
        "\n",
        "        # --- Visualization of Final Field State (2D Slice) ---\n",
        "        # Use r-string for print statements with LaTeX math\n",
        "        print(r\"\\n--- Visualization of Final Field State (2D Slice) ---\")\n",
        "        if phi_final_cpu.ndim == 3 and phi_final_cpu.shape[0] > 0:\n",
        "            N_grid = phi_final_cpu.shape[0]\n",
        "            center_slice_idx = N_grid // 2\n",
        "            phi_slice = phi_final_cpu[:, :, center_slice_idx] # XY-plane slice\n",
        "            \n",
        "            L_sim_unit = config['L_sim_unit']\n",
        "            extent = [-L_sim_unit/2, L_sim_unit/2, -L_sim_unit/2, L_sim_unit/2]\n",
        "\n",
        "            plt.figure(figsize=(8, 7))\n",
        "            plt.imshow(phi_slice.T, origin='lower', cmap='viridis', extent=extent) # .T for correct orientation\n",
        "            plt.colorbar(label='Field Amplitude (Dimensionless)')\n",
        "            plt.title(f'Final Field State (φ) - Central XY Slice (t={time_sim_unit[-1]:.4g} Sim Units)')\n",
        "            plt.xlabel('Dimensionless Spatial Unit')\n",
        "            plt.ylabel('Dimensionless Spatial Unit')\n",
        "            plt.grid(True)\n",
        "            plot_filename_phi_slice = os.path.join(data_path_lss, f\"lss_phi_slice_{config['run_id']}.png\")\n",
        "            plt.savefig(plot_filename_phi_slice)\n",
        "            plt.show()\n",
        "            plt.close()\n",
        "\n",
        "            print(r\"\\n**EFM Interpretation of Final Field State for Structure Formation [4, Compendium p.260]:**\")\n",
        "            print(r\"The visualization of the final φ field state (a 2D slice) provides qualitative evidence of emergent density perturbations. After being seeded with specific long-wavelength modes and evolving under EFM dynamics, the field should show patterns that are the precursors to large-scale cosmic structures. The aim is to visibly demonstrate the 'Fluxonic Clustering' mechanism, where the field self-organizes without the need for dark matter halos. The clarity and prominence of these patterns will indicate the success of the chosen parameters and simulation duration in amplifying the initial seeded modes.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Final phi field data not available or not in expected 3D format for slice visualization.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error during plotting/analysis: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # --- Colab specific setup --- \n",
        "    # This cell is specific to Google Colab environments\n",
        "    # Define paths for checkpoints and data/plots - Adjusted for Google Drive\n",
        "    checkpoint_path_lss = '/content/drive/My Drive/EFM_Simulations/checkpoints/LSS_DIMLESS_A100_v2/'\n",
        "    data_path_lss = '/content/drive/My Drive/EFM_Simulations/data/LSS_DIMLESS_A100_v2/'\n",
        "\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "        os.makedirs(checkpoint_path_lss, exist_ok=True)\n",
        "        os.makedirs(data_path_lss, exist_ok=True)\n",
        "    except ImportError:\n",
        "        print(\"Not in Google Colab environment. Skipping Google Drive mount.\")\n",
        "        # Define paths for checkpoints and data/plots for local execution\n",
        "        checkpoint_path_lss = './EFM_Simulations/checkpoints/LSS_DIMLESS_A100_v2/'\n",
        "        data_path_lss = './EFM_Simulations/data/LSS_DIMLESS_A100_v2/'\n",
        "        os.makedirs(checkpoint_path_lss, exist_ok=True)\n",
        "        os.makedirs(data_path_lss, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}. Please ensure you're logged in and have granted permissions.\")\n",
        "        # Fallback to local paths if Google Drive mount fails\n",
        "        checkpoint_path_lss = './EFM_Simulations/checkpoints/LSS_DIMLESS_A100_v2/'\n",
        "        data_path_lss = './EFM_Simulations/data/LSS_DIMLESS_A100_v2/'\n",
        "        os.makedirs(checkpoint_path_lss, exist_ok=True)\n",
        "        os.makedirs(data_path_lss, exist_ok=True)\n",
        "\n",
        "    print(f\"LSS Checkpoints will be saved to: {checkpoint_path_lss}\")\n",
        "    print(f\"LSS Data/Plots will be saved to: {data_path_lss}\")\n",
        "\n",
        "    # --- Your actual config_lss_run definitions should be here --- \n",
        "    # This ensures config_lss_run is defined before run_lss_simulation is called\n",
        "    config_lss_run = {\n",
        "        'N': 450,\n",
        "        'L_sim_unit': 10.0,\n",
        "        'dx_sim_unit': 10.0 / 450,\n",
        "        'c_sim_unit': 1.0,\n",
        "        'dt_cfl_factor': 0.001,\n",
        "        'dt_sim_unit': 0.001 * (10.0 / 450) / 1.0,\n",
        "        'T_steps': 200000,\n",
        "        'm_sim_unit_inv': 1.0,\n",
        "        'g_sim': 0.1,\n",
        "        'eta_sim': 0.01,\n",
        "        'k_efm_gravity_coupling': 0.005,\n",
        "        'G_sim_unit': 1.0,\n",
        "        'alpha_sim': 0.7,\n",
        "        'delta_sim': 0.0002,\n",
        "        'seeded_perturbation_amplitude': 1.0e-3,\n",
        "        'background_noise_amplitude': 1.0e-6,\n",
        "        'k_seed_primary': 2 * np.pi / (10.0 / 2.0), # L/2 wavelength -> k=2pi/(L/2)\n",
        "        'k_seed_secondary': 2 * np.pi / (10.0 / 5.0), # L/5 wavelength -> k=2pi/(L/5)\n",
        "        'run_id': r\"LSS_N450_T200000_m1.0e+00_g1.0e-01_eta1.0e-02_k5.0e-03_G1.0e+00_alpha7.0e-01_delta2.0e-04_CFL1.0e-03_A100_DIMLESS_LSS_v3_Seeded\",\n",
        "        'history_every_n_steps': 1000,\n",
        "        'checkpoint_every_n_steps': 5000,\n",
        "    }\n",
        "    print(f\"\\n--- EFM LSS Simulation Configuration ({config_lss_run['run_id']}) ---\\n\")\n",
        "    for key, value in config_lss_run.items():\n",
        "        if isinstance(value, (float, np.float32, np.float64)): \n",
        "            print(f\"{key}: {value:.4g}\")\n",
        "        else:\n",
        "            print(f\"{key}: {value}\")\n",
        "    print(r\"\\n--- Physical Scaling (for interpretation of dimensionless results) ---\\n\")\n",
        "    print(r\"The simulation runs in dimensionless units. Physical scales (Mpc, Gyr) will be derived post-simulation by matching emergent peaks to EFM's predicted 147 Mpc and 628 Mpc scales.\")\n",
        "    print(f\"Dimensionless L: {config_lss_run['L_sim_unit']} units, dx: {config_lss_run['dx_sim_unit']:.4g} units\")\n",
        "    print(f\"Dimensionless dt: {config_lss_run['dt_sim_unit']:.4g} units\")\n",
        "    print(f\"Dimensionless k_seed_primary: {config_lss_run['k_seed_primary']:.4g}\")\n",
        "    print(f\"Dimensionless k_seed_secondary: {config_lss_run['k_seed_secondary']:.4g}\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        main_device = torch.device('cuda:0')\n",
        "    else:\n",
        "        main_device = torch.device('cpu')\n",
        "    \n",
        "    # Check for existing checkpoint to resume if needed\n",
        "    latest_checkpoint_file = None\n",
        "    checkpoint_files = sorted(glob.glob(os.path.join(checkpoint_path_lss, f\"intermediate_CKPT_{config_lss_run['run_id']}_step_*.npz\")),\\\n",
        "                              key=lambda f: int(os.path.basename(f).split('_step_')[1].split('.npz')[0]), reverse=True)\n",
        "    if checkpoint_files:\n",
        "        latest_checkpoint_file = checkpoint_files[0]\n",
        "        print(f\"\\nFound latest checkpoint: {latest_checkpoint_file}. Attempting to resume.\\n\")\n",
        "    else:\n",
        "        print(r\"\\nNo existing checkpoint found. Starting simulation from scratch.\\n\")\n",
        "\n",
        "    final_data_file = run_lss_simulation(config_lss_run, main_device, latest_checkpoint_file)\n",
        "    \n",
        "    # Plot the results\\n",
        "    if final_data_file:\\n",
        "        plot_lss_results(final_data_file)\\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}