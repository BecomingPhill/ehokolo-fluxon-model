{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EFM Density States and Clustering Scale Validation\n\nThis notebook performs a high-fidelity simulation to validate the Ehokolo Fluxon Model (EFM) density states and clustering scales on a CoCalc server with 4x NVIDIA H100 GPUs (80 GB VRAM each, total 320 GB), 104 vCPUs, 936 GB RAM, and a 1 TB SSD, running the Colab Docker image. The simulation reproduces the BAO scale (~147 Mpc) through solitonic dynamics and allows larger scales (~628 Mpc) to emerge naturally. It targets a runtime of ~1.3-1.5 hours for a 500^3 grid with 50k steps, optimized for 4-GPU execution with mixed precision, larger chunks, and minimal synchronization, keeping VRAM usage below 200 GB total and RAM below 800 GB. Progress is tracked with `tqdm` bars, with silent chunk processing to minimize screen clutter and robust filesystem error handling.\n\n## Objectives\n- Run a simulation on a 500^3 grid (1000 Mpc box) to validate S/T state clustering scales.\n- Derive the BAO scale (~147 Mpc) using solitonic dynamics with a tuned mass parameter m.\n- Use Gaussian noise as the initial condition to let clustering scales emerge naturally.\n- Compute the power spectrum and correlation function to identify clustering scales (147 Mpc, 628 Mpc).\n- Validate against DESI BAO data (147.09 ± 0.26 Mpc) and check for the 628 Mpc scale.\n- Use local storage (1 TB SSD) for all data.\n\n## Hardware\n- GPUs: 4x NVIDIA H100 (80 GB VRAM each, total 320 GB)\n- System RAM: 936 GB\n- Storage: 1 TB SSD\n- Environment: CoCalc with Colab Docker image, PyTorch 2.0+, CUDA 12.x\n\n## Setup Instructions\n1. Ensure the CoCalc project runs on the specified server (4x H100).\n2. Execute all cells sequentially to run the simulation, or skip to the Compute Final Observables cell to analyze an existing checkpoint.\n3. Monitor VRAM (<200 GB total), RAM (<800 GB), and SSD (<1 TB) via `tqdm` postfix.\n4. Outputs are saved to local directories (`~/EFM_checkpoints/` and `~/EFM_data/`).\n5. Expect a single `tqdm` progress bar for the main simulation, with silent chunk processing and filesystem error resilience."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Environment Setup\n\nSet up environment variables, install dependencies, create local directories, and verify GPUs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import subprocess\n",
        "import torch\n",
        "import gc\n",
        "import psutil\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Set environment variable to reduce memory fragmentation\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Clear GPU memory\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Function to check if a package is installed and meets version requirements\n",
        "def check_package(package_name, min_version=None):\n",
        "    try:\n",
        "        result = subprocess.run(['pip', 'show', package_name], capture_output=True, text=True)\n",
        "        if result.returncode != 0:\n",
        "            return False, None\n",
        "        version_line = [line for line in result.stdout.split('\\n') if line.startswith('Version: ')][0]\n",
        "        version = version_line.split(': ')[1]\n",
        "        if min_version:\n",
        "            from pkg_resources import parse_version\n",
        "            if parse_version(version) < parse_version(min_version):\n",
        "                return False, version\n",
        "        return True, version\n",
        "    except Exception:\n",
        "        return False, None\n",
        "\n",
        "# Install required packages if missing\n",
        "required_packages = [\n",
        "    ('torch', '2.0.0'),  # Ensure CUDA support for H100\n",
        "    ('numpy', None),\n",
        "    ('tqdm', None),\n",
        "    ('psutil', None),\n",
        "    ('scipy', None),\n",
        "    ('matplotlib', None)\n",
        "]\n",
        "for pkg_name, min_version in tqdm(required_packages, desc=\"Checking packages\", unit=\"package\"):\n",
        "    with tqdm(total=1, desc=f\"Checking {pkg_name}\", leave=False) as pbar:\n",
        "        installed, version = check_package(pkg_name, min_version)\n",
        "        if installed:\n",
        "            print(f\"{pkg_name} (version {version}) is already installed.\")\n",
        "        else:\n",
        "            print(f\"Installing {pkg_name}...\")\n",
        "            subprocess.run(['pip', 'install', pkg_name, '--quiet'])\n",
        "            if pkg_name == 'torch' and min_version:\n",
        "                subprocess.run(['pip', 'install', f'torch>={min_version}', '--quiet'])\n",
        "        pbar.update(1)\n",
        "\n",
        "# Import libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "from scipy.fft import fftn, fftfreq, ifftn\n",
        "import torch.nn.functional as F\n",
        "import torch.cuda.amp as amp  # For mixed precision\n",
        "\n",
        "# Check GPUs and memory\n",
        "devices = [torch.device(f'cuda:{i}') for i in range(torch.cuda.device_count())]\n",
        "primary_device = devices[0] if devices else torch.device('cpu')\n",
        "print(f\"Primary device: {primary_device}\")\n",
        "if devices:\n",
        "    for i, dev in enumerate(devices):\n",
        "        props = torch.cuda.get_device_properties(dev)\n",
        "        print(f\"GPU {i}: {props.name}, VRAM: {props.total_memory / 1e9:.2f} GB\")\n",
        "print(f\"System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "try:\n",
        "    print(f\"SSD Storage Available: {psutil.disk_usage('/home/user').free / 1e9:.2f} GB\")\n",
        "except OSError as e:\n",
        "    print(f\"Warning: Could not check disk usage: {e}. Assuming 1000 GB free.\")\n",
        "\n",
        "# Create local directories\n",
        "checkpoint_path = os.path.expanduser('~/EFM_checkpoints/')\n",
        "data_path = os.path.expanduser('~/EFM_data/')\n",
        "with tqdm(total=2, desc=\"Creating directories\", leave=False) as pbar:\n",
        "    os.makedirs(checkpoint_path, exist_ok=True)\n",
        "    pbar.update(1)\n",
        "    os.makedirs(data_path, exist_ok=True)\n",
        "    pbar.update(1)\n",
        "print(f\"Checkpoints will be saved to: {checkpoint_path}\")\n",
        "print(f\"Data will be saved to: {data_path}\")\n",
        "\n",
        "# Verify GPUs\n",
        "print(\"Verifying GPUs...\")\n",
        "subprocess.run(['nvidia-smi'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configuration\n\nSet simulation parameters, tuned for a 500^3 grid to reproduce the BAO scale (~147 Mpc)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Simulation parameters\n",
        "config = {}\n",
        "config['N'] = 500  # Grid size (N x N x N)\n",
        "config['L'] = 1000.0  # Box size (1000 Mpc)\n",
        "config['dx'] = config['L'] / config['N']  # Spatial step\n",
        "config['c'] = 3e8  # Wave speed (m/s, speed of light)\n",
        "config['dt_cfl_factor'] = 0.000007  # Reduced CFL factor for stability\n",
        "config['dt'] = config['dt_cfl_factor'] * config['dx'] / config['c']  # Time step\n",
        "config['T'] = 50000  # Total steps (~1 Gyr with smaller dt)\n",
        "config['chunk_size'] = 125  # Increased for H100 (must divide N evenly)\n",
        "config['boundary_width_factor'] = 0.0  # Periodic boundaries\n",
        "\n",
        "# Physical parameters\n",
        "config['m'] = 4.16e-16  # Mass term (s^-1, tuned for ~147 Mpc)\n",
        "config['g'] = 0.01  # Cubic nonlinearity\n",
        "config['eta'] = 0.001  # Quintic nonlinearity\n",
        "config['k'] = 0.0  # Density scaling (no gravitational term)\n",
        "config['G'] = 6.674e-11  # Gravitational constant (m^3 kg^-1 s^-2)\n",
        "\n",
        "# Assign to local variables\n",
        "N = config['N']\n",
        "L = config['L']\n",
        "dx = config['dx']\n",
        "c = config['c']\n",
        "dt = config['dt']\n",
        "T = config['T']\n",
        "chunk_size = config['chunk_size']\n",
        "m = config['m']\n",
        "g = config['g']\n",
        "eta = config['eta']\n",
        "k = config['k']\n",
        "G = config['G']\n",
        "\n",
        "# Validate chunk_size\n",
        "if N % chunk_size != 0:\n",
        "    raise ValueError(f\"chunk_size ({chunk_size}) must divide N ({N}) evenly.\")\n",
        "\n",
        "print(f\"Grid size: {N} x {N} x {N}\")\n",
        "print(f\"Box size: {L} Mpc\")\n",
        "print(f\"Total steps: {T}\")\n",
        "print(f\"Chunk size: {chunk_size} z-slices per batch\")\n",
        "print(f\"Time step: {dt:.2e} seconds (~{dt / (3.156e7):.2f} years)\")\n",
        "print(f\"Soliton wavelength (from m): {2 * 3.14159 * c / m / (3.086e22):.2f} Mpc\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Simulation Setup\n\n- Grid Size: 500 x 500 x 500\n- Box Size: 1000 Mpc\n- Spatial Step: dx = 2 Mpc\n- Time Step: dt ~ 1e9 seconds (~0.032 Myr)\n- Steps: 50000 (~1 Gyr)\n- Chunk Size: 125 z-slices (optimized for H100)\n- Initial Conditions: Gaussian noise (amplitude 0.01)\n- Boundary Condition: Periodic\n- Equation: Nonlinear Klein-Gordon, no gravitational term\n\n## Numerical Methods\n- Integrator: 4th-order Runge-Kutta (RK4), optimized for 4 GPUs with mixed precision\n- Laplacian: 7-point stencil convolution\n- Boundary Conditions: Periodic\n- Power Spectrum: Full 3D FFT\n- Correlation Function: FFT-based\n\n## Parameter Justifications\n- m = 4.16e-16 s^-1: Sets ~147 Mpc scale\n- g = 0.01, eta = 0.001: Ensures soliton stability\n- k = 0.0: Removes destabilizing term\n- c = 3e8 m/s: Cosmological scale\n- dt_cfl_factor = 0.000007: Ensures stability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import torch.nn.functional as F\n",
        "import torch.cuda.amp as amp\n",
        "\n",
        "# Parameter set\n",
        "param_sets = [{\"m\": m, \"g\": g, \"eta\": eta, \"k\": k, \"label\": \"Baseline\"}]\n",
        "boundary_conditions = [\"periodic\"]\n",
        "results = []\n",
        "\n",
        "# Potential function\n",
        "def potential(phi, m, g, eta, k):\n",
        "    return m**2 * phi + g * phi**3 + eta * phi**5\n",
        "\n",
        "# Damping mask (periodic boundaries)\n",
        "def create_damping_mask(N, boundary_width, damping_factor, device):\n",
        "    return torch.ones((N, N, N), dtype=torch.float16, device=device)\n",
        "\n",
        "# Convolution-based Laplacian with mixed precision\n",
        "def conv_laplacian(phi, dx, device):\n",
        "    try:\n",
        "        with amp.autocast():\n",
        "            stencil = torch.tensor([[[0, 0, 0], [0, 1, 0], [0, 0, 0]],\n",
        "                                    [[0, 1, 0], [1, -6, 1], [0, 1, 0]],\n",
        "                                    [[0, 0, 0], [0, 1, 0], [0, 0, 0]]],\n",
        "                                   dtype=torch.float16, device=device)\n",
        "            stencil = stencil / (dx**2)\n",
        "            stencil = stencil.view(1, 1, 3, 3, 3)\n",
        "            phi = phi.view(1, 1, phi.shape[0], phi.shape[1], phi.shape[2])\n",
        "            try:\n",
        "                laplacian = F.conv3d(phi, stencil, padding=1, padding_mode='circular')\n",
        "            except TypeError:\n",
        "                phi_padded = torch.nn.functional.pad(phi, (1, 1, 1, 1, 1, 1), mode='circular')\n",
        "                laplacian = F.conv3d(phi_padded, stencil, padding=0)\n",
        "            return laplacian.view(phi.shape[2], phi.shape[3], phi.shape[4])\n",
        "    except Exception as e:\n",
        "        print(f\"Error in conv_laplacian: {e}\")\n",
        "        raise\n",
        "\n",
        "# NLKG derivative with mixed precision\n",
        "def nlkg_derivative(phi, phi_dot, m, g, eta, k, damping_mask):\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            with amp.autocast():\n",
        "                laplacian = conv_laplacian(phi, dx, phi.device)\n",
        "                if phi.shape != damping_mask.shape or laplacian.shape != phi.shape:\n",
        "                    raise ValueError(f\"Shape mismatch: phi {phi.shape}, damping_mask {damping_mask.shape}, laplacian {laplacian.shape}\")\n",
        "                phi.mul_(damping_mask)\n",
        "                phi_dot.mul_(damping_mask)\n",
        "                dV_dphi = 2 * m**2 * phi + 3 * g * phi**2 + 5 * eta * phi**4\n",
        "                phi_ddot = c**2 * laplacian - dV_dphi\n",
        "                return phi, phi_ddot\n",
        "    except Exception as e:\n",
        "        print(f\"Error in nlkg_derivative: {e}\")\n",
        "        raise\n",
        "\n",
        "# RK4 integrator with multi-GPU support and mixed precision\n",
        "def update_phi_rk4_chunked(phi, phi_dot, dt, m, g, eta, k, damping_mask, chunk_size, devices):\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            if phi.shape != phi_dot.shape or phi.shape != damping_mask.shape:\n",
        "                raise ValueError(f\"Shape mismatch: phi {phi.shape}, phi_dot {phi_dot.shape}, damping_mask {damping_mask.shape}\")\n",
        "            phi_new = torch.empty_like(phi, device=phi.device, dtype=torch.float16)\n",
        "            phi_dot_new = torch.empty_like(phi_dot, device=phi_dot.device, dtype=torch.float16)\n",
        "            num_gpus = len(devices)\n",
        "            total_chunks = (phi.shape[0] + chunk_size - 1) // chunk_size\n",
        "            for i in range(0, phi.shape[0], chunk_size):\n",
        "                chunk = slice(i, min(i + chunk_size, phi.shape[0]))\n",
        "                gpu_idx = (i // chunk_size) % num_gpus\n",
        "                dev = devices[gpu_idx]\n",
        "                stream = torch.cuda.Stream(device=dev)\n",
        "                with torch.cuda.stream(stream):\n",
        "                    with amp.autocast():\n",
        "                        phi_chunk = phi[chunk].to(dev, non_blocking=True)\n",
        "                        phi_dot_chunk = phi_dot[chunk].to(dev, non_blocking=True)\n",
        "                        damping_chunk = damping_mask[chunk].to(dev, non_blocking=True)\n",
        "                        temp = torch.empty_like(phi_chunk, device=dev, dtype=torch.float16)\n",
        "                        k1_v, k1_a = nlkg_derivative(phi_chunk, phi_dot_chunk, m, g, eta, k, damping_chunk)\n",
        "                        temp.copy_(phi_chunk + 0.5 * dt * k1_v)\n",
        "                        k2_v, k2_a = nlkg_derivative(temp, phi_dot_chunk + 0.5 * dt * k1_a, m, g, eta, k, damping_chunk)\n",
        "                        temp.copy_(phi_chunk + 0.5 * dt * k2_v)\n",
        "                        k3_v, k3_a = nlkg_derivative(temp, phi_dot_chunk + 0.5 * dt * k2_a, m, g, eta, k, damping_chunk)\n",
        "                        temp.copy_(phi_chunk + dt * k3_v)\n",
        "                        k4_v, k4_a = nlkg_derivative(temp, phi_dot_chunk + dt * k3_a, m, g, eta, k, damping_chunk)\n",
        "                        phi_new[chunk] = phi_chunk + (dt / 6.0) * (k1_v + 2 * k2_v + 2 * k3_v + k4_v)\n",
        "                        phi_dot_new[chunk] = phi_dot_chunk + (dt / 6.0) * (k1_a + 2 * k2_a + 2 * k3_a + k4_a)\n",
        "                        phi_new[chunk].clamp_(-5, 5)\n",
        "                        phi_dot_new[chunk].clamp_(-5, 5)\n",
        "                        del phi_chunk, phi_dot_chunk, damping_chunk, temp, k1_v, k1_a, k2_v, k2_a, k3_v, k3_a, k4_v, k4_a\n",
        "            return phi_new, phi_dot_new\n",
        "    except Exception as e:\n",
        "        print(f\"Error in update_phi_rk4_chunked: {e}\")\n",
        "        raise\n",
        "\n",
        "# Energy calculation with mixed precision\n",
        "def compute_energy(phi, phi_dot, m, g, eta, k, chunk_size, dx, c):\n",
        "    try:\n",
        "        with torch.no_grad():\n",
        "            total_energy = 0.0\n",
        "            kinetic_total = 0.0\n",
        "            gradient_total = 0.0\n",
        "            potential_total = 0.0\n",
        "            num_chunks = 0\n",
        "            total_chunks = (phi.shape[0] + chunk_size - 1) // chunk_size\n",
        "            for i in range(0, phi.shape[0], chunk_size):\n",
        "                chunk = slice(i, min(i + chunk_size, phi.shape[0]))\n",
        "                phi_chunk = phi[chunk]\n",
        "                phi_dot_chunk = phi_dot[chunk]\n",
        "                if torch.any(torch.isinf(phi_chunk)) or torch.any(torch.isnan(phi_chunk)):\n",
        "                    print(f\"Warning: phi contains inf or nan in chunk {i}\")\n",
        "                    return float('inf'), float('inf'), float('inf'), float('inf')\n",
        "                if torch.any(torch.isinf(phi_dot_chunk)) or torch.any(torch.isnan(phi_dot_chunk)):\n",
        "                    print(f\"Warning: phi_dot contains inf or nan in chunk {i}\")\n",
        "                    return float('inf'), float('inf'), float('inf'), float('inf')\n",
        "                with amp.autocast():\n",
        "                    kinetic = 0.5 * phi_dot_chunk**2\n",
        "                    potential_energy = 0.5 * m**2 * phi_chunk**2 + 0.25 * g * phi_chunk**4 + 0.1667 * eta * phi_chunk**6\n",
        "                    gradient = torch.zeros_like(phi_chunk, dtype=torch.float16)\n",
        "                    for d in range(3):\n",
        "                        grad_d = torch.gradient(phi_chunk, spacing=dx, dim=d)[0]\n",
        "                        gradient += grad_d**2\n",
        "                    gradient *= 0.5 * c**2\n",
        "                    kinetic_mean = torch.mean(kinetic).item() if not torch.isnan(kinetic).any() else 0.0\n",
        "                    gradient_mean = torch.mean(gradient).item() if not torch.isnan(gradient).any() else 0.0\n",
        "                    potential_mean = torch.mean(potential_energy).item() if not torch.isnan(potential_energy).any() else 0.0\n",
        "                    kinetic_total += kinetic_mean\n",
        "                    gradient_total += gradient_mean\n",
        "                    potential_total += potential_mean\n",
        "                    num_chunks += 1\n",
        "            kinetic_total /= num_chunks\n",
        "            gradient_total /= num_chunks\n",
        "            potential_total /= num_chunks\n",
        "            total_energy = kinetic_total + gradient_total + potential_total\n",
        "            return total_energy, kinetic_total, gradient_total, potential_total\n",
        "    except Exception as e:\n",
        "        print(f\"Error in compute_energy: {e}\")\n",
        "        raise\n",
        "\n",
        "# Power spectrum calculation\n",
        "def compute_power_spectrum(phi, k_range=[0.005, 0.1], chunk_size=125, dx=1.0, N=500):\n",
        "    try:\n",
        "        fft_result = np.zeros((phi.shape[0], phi.shape[1], phi.shape[2]), dtype=np.complex64)\n",
        "        total_chunks = (phi.shape[0] + chunk_size - 1) // chunk_size\n",
        "        pbar = tqdm(range(0, phi.shape[0], chunk_size), desc=\"Computing FFT for power spectrum\", leave=False, unit=\"chunk\", total=total_chunks)\n",
        "        for i in pbar:\n",
        "            chunk = slice(i, min(i + chunk_size, phi.shape[0]))\n",
        "            phi_chunk = phi[chunk].cpu().numpy()\n",
        "            fft_chunk = fftn(phi_chunk)\n",
        "            fft_result[chunk] = fft_chunk\n",
        "            del phi_chunk, fft_chunk\n",
        "            gc.collect()\n",
        "        pbar.close()\n",
        "        kx = fftfreq(N, d=dx)\n",
        "        ky = fftfreq(N, d=dx)\n",
        "        kz = fftfreq(N, d=dx)\n",
        "        kx, ky, kz = np.meshgrid(kx, ky, kz, indexing='ij')\n",
        "        k = np.sqrt(kx**2 + ky**2 + kz**2)\n",
        "        power = np.abs(fft_result)**2\n",
        "        k_bins = np.linspace(k_range[0], k_range[1], 50)\n",
        "        power_binned = np.zeros(len(k_bins) - 1)\n",
        "        pbar_bin = tqdm(range(len(k_bins) - 1), desc=\"Binning power spectrum\", leave=False, unit=\"bin\")\n",
        "        for i in pbar_bin:\n",
        "            mask = (k >= k_bins[i]) & (k < k_bins[i + 1])\n",
        "            power_binned[i] = np.mean(power[mask]) if np.any(mask) else 0\n",
        "        pbar_bin.close()\n",
        "        del fft_result, kx, ky, kz, k, power\n",
        "        gc.collect()\n",
        "        return k_bins[:-1], power_binned\n",
        "    except Exception as e:\n",
        "        print(f\"Error in compute_power_spectrum: {e}\")\n",
        "        raise\n",
        "\n",
        "# Correlation function\n",
        "def compute_correlation_function(phi, chunk_size=125, dx=1.0, N=500):\n",
        "    try:\n",
        "        fft_result = np.zeros((phi.shape[0], phi.shape[1], phi.shape[2]), dtype=np.complex64)\n",
        "        total_chunks = (phi.shape[0] + chunk_size - 1) // chunk_size\n",
        "        pbar = tqdm(range(0, phi.shape[0], chunk_size), desc=\"Computing FFT for correlation\", leave=False, unit=\"chunk\", total=total_chunks)\n",
        "        for i in pbar:\n",
        "            chunk = slice(i, min(i + chunk_size, phi.shape[0]))\n",
        "            phi_chunk = phi[chunk].cpu().numpy()\n",
        "            fft_chunk = fftn(phi_chunk)\n",
        "            fft_result[chunk] = fft_chunk\n",
        "            del phi_chunk, fft_chunk\n",
        "            gc.collect()\n",
        "        pbar.close()\n",
        "        power = np.abs(fft_result)**2\n",
        "        corr = ifftn(power).real\n",
        "        indices = np.arange(-N//2, N//2)\n",
        "        x, y, z = np.meshgrid(indices, indices, indices, indexing='ij')\n",
        "        r = np.sqrt(x**2 + y**2 + z**2) * dx\n",
        "        r_bins = np.linspace(0, 500, 50)\n",
        "        corr_binned = np.zeros(len(r_bins) - 1)\n",
        "        pbar_bin = tqdm(range(len(r_bins) - 1), desc=\"Binning correlation function\", leave=False, unit=\"bin\")\n",
        "        for i in pbar_bin:\n",
        "            mask = (r >= r_bins[i]) & (r < r_bins[i + 1])\n",
        "            corr_binned[i] = np.mean(corr[mask]) if np.any(mask) else 0\n",
        "        pbar_bin.close()\n",
        "        del fft_result, power, corr, x, y, z, r\n",
        "        gc.collect()\n",
        "        return r_bins[:-1], corr_binned / np.max(corr_binned) if np.max(corr_binned) != 0 else corr_binned\n",
        "    except Exception as e:\n",
        "        print(f\"Error in compute_correlation_function: {e}\")\n",
        "        raise\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Precompute Initial Conditions\n\nCompute and save initial fields to disk."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import os\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Precompute initial conditions\n",
        "init_path = f\"{data_path}initial_conditions_N{N}.npz\"\n",
        "if not os.path.exists(init_path):\n",
        "    with tqdm(total=2, desc=\"Computing initial conditions\", leave=False) as pbar:\n",
        "        try:\n",
        "            phi_ST = np.random.normal(0, 1, (N, N, N)).astype(np.float32) * 0.01\n",
        "            phi_dot_ST = np.zeros((N, N, N), dtype=np.float32)\n",
        "            pbar.update(1)\n",
        "            np.savez_compressed(init_path, phi_ST=phi_ST, phi_dot_ST=phi_dot_ST)\n",
        "            pbar.update(1)\n",
        "            print(\"Initial conditions saved.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing initial conditions: {e}\")\n",
        "            raise\n",
        "else:\n",
        "    with tqdm(total=1, desc=\"Loading initial conditions\", leave=False) as pbar:\n",
        "        try:\n",
        "            init_data = np.load(init_path)\n",
        "            phi_ST = init_data['phi_ST']\n",
        "            phi_dot_ST = init_data['phi_dot_ST']\n",
        "            pbar.update(1)\n",
        "            print(\"Initial conditions loaded.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading initial conditions: {e}\")\n",
        "            raise\n",
        "\n",
        "# Load to GPU\n",
        "with tqdm(total=3, desc=\"Preparing tensors\", leave=False) as pbar:\n",
        "    phi_ST_tensor = torch.from_numpy(phi_ST).to(primary_device, dtype=torch.float16, non_blocking=True)\n",
        "    pbar.update(1)\n",
        "    phi_dot_ST_tensor = torch.from_numpy(phi_dot_ST).to(primary_device, dtype=torch.float16, non_blocking=True)\n",
        "    pbar.update(1)\n",
        "    damping_mask = torch.ones((N, N, N), dtype=torch.float16, device=primary_device)\n",
        "    pbar.update(1)\n",
        "\n",
        "# Validate\n",
        "if phi_ST_tensor.shape != (N, N, N) or phi_dot_ST_tensor.shape != (N, N, N) or damping_mask.shape != (N, N, N):\n",
        "    raise ValueError(f\"Shape mismatch: phi_ST {phi_ST_tensor.shape}, phi_dot_ST {phi_dot_ST_tensor.shape}, damping_mask {damping_mask.shape}\")\n",
        "if torch.any(torch.isnan(phi_ST_tensor)) or torch.any(torch.isinf(phi_ST_tensor)):\n",
        "    raise ValueError(\"phi_ST_tensor contains NaN or Inf values\")\n",
        "if torch.any(torch.isnan(phi_dot_ST_tensor)) or torch.any(torch.isinf(phi_dot_ST_tensor)):\n",
        "    raise ValueError(\"phi_dot_ST_tensor contains NaN or Inf values\")\n",
        "\n",
        "print(\"Initial conditions prepared on device:\", primary_device)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Main Simulation Loop\n\nRuns the simulation and saves a checkpoint if completed, with partial checkpointing on interruption."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import psutil\n",
        "import time\n",
        "import os\n",
        "import gc\n",
        "\n",
        "# Confirmation prompt\n",
        "confirm = input(f\"Are you sure you want to run the main simulation ({N}^3 grid, {T} steps)? This should take approximately {(T * 0.1) / 3600:.2f} hours. Type 'yes' to proceed: \")\n",
        "if confirm.lower() != 'yes':\n",
        "    print(\"Simulation aborted.\")\n",
        "else:\n",
        "    for param in param_sets:\n",
        "        for boundary_type in boundary_conditions:\n",
        "            print(f\"Running simulation: {param['label']}, Boundary: {boundary_type}\")\n",
        "            energy_history = np.zeros(2, dtype=np.float32)\n",
        "            kinetic_history = np.zeros(2, dtype=np.float32)\n",
        "            gradient_history = np.zeros(2, dtype=np.float32)\n",
        "            potential_history = np.zeros(2, dtype=np.float32)\n",
        "            history_idx = 0\n",
        "            start_time = time.time()\n",
        "            last_disk_free = 1000.0  # Default in GB\n",
        "            start_step = 0\n",
        "\n",
        "            # Check for partial checkpoint\n",
        "            partial_checkpoint = max([f for f in os.listdir(checkpoint_path) if f.startswith(f\"checkpoint_{param['label']}_{boundary_type}_partial_step\") and f.endswith(f\"_N{N}.npz\")], key=lambda x: int(x.split('_step')[1].split('_')[0]), default=None)\n",
        "            if partial_checkpoint:\n",
        "                partial_checkpoint = os.path.join(checkpoint_path, partial_checkpoint)\n",
        "                with tqdm(total=3, desc=\"Loading partial checkpoint\", leave=False) as pbar:\n",
        "                    try:\n",
        "                        checkpoint_data = np.load(partial_checkpoint)\n",
        "                        phi_ST_tensor = torch.from_numpy(checkpoint_data['phi_ST']).to(primary_device, dtype=torch.float16, non_blocking=True)\n",
        "                        pbar.update(1)\n",
        "                        phi_dot_ST_tensor = torch.from_numpy(checkpoint_data['phi_dot_ST']).to(primary_device, dtype=torch.float16, non_blocking=True)\n",
        "                        pbar.update(1)\n",
        "                        energy_history = checkpoint_data['energy_history']\n",
        "                        kinetic_history = checkpoint_data['kinetic_history']\n",
        "                        gradient_history = checkpoint_data['gradient_history']\n",
        "                        potential_history = checkpoint_data['potential_history']\n",
        "                        start_step = int(checkpoint_data['last_step']) + 1\n",
        "                        history_idx = 1\n",
        "                        print(f\"Resuming from partial checkpoint at step {start_step}\")\n",
        "                        pbar.update(1)\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error loading partial checkpoint: {e}. Starting from step 0.\")\n",
        "                        start_step = 0\n",
        "\n",
        "            if start_step == 0:\n",
        "                with tqdm(total=1, desc=\"Computing initial energy\", leave=False) as pbar_energy:\n",
        "                    total_energy, kinetic, gradient, pot_energy = compute_energy(phi_ST_tensor, phi_dot_ST_tensor, param['m'], param['g'], param['eta'], param['k'], chunk_size, dx, c)\n",
        "                    pbar_energy.update(1)\n",
        "                if np.isinf(total_energy) or np.isnan(total_energy):\n",
        "                    print(\"Error: Initial energy is invalid (Inf or NaN). Check input tensors.\")\n",
        "                    break\n",
        "                energy_history[history_idx] = total_energy\n",
        "                kinetic_history[history_idx] = kinetic\n",
        "                gradient_history[history_idx] = gradient\n",
        "                potential_history[history_idx] = pot_energy\n",
        "                history_idx += 1\n",
        "\n",
        "            pbar = tqdm(range(start_step, T), desc=f\"Simulation Progress ({param['label']}, {boundary_type})\", unit=\"step\")\n",
        "            for t in pbar:\n",
        "                try:\n",
        "                    phi_ST_tensor, phi_dot_ST_tensor = update_phi_rk4_chunked(phi_ST_tensor, phi_dot_ST_tensor, dt, param['m'], param['g'], param['eta'], param['k'], damping_mask, chunk_size, devices)\n",
        "                    if torch.any(torch.isnan(phi_ST_tensor)) or torch.any(torch.isinf(phi_ST_tensor)):\n",
        "                        print(f\"Error at step {t}: phi_ST_tensor contains NaN or Inf values\")\n",
        "                        break\n",
        "                    if torch.any(torch.isnan(phi_dot_ST_tensor)) or torch.any(torch.isinf(phi_dot_ST_tensor)):\n",
        "                        print(f\"Error at step {t}: phi_dot_ST_tensor contains NaN or Inf values\")\n",
        "                        break\n",
        "                except Exception as e:\n",
        "                    print(f\"Error at step {t}: {e}\")\n",
        "                    # Save partial checkpoint\n",
        "                    try:\n",
        "                        with tqdm(total=1, desc=\"Saving partial checkpoint\", leave=False) as pbar_save:\n",
        "                            np.savez_compressed(\n",
        "                                f\"{checkpoint_path}checkpoint_{param['label']}_{boundary_type}_partial_step{t}_N{N}.npz\",\n",
        "                                phi_ST=phi_ST_tensor.cpu().numpy(),\n",
        "                                phi_dot_ST=phi_dot_ST_tensor.cpu().numpy(),\n",
        "                                energy_history=energy_history,\n",
        "                                kinetic_history=kinetic_history,\n",
        "                                gradient_history=gradient_history,\n",
        "                                potential_history=potential_history,\n",
        "                                last_step=t\n",
        "                            )\n",
        "                            pbar_save.update(1)\n",
        "                        print(f\"Partial checkpoint saved at step {t}\")\n",
        "                    except Exception as save_e:\n",
        "                        print(f\"Error saving partial checkpoint at step {t}: {save_e}\")\n",
        "                    break\n",
        "\n",
        "                if t % 500 == 0:  # Check resources less frequently\n",
        "                    vram_used = sum(torch.cuda.memory_allocated(dev) / 1e9 for dev in devices)\n",
        "                    vram_reserved = sum(torch.cuda.memory_reserved(dev) / 1e9 for dev in devices)\n",
        "                    ram_used = psutil.virtual_memory().used / 1e9\n",
        "                    try:\n",
        "                        disk_free = psutil.disk_usage('/home/user').free / 1e9\n",
        "                        last_disk_free = disk_free\n",
        "                    except OSError as e:\n",
        "                        print(f\"Warning: Disk usage check failed at step {t}: {e}. Using last known value: {last_disk_free:.2f} GB\")\n",
        "                        disk_free = last_disk_free\n",
        "                    pbar.set_postfix({'VRAM': f'{vram_used:.2f}GB', 'RAM': f'{ram_used:.2f}GB', 'Disk Free': f'{disk_free:.2f}GB'})\n",
        "                    if vram_used > 200 or vram_reserved > 280 or ram_used > 800 or disk_free < 50:\n",
        "                        print(f\"Warning: Resource usage high at step {t}\")\n",
        "                        # Save partial checkpoint\n",
        "                        try:\n",
        "                            with tqdm(total=1, desc=\"Saving partial checkpoint\", leave=False) as pbar_save:\n",
        "                                np.savez_compressed(\n",
        "                                    f\"{checkpoint_path}checkpoint_{param['label']}_{boundary_type}_partial_step{t}_N{N}.npz\",\n",
        "                                    phi_ST=phi_ST_tensor.cpu().numpy(),\n",
        "                                    phi_dot_ST=phi_dot_ST_tensor.cpu().numpy(),\n",
        "                                    energy_history=energy_history,\n",
        "                                    kinetic_history=kinetic_history,\n",
        "                                    gradient_history=gradient_history,\n",
        "                                    potential_history=potential_history,\n",
        "                                    last_step=t\n",
        "                                )\n",
        "                                pbar_save.update(1)\n",
        "                            print(f\"Partial checkpoint saved at step {t}\")\n",
        "                        except Exception as save_e:\n",
        "                            print(f\"Error saving partial checkpoint at step {t}: {save_e}\")\n",
        "                        break\n",
        "\n",
        "            pbar.close()\n",
        "\n",
        "            try:\n",
        "                if t == T - 1:\n",
        "                    with tqdm(total=1, desc=\"Computing final energy\", leave=False) as pbar_energy:\n",
        "                        total_energy, kinetic, gradient, pot_energy = compute_energy(phi_ST_tensor, phi_dot_ST_tensor, param['m'], param['g'], param['eta'], param['k'], chunk_size, dx, c)\n",
        "                        pbar_energy.update(1)\n",
        "                    energy_history[history_idx] = total_energy\n",
        "                    kinetic_history[history_idx] = kinetic\n",
        "                    gradient_history[history_idx] = gradient\n",
        "                    potential_history[history_idx] = pot_energy\n",
        "\n",
        "                    with tqdm(total=1, desc=\"Saving final checkpoint\", leave=False) as pbar_save:\n",
        "                        np.savez_compressed(\n",
        "                            f\"{checkpoint_path}checkpoint_{param['label']}_{boundary_type}_{T}_N{N}.npz\",\n",
        "                            phi_ST=phi_ST_tensor.cpu().numpy(),\n",
        "                            phi_dot_ST=phi_dot_ST_tensor.cpu().numpy(),\n",
        "                            energy_history=energy_history,\n",
        "                            kinetic_history=kinetic_history,\n",
        "                            gradient_history=gradient_history,\n",
        "                            potential_history=potential_history\n",
        "                        )\n",
        "                        pbar_save.update(1)\n",
        "                    print(f\"Checkpoint saved at step {T}\")\n",
        "                else:\n",
        "                    print(f\"Simulation stopped early at step {t}. No final checkpoint saved.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error saving final checkpoint: {e}\")\n",
        "\n",
        "            end_time = time.time()\n",
        "            runtime = end_time - start_time\n",
        "            print(f\"Simulation completed in {runtime:.2f} seconds (~{runtime / 3600:.2f} hours)\")\n",
        "\n",
        "            torch.cuda.empty_cache()\n",
        "            gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute Final Observables\n\nLoad checkpoint and compute power spectrum and correlation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import gc\n",
        "\n",
        "# Parameters to match checkpoint\n",
        "N = 500\n",
        "T = 50000\n",
        "L = 1000.0\n",
        "dx = L / N\n",
        "chunk_size = 125\n",
        "m = 4.16e-16\n",
        "g = 0.01\n",
        "eta = 0.001\n",
        "k = 0.0\n",
        "G = 6.674e-11\n",
        "c = 3e8\n",
        "label = \"Baseline\"\n",
        "boundary_type = \"periodic\"\n",
        "\n",
        "# Load checkpoint\n",
        "checkpoint_file = f\"{checkpoint_path}checkpoint_{label}_{boundary_type}_{T}_N{N}.npz\"\n",
        "with tqdm(total=1, desc=\"Loading checkpoint\", leave=False) as pbar:\n",
        "    try:\n",
        "        checkpoint_data = np.load(checkpoint_file)\n",
        "        pbar.update(1)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Checkpoint file {checkpoint_file} not found.\")\n",
        "        raise\n",
        "\n",
        "with tqdm(total=2, desc=\"Loading tensors\", leave=False) as pbar:\n",
        "    phi_ST = torch.from_numpy(checkpoint_data['phi_ST']).to(primary_device, dtype=torch.float16, non_blocking=True)\n",
        "    pbar.update(1)\n",
        "    phi_dot_ST = torch.from_numpy(checkpoint_data['phi_dot_ST']).to(primary_device, dtype=torch.float16, non_blocking=True)\n",
        "    pbar.update(1)\n",
        "energy_history = checkpoint_data['energy_history']\n",
        "kinetic_history = checkpoint_data['kinetic_history']\n",
        "gradient_history = checkpoint_data['gradient_history']\n",
        "potential_history = checkpoint_data['potential_history']\n",
        "print(\"Checkpoint loaded.\")\n",
        "\n",
        "# Validate tensors\n",
        "if torch.any(torch.isinf(phi_ST)) or torch.any(torch.isnan(phi_ST)):\n",
        "    print(\"Warning: phi_ST contains inf or nan values.\")\n",
        "if torch.any(torch.isinf(phi_dot_ST)) or torch.any(torch.isnan(phi_dot_ST)):\n",
        "    print(\"Warning: phi_dot_ST contains inf or nan values.\")\n",
        "\n",
        "# Compute observables\n",
        "try:\n",
        "    density_norm = torch.sum(phi_ST**2).item() * k\n",
        "    if np.isinf(density_norm) or np.isnan(density_norm):\n",
        "        print(\"Warning: Density norm is inf or nan.\")\n",
        "\n",
        "    with tqdm(total=1, desc=\"Computing power spectrum\", leave=False) as pbar:\n",
        "        k_bins, power_spectrum = compute_power_spectrum(phi_ST, k_range=[0.005, 0.1], chunk_size=chunk_size, dx=dx, N=N)\n",
        "        pbar.update(1)\n",
        "    with tqdm(total=1, desc=\"Computing correlation function\", leave=False) as pbar:\n",
        "        r, corr_func = compute_correlation_function(phi_ST, chunk_size=chunk_size, dx=dx, N=N)\n",
        "        pbar.update(1)\n",
        "\n",
        "    results.append({\n",
        "        'params': {\"m\": m, \"g\": g, \"eta\": eta, \"k\": k, \"label\": label},\n",
        "        'boundary': boundary_type,\n",
        "        'density_norm': density_norm,\n",
        "        'power_spectrum': (k_bins, power_spectrum),\n",
        "        'correlation_function': (r, corr_func),\n",
        "        'energy_history': energy_history,\n",
        "        'runtime': 5400  # Placeholder: 1.5 hours\n",
        "    })\n",
        "    print(\"Final observables computed.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error computing observables: {e}\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Validation Against Public Datasets\n\nValidate clustering scales against DESI BAO data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Validation\n",
        "for result in tqdm(results, desc=\"Validating results\", unit=\"result\"):\n",
        "    print(f\"\\nValidation for {result['params']['label']}, Boundary: {result['boundary']}\")\n",
        "    print(f\"Density Norm (S/T): {result['density_norm']}\")\n",
        "    r_peak = result['correlation_function'][0][np.argmax(result['correlation_function'][1])]\n",
        "    print(f\"Clustering Scale (Correlation): {r_peak:.2f} Mpc (DESI BAO: 147.09 ± 0.26 Mpc, EFM Expected: ~147 Mpc, ~628 Mpc)\")\n",
        "    k_peak = result['power_spectrum'][0][np.argmax(result['power_spectrum'][1])]\n",
        "    lambda_peak = 2 * np.pi / k_peak if k_peak != 0 else float('inf')\n",
        "    print(f\"Clustering Scale (Power Spectrum): {lambda_peak:.2f} Mpc (DESI BAO: 147.09 ± 0.26 Mpc, EFM Expected: ~147 Mpc, ~628 Mpc)\")\n",
        "\n",
        "# Save results\n",
        "with tqdm(total=1, desc=\"Saving results\", leave=False) as pbar:\n",
        "    try:\n",
        "        np.save(f\"{data_path}simulation_results_N{N}.npy\", results)\n",
        "        pbar.update(1)\n",
        "        print(f\"Results saved to {data_path}simulation_results_N{N}.npy\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving results: {e}\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Post-Processing: Generate Plots\n\nVisualize field distributions, energy, power spectrum, and correlation function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Parameters\n",
        "N = 500\n",
        "T = 50000\n",
        "L = 1000.0\n",
        "dx = L / N\n",
        "chunk_size = 125\n",
        "label = \"Baseline\"\n",
        "boundary_type = \"periodic\"\n",
        "\n",
        "# Plot\n",
        "final_checkpoint = f\"{checkpoint_path}checkpoint_{label}_{boundary_type}_{T}_N{N}.npz\"\n",
        "if os.path.exists(final_checkpoint):\n",
        "    try:\n",
        "        for plot_type in tqdm([\"field\", \"energy\", \"power_spectrum\", \"correlation\"], desc=\"Generating plots\", unit=\"plot\"):\n",
        "            if plot_type == \"field\":\n",
        "                with tqdm(total=1, desc=\"Plotting field\", leave=False) as pbar:\n",
        "                    plt.figure(figsize=(10, 8))\n",
        "                    plt.imshow(phi_ST[N//2, :, :].cpu().numpy(), extent=[-L/2, L/2, -L/2, L/2], cmap='viridis')\n",
        "                    plt.colorbar(label='phi_ST')\n",
        "                    plt.title(f'S/T Field (z=0) at Step {T}')\n",
        "                    plt.xlabel('x (Mpc)')\n",
        "                    plt.ylabel('y (Mpc)')\n",
        "                    plt.savefig(f\"{data_path}field_ST_{label}_{boundary_type}_N{N}_final.png\")\n",
        "                    plt.close()\n",
        "                    pbar.update(1)\n",
        "\n",
        "            elif plot_type == \"energy\":\n",
        "                with tqdm(total=1, desc=\"Plotting energy\", leave=False) as pbar:\n",
        "                    plt.figure(figsize=(10, 5))\n",
        "                    plt.plot(energy_history, label='Total Energy')\n",
        "                    plt.plot(kinetic_history, label='Kinetic', linestyle='--')\n",
        "                    plt.plot(gradient_history, label='Gradient', linestyle='-.')\n",
        "                    plt.plot(potential_history, label='Potential', linestyle=':')\n",
        "                    plt.xlabel('Step (0 and End)')\n",
        "                    plt.ylabel('Energy')\n",
        "                    plt.title(f'Energy Evolution ({label}, {boundary_type})')\n",
        "                    plt.legend()\n",
        "                    plt.grid()\n",
        "                    plt.savefig(f\"{data_path}energy_{label}_{boundary_type}_N{N}_final.png\")\n",
        "                    plt.close()\n",
        "                    pbar.update(1)\n",
        "\n",
        "            elif plot_type == \"power_spectrum\":\n",
        "                with tqdm(total=1, desc=\"Plotting power spectrum\", leave=False) as pbar:\n",
        "                    k_bins, power_spectrum = compute_power_spectrum(phi_ST, chunk_size=chunk_size, dx=dx, N=N)\n",
        "                    plt.figure(figsize=(10, 5))\n",
        "                    plt.loglog(k_bins, power_spectrum, label='Power Spectrum')\n",
        "                    plt.axvline(x=2 * np.pi / 147, color='r', linestyle='--', label='147 Mpc')\n",
        "                    plt.axvline(x=2 * np.pi / 628, color='g', linestyle='--', label='628 Mpc')\n",
        "                    plt.xlabel('k (Mpc^-1)')\n",
        "                    plt.ylabel('P(k)')\n",
        "                    plt.title(f'Power Spectrum ({label}, {boundary_type})')\n",
        "                    plt.legend()\n",
        "                    plt.grid()\n",
        "                    plt.savefig(f\"{data_path}power_spectrum_{label}_{boundary_type}_N{N}_final.png\")\n",
        "                    plt.close()\n",
        "                    pbar.update(1)\n",
        "\n",
        "            elif plot_type == \"correlation\":\n",
        "                with tqdm(total=1, desc=\"Plotting correlation\", leave=False) as pbar:\n",
        "                    r, corr_func = compute_correlation_function(phi_ST, chunk_size=chunk_size, dx=dx, N=N)\n",
        "                    plt.figure(figsize=(10, 5))\n",
        "                    plt.plot(r, corr_func, label='Correlation Function')\n",
        "                    plt.axvline(x=147, color='r', linestyle='--', label='147 Mpc')\n",
        "                    plt.axvline(x=628, color='g', linestyle='--', label='628 Mpc')\n",
        "                    plt.xlabel('r (Mpc)')\n",
        "                    plt.ylabel('Correlation')\n",
        "                    plt.title(f'Correlation Function ({label}, {boundary_type})')\n",
        "                    plt.legend()\n",
        "                    plt.grid()\n",
        "                    plt.savefig(f\"{data_path}correlation_{label}_{boundary_type}_N{N}_final.png\")\n",
        "                    plt.close()\n",
        "                    pbar.update(1)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in post-processing: {e}\")\n",
        "else:\n",
        "    print(f\"Checkpoint file {final_checkpoint} not found.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameter Justifications\n\n- m = 4.16e-16 s^-1: Sets ~147 Mpc scale\n- g = 0.01, eta = 0.001: Ensures soliton stability\n- k = 0.0: Removes destabilizing term\n- c = 3e8 m/s: Cosmological scale\n- dt_cfl_factor = 0.000007: Ensures stability\n- Initial Conditions: Gaussian noise\n- Boundary Condition: Periodic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Next Steps\n\n- Run test simulation to verify stability\n- Perform full simulation\n- Validate against DESI, SDSS datasets\n- Draft LaTeX paper"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test Mode\n\nRun a small-scale simulation to debug."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import psutil\n",
        "\n",
        "# Test mode\n",
        "N_test = 100\n",
        "L_test = 1000.0\n",
        "dx_test = L_test / N_test\n",
        "dt_cfl_factor = 0.000007\n",
        "dt_test = dt_cfl_factor * dx_test / c\n",
        "T_test = min(T, 10)\n",
        "chunk_size_test = 25\n",
        "\n",
        "with tqdm(total=3, desc=\"Preparing test tensors\", leave=False) as pbar:\n",
        "    phi_ST_test = torch.from_numpy(np.random.normal(0, 1, (N_test, N_test, N_test)).astype(np.float32) * 0.01).to(primary_device, dtype=torch.float16, non_blocking=True)\n",
        "    pbar.update(1)\n",
        "    phi_dot_ST_test = torch.zeros((N_test, N_test, N_test), device=primary_device, dtype=torch.float16)\n",
        "    pbar.update(1)\n",
        "    damping_mask_test = torch.ones((N_test, N_test, N_test), device=primary_device, dtype=torch.float16)\n",
        "    pbar.update(1)\n",
        "\n",
        "pbar = tqdm(range(T_test), desc=\"Test Simulation Progress\", unit=\"step\")\n",
        "for t in pbar:\n",
        "    try:\n",
        "        param = param_sets[0]\n",
        "        phi_ST_test, phi_dot_ST_test = update_phi_rk4_chunked(phi_ST_test, phi_dot_ST_test, dt_test, param['m'], param['g'], param['eta'], param['k'], damping_mask_test, chunk_size_test, devices)\n",
        "        vram_used = sum(torch.cuda.memory_allocated(dev) / 1e9 for dev in devices)\n",
        "        ram_used = psutil.virtual_memory().used / 1e9\n",
        "        try:\n",
        "            disk_free = psutil.disk_usage('/home/user').free / 1e9\n",
        "        except OSError as e:\n",
        "            print(f\"Warning: Disk usage check failed at test step {t}: {e}. Assuming 1000 GB free.\")\n",
        "            disk_free = 1000.0\n",
        "        pbar.set_postfix({'VRAM': f'{vram_used:.2f}GB', 'RAM': f'{ram_used:.2f}GB', 'Disk Free': f'{disk_free:.2f}GB'})\n",
        "    except Exception as e:\n",
        "        print(f\"Test simulation failed at step {t}: {e}\")\n",
        "        break\n",
        "pbar.close()\n",
        "print(\"Test simulation completed.\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}