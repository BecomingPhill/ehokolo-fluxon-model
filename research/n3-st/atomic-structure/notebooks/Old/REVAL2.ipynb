{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# EFM Clustering Scale and H₀ Simulation (S/T State, N=500)\n",
        "\n",
        "This notebook simulates the Ehokolo Fluxon Model (EFM) in the Space/Time (S/T) state to derive the clustering scale and Hubble constant (H₀) from first principles. We use a 500³ grid (L=1000 Mpc) to capture large-scale structure formation, targeting a clustering scale of ~628 Mpc (DESI/SDSS) and H₀ consistent with SHOES (73.0 ± 1.0 km/s/Mpc) and Planck (67.4 ± 0.5 km/s/Mpc).\n",
        "\n",
        "## Objectives\n",
        "- Simulate cosmological structure formation using the EFM NLKG equation in the S/T state.\n",
        "- Use a properly normalized Gaussian random field initial condition with a Harrison-Zel'dovich power spectrum.\n",
        "- Compute the clustering scale via the correlation function and H₀ via velocity field analysis.\n",
        "- Optimize for Google Colab Pro+ (A100 GPU, 40 GB VRAM, 80 GB RAM) to scale up to 1000³.\n",
        "- Validate results against DESI/SDSS, SHOES, and Planck datasets.\n",
        "\n",
        "## Hardware\n",
        "- GPU: NVIDIA A100 (40 GB VRAM)\n",
        "- System RAM: 80 GB\n",
        "- Environment: Google Colab Pro+\n",
        "\n",
        "## Setup Instructions\n",
        "1. Set runtime to A100 GPU.\n",
        "2. Execute all cells sequentially.\n",
        "3. Monitor VRAM (<20 GB) and RAM (<40 GB) usage.\n",
        "4. Save outputs to Google Drive."
      ],
      "metadata": {
        "id": "intro_markdown"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup: Libraries, GPU Check, Drive Mount, Paths"
      ],
      "metadata": {
        "id": "setup_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear GPU memory\n",
        "import torch\n",
        "import gc\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "# Install/Import libraries\n",
        "!nvidia-smi\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import psutil\n",
        "import time\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "import os\n",
        "import scipy.fft as fft\n",
        "\n",
        "# Check GPU and memory\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == \"cuda\":\n",
        "    print(f\"GPU VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "print(f\"System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "base_path = '/content/drive/MyDrive/EFM_Simulations/'\n",
        "sim_path = os.path.join(base_path, 'Clustering_Scale_Simulation/')\n",
        "checkpoint_path = os.path.join(sim_path, 'checkpoints/')\n",
        "data_path = os.path.join(sim_path, 'data/')\n",
        "plot_path = os.path.join(sim_path, 'plots/')\n",
        "os.makedirs(checkpoint_path, exist_ok=True)\n",
        "os.makedirs(data_path, exist_ok=True)\n",
        "os.makedirs(plot_path, exist_ok=True)\n",
        "print(f\"Paths created/checked:\\n Checkpoints: {checkpoint_path}\\n Data: {data_path}\\n Plots: {plot_path}\")"
      ],
      "metadata": {
        "id": "setup_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Simulation Parameters (S/T State, N=500)"
      ],
      "metadata": {
        "id": "params_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Numerical Parameters\n",
        "N = 500  # Grid size\n",
        "L = 1000.0  # Box size (Mpc)\n",
        "dx = L / N  # Spatial step (Mpc)\n",
        "dx_m = dx * 3.086e22  # Spatial step (meters)\n",
        "dt = 1e11  # Time step (seconds, ~3,170 years)\n",
        "T_steps = 1000  # Total steps (~3.17e7 years)\n",
        "save_interval = 100\n",
        "checkpoint_interval = 500\n",
        "\n",
        "# Physical Parameters\n",
        "c = 3e8  # Speed of light (m/s)\n",
        "m = 0.0  # Massless field for cosmology\n",
        "g = 0.01  # Reduced cubic nonlinearity\n",
        "eta = 0.001  # Reduced quintic nonlinearity\n",
        "alpha = 0.1\n",
        "delta = 0.05\n",
        "gamma_damp = 0.1  # Linear damping\n",
        "k = 0.01  # Density coupling\n",
        "G = 6.674e-11  # Gravitational constant\n",
        "rho_ref = 1.5\n",
        "\n",
        "# Initial Conditions\n",
        "A = 1e-5  # Amplitude of primordial fluctuations\n",
        "ns = 0.96  # Spectral index (Harrison-Zel'dovich)\n",
        "\n",
        "# Precision\n",
        "dtype = torch.float16\n",
        "\n",
        "print(\"--- S/T Clustering Simulation Parameters ---\")\n",
        "print(f\"Grid Size (N): {N}^3\")\n",
        "print(f\"Box Size (L): {L} Mpc\")\n",
        "print(f\"Time Step (dt): {dt:.2e} seconds\")\n",
        "print(f\"Total Steps: {T_steps}\")\n",
        "print(f\"Precision: {dtype}\")"
      ],
      "metadata": {
        "id": "params_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Helper Functions"
      ],
      "metadata": {
        "id": "helpers_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Potential function V(phi) = m*phi^2 + g*phi^4 + eta*phi^6\n",
        "def potential(phi, m_p, g_p, eta_p):\n",
        "    phi_f32 = phi.to(torch.float32)\n",
        "    m_p_f32 = torch.tensor(m_p, dtype=torch.float32, device=phi.device)\n",
        "    g_p_f32 = torch.tensor(g_p, dtype=torch.float32, device=phi.device)\n",
        "    eta_p_f32 = torch.tensor(eta_p, dtype=torch.float32, device=phi.device)\n",
        "    term2 = m_p_f32 * phi_f32**2\n",
        "    term4 = g_p_f32 * phi_f32**4\n",
        "    term6 = eta_p_f32 * phi_f32**6\n",
        "    return (term2 + term4 + term6).to(phi.dtype)\n",
        "\n",
        "# NLKG derivative with linear damping\n",
        "def nlkg_derivative(phi, phi_dot, m_p, g_p, eta_p, c_p, alpha_p, delta_p, gamma_damp_p, k_p, G_p, dx_p, N_p, device_p):\n",
        "    phi_f32 = phi.to(torch.float32)\n",
        "    phi_dot_f32 = phi_dot.to(torch.float32)\n",
        "    dx_p_f32 = torch.tensor(dx_p, dtype=torch.float32, device=device_p)\n",
        "    \n",
        "    # Laplacian\n",
        "    laplacian_f32 = torch.zeros_like(phi_f32)\n",
        "    for dim in range(3):\n",
        "        laplacian_f32 += torch.roll(phi_f32, shifts=1, dims=dim)\n",
        "        laplacian_f32 += torch.roll(phi_f32, shifts=-1, dims=dim)\n",
        "    laplacian = (laplacian_f32 - 6.0 * phi_f32) / dx_p_f32**2\n",
        "\n",
        "    # V'(phi)\n",
        "    dV_dphi = 2 * m_p * phi_f32 + 4 * g_p * phi_f32**3 + 6 * eta_p * phi_f32**5\n",
        "    \n",
        "    # Coupling and dissipation terms\n",
        "    grad_phi = torch.stack([torch.gradient(phi_f32, spacing=dx_p_f32, dim=d)[0] for d in range(3)])\n",
        "    alpha_term = alpha_p * phi_f32 * (phi_dot_f32 * grad_phi[0])\n",
        "    dissipation = delta_p * (phi_dot_f32**2) * phi_f32\n",
        "    linear_damping = -gamma_damp_p * phi_dot_f32\n",
        "    gravity = 8 * np.pi * G_p * k_p * phi_f32**2\n",
        "    \n",
        "    # phi_ddot\n",
        "    c_p_f32 = torch.tensor(c_p, dtype=torch.float32, device=device_p)\n",
        "    phi_ddot = c_p_f32**2 * laplacian - dV_dphi + alpha_term + dissipation + linear_damping - gravity\n",
        "    \n",
        "    # Absorbing boundary conditions\n",
        "    boundary_width = int(0.1 * N_p)\n",
        "    damping_factor = 0.05\n",
        "    mask = torch.ones_like(phi)\n",
        "    indices = torch.arange(N_p, device=device_p, dtype=phi.dtype)\n",
        "    damping_profile = torch.ones(N_p, device=device_p, dtype=phi.dtype)\n",
        "    ramp = torch.linspace(1.0, damping_factor, boundary_width, device=device_p, dtype=phi.dtype)\n",
        "    damping_profile[:boundary_width] = ramp.flip(dims=[0])\n",
        "    damping_profile[-boundary_width:] = ramp\n",
        "    for dim in range(3):\n",
        "        if dim == 0: mask *= damping_profile[:, None, None]\n",
        "        elif dim == 1: mask *= damping_profile[None, :, None]\n",
        "        else: mask *= damping_profile[None, None, :]\n",
        "    \n",
        "    phi_dot_damped = phi_dot_f32 * mask.to(torch.float32)\n",
        "    phi_ddot_damped = phi_ddot * mask.to(torch.float32)\n",
        "    return phi_dot_damped.to(phi_dot.dtype), phi_ddot_damped.to(phi_ddot.dtype)\n",
        "\n",
        "# RK4 integrator\n",
        "def update_phi_rk4(phi, phi_dot, dt_p, m_p, g_p, eta_p, c_p, alpha_p, delta_p, gamma_damp_p, k_p, G_p, dx_p, N_p, device_p):\n",
        "    with torch.no_grad():\n",
        "        dt_p_f32 = torch.tensor(dt_p, dtype=torch.float32, device=device_p)\n",
        "        k1_v, k1_a = nlkg_derivative(phi, phi_dot, m_p, g_p, eta_p, c_p, alpha_p, delta_p, gamma_damp_p, k_p, G_p, dx_p, N_p, device_p)\n",
        "        k2_v, k2_a = nlkg_derivative(phi + 0.5 * dt_p_f32 * k1_v, phi_dot + 0.5 * dt_p_f32 * k1_a, m_p, g_p, eta_p, c_p, alpha_p, delta_p, gamma_damp_p, k_p, G_p, dx_p, N_p, device_p)\n",
        "        k3_v, k3_a = nlkg_derivative(phi + 0.5 * dt_p_f32 * k2_v, phi_dot + 0.5 * dt_p_f32 * k2_a, m_p, g_p, eta_p, c_p, alpha_p, delta_p, gamma_damp_p, k_p, G_p, dx_p, N_p, device_p)\n",
        "        k4_v, k4_a = nlkg_derivative(phi + dt_p_f32 * k3_v, phi_dot + dt_p_f32 * k3_a, m_p, g_p, eta_p, c_p, alpha_p, delta_p, gamma_damp_p, k_p, G_p, dx_p, N_p, device_p)\n",
        "        phi_new = phi + (dt_p_f32 / 6.0) * (k1_v + 2.0 * k2_v + 2.0 * k3_v + k4_v)\n",
        "        phi_dot_new = phi_dot + (dt_p_f32 / 6.0) * (k1_a + 2.0 * k2_a + 2.0 * k3_a + k4_a)\n",
        "        phi_new.clamp_(-1e-3, 1e-3)  # Clamp to prevent runaway growth\n",
        "        phi_dot_new.clamp_(-1e-3, 1e-3)\n",
        "        return phi_new, phi_dot_new\n",
        "\n",
        "# Metrics computation\n",
        "def compute_metrics(phi, phi_dot, k_p, m_p, g_p, eta_p, c_p, dx_p, device_p):\n",
        "    with torch.no_grad():\n",
        "        phi_f32 = phi.to(torch.float32)\n",
        "        phi_dot_f32 = phi_dot.to(torch.float32)\n",
        "        dx_p_f32 = torch.tensor(dx_p, dtype=torch.float32, device=device_p)\n",
        "        c_p_f32 = torch.tensor(c_p, dtype=torch.float32, device=device_p)\n",
        "\n",
        "        max_amp = torch.max(torch.abs(phi_f32)).item()\n",
        "        avg_density = k_p * torch.mean(phi_f32**2).item()\n",
        "\n",
        "        kinetic = 0.5 * phi_dot_f32**2\n",
        "        grad_phi = torch.stack([torch.gradient(phi_f32, spacing=dx_p_f32, dim=d)[0] for d in range(3)])\n",
        "        gradient = 0.5 * c_p_f32**2 * (grad_phi[0]**2 + grad_phi[1]**2 + grad_phi[2]**2)\n",
        "        potential_energy = potential(phi_f32, m_p, g_p, eta_p)\n",
        "        total_energy = (kinetic + gradient + potential_energy).sum().item() * (dx_p_f32.item()**3)\n",
        "\n",
        "        if not (np.isfinite(max_amp) and np.isfinite(avg_density) and np.isfinite(total_energy)):\n",
        "            return float('nan'), float('nan'), float('nan')\n",
        "        return max_amp, avg_density, total_energy\n",
        "\n",
        "# Initial conditions: Properly normalized Gaussian random field\n",
        "def generate_initial_conditions(N, L, A, ns):\n",
        "    kx = fft.fftfreq(N, d=dx)\n",
        "    ky = fft.fftfreq(N, d=dx)\n",
        "    kz = fft.fftfreq(N, d=dx)\n",
        "    KX, KY, KZ = np.meshgrid(kx, ky, kz, indexing='ij')\n",
        "    k = np.sqrt(KX**2 + KY**2 + KZ**2)\n",
        "    k[k == 0] = 1e-10\n",
        "    Pk = A * (k ** ns)\n",
        "    delta_k = np.random.normal(0, np.sqrt(Pk/2), (N, N, N)) + 1j * np.random.normal(0, np.sqrt(Pk/2), (N, N, N))\n",
        "    delta = fft.ifftn(delta_k) * (N**3)\n",
        "    delta = np.real(delta)\n",
        "    # Normalize to RMS ~ A\n",
        "    rms = np.sqrt(np.mean(delta**2))\n",
        "    if rms > 0:\n",
        "        delta = delta * (A / rms)\n",
        "    return delta"
      ],
      "metadata": {
        "id": "helpers_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Simulation Execution"
      ],
      "metadata": {
        "id": "exec_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\n--- Starting S/T State Simulation ---\")\n",
        "sim_start_time = time.time()\n",
        "\n",
        "# Initialize fields\n",
        "init_path = f\"{data_path}initial_conditions_N{N}_L{L}.npz\"\n",
        "if not os.path.exists(init_path):\n",
        "    phi_ST = generate_initial_conditions(N, L, A, ns)\n",
        "    phi_dot_ST = np.zeros((N, N, N))\n",
        "    np.savez_compressed(init_path, phi_ST=phi_ST, phi_dot_ST=phi_dot_ST)\n",
        "    print(\"Initial conditions saved.\")\n",
        "else:\n",
        "    init_data = np.load(init_path)\n",
        "    phi_ST = init_data['phi_ST']\n",
        "    phi_dot_ST = init_data['phi_dot_ST']\n",
        "    print(\"Initial conditions loaded.\")\n",
        "\n",
        "phi = torch.from_numpy(phi_ST).to(device, dtype=dtype)\n",
        "phi_dot = torch.from_numpy(phi_dot_ST).to(device, dtype=dtype)\n",
        "\n",
        "# Initial metrics\n",
        "initial_max_amp, initial_avg_rho, initial_energy = compute_metrics(phi, phi_dot, k, m, g, eta, c, dx_m, device)\n",
        "print(f\"Initial Metrics: Max|φ|={initial_max_amp:.2e}, <ρ>={initial_avg_rho:.2e}, E={initial_energy:.2e}\")\n",
        "\n",
        "# Simulation loop\n",
        "max_amp_history = [initial_max_amp]\n",
        "avg_rho_history = [initial_avg_rho]\n",
        "energy_history = [initial_energy]\n",
        "stable = True\n",
        "steps_completed = 0\n",
        "\n",
        "pbar = tqdm(range(T_steps), desc=\"S/T State Simulation\")\n",
        "for t in pbar:\n",
        "    try:\n",
        "        phi, phi_dot = update_phi_rk4(phi, phi_dot, dt, m, g, eta, c, alpha, delta, gamma_damp, k, G, dx_m, N, device)\n",
        "        steps_completed = t + 1\n",
        "    except Exception as e:\n",
        "        print(f\"Error at step {t}: {e}\")\n",
        "        stable = False\n",
        "        break\n",
        "\n",
        "    if t % save_interval == 0 or t == T_steps - 1:\n",
        "        max_amp, avg_density, total_energy = compute_metrics(phi, phi_dot, k, m, g, eta, c, dx_m, device)\n",
        "        if not (np.isfinite(max_amp) and np.isfinite(avg_density) and np.isfinite(total_energy)) or max_amp > 1e-2:\n",
        "            print(f\"Instability at step {t}: Max|φ|={max_amp:.2e}, <ρ>={avg_density:.2e}, E={total_energy:.2e}\")\n",
        "            stable = False\n",
        "            break\n",
        "        max_amp_history.append(max_amp)\n",
        "        avg_rho_history.append(avg_density)\n",
        "        energy_history.append(total_energy)\n",
        "        pbar.set_postfix({'Max|φ|': f'{max_amp:.2e}', '<ρ>': f'{avg_density:.2e}', 'E': f'{total_energy:.2e}'})\n",
        "\n",
        "    if t % checkpoint_interval == 0 and t > 0:\n",
        "        torch.save({'step': t, 'phi': phi.cpu(), 'phi_dot': phi_dot.cpu()}, os.path.join(checkpoint_path, f\"step{t}.pt\"))\n",
        "        phi = phi.to(device)\n",
        "        phi_dot = phi_dot.to(device)\n",
        "\n",
        "pbar.close()\n",
        "\n",
        "# Save final state\n",
        "max_amp_history = np.array(max_amp_history, dtype=np.float32)\n",
        "avg_rho_history = np.array(avg_rho_history, dtype=np.float32)\n",
        "energy_history = np.array(energy_history, dtype=np.float32)\n",
        "max_amp_history = np.where(np.isfinite(max_amp_history), max_amp_history, 0)\n",
        "avg_rho_history = np.where(np.isfinite(avg_rho_history), avg_rho_history, 0)\n",
        "energy_history = np.where(np.isfinite(energy_history), energy_history, 0)\n",
        "\n",
        "final_results = {\n",
        "    'stable': stable,\n",
        "    'steps_completed': steps_completed,\n",
        "    'max_amp': max_amp_history,\n",
        "    'avg_rho': avg_rho_history,\n",
        "    'energy': energy_history,\n",
        "    'final_phi': phi.cpu().numpy() if stable else None,\n",
        "    'final_phi_dot': phi_dot.cpu().numpy() if stable else None\n",
        "}\n",
        "np.savez(os.path.join(data_path, f\"results_N{N}_T{T_steps}.npz\"), **final_results)\n",
        "print(f\"Saved results to {data_path}\")\n",
        "\n",
        "sim_end_time = time.time()\n",
        "print(f\"Simulation completed in {(sim_end_time - sim_start_time) / 60:.2f} minutes\")"
      ],
      "metadata": {
        "id": "exec_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Analysis: Clustering Scale and H₀"
      ],
      "metadata": {
        "id": "analysis_markdown"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load results\n",
        "results_file = os.path.join(data_path, f\"results_N{N}_T{T_steps}.npz\")\n",
        "results = np.load(results_file, allow_pickle=True)\n",
        "results = {key: results[key].item() if results[key].ndim == 0 else results[key] for key in results.files}\n",
        "\n",
        "if results['stable']:\n",
        "    phi = torch.from_numpy(results['final_phi']).to(device, dtype=dtype)\n",
        "    phi_dot = torch.from_numpy(results['final_phi_dot']).to(device, dtype=dtype)\n",
        "\n",
        "    # Compute density and velocity\n",
        "    rho = k * phi**2\n",
        "    velocity = phi_dot\n",
        "    positions = torch.linspace(-L/2, L/2, N, device=device)\n",
        "    X, Y, Z = torch.meshgrid(positions, positions, positions, indexing='ij')\n",
        "    r = torch.sqrt(X**2 + Y**2 + Z**2)\n",
        "    v_r = velocity * (X/r + Y/r + Z/r)\n",
        "    mask = (r > 0) & (r < 500)\n",
        "    H0 = torch.mean(v_r[mask] / r[mask]).item() * 3.086e19  # km/s/Mpc\n",
        "\n",
        "    # Correlation function\n",
        "    def compute_correlation_function(phi):\n",
        "        phi_slice = phi[N//2, :, :].cpu().numpy()\n",
        "        phi_flat = phi_slice.flatten()\n",
        "        corr = np.correlate(phi_flat, phi_flat, mode='full')\n",
        "        r = np.linspace(-L/2, L/2, len(corr))\n",
        "        return r, corr / np.max(corr)\n",
        "\n",
        "    r, corr_func = compute_correlation_function(rho)\n",
        "    r_peak = r[np.argmax(corr_func)]\n",
        "\n",
        "    # Validation\n",
        "    print(f\"Clustering Scale: {r_peak:.2f} Mpc (DESI/SDSS: ~628 Mpc)\")\n",
        "    print(f\"H0: {H0:.2f} km/s/Mpc (SHOES: 73.0 ± 1.0, Planck: 67.4 ± 0.5)\")\n",
        "    chi2_shoes = ((H0 - 73.0) / 1.0)**2\n",
        "    chi2_planck = ((H0 - 67.4) / 0.5)**2\n",
        "    print(f\"χ² (SHOES): {chi2_shoes:.2f}, χ² (Planck): {chi2_planck:.2f}\")\n",
        "\n",
        "    # Plots\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(r, corr_func, label='Correlation Function')\n",
        "    plt.xlabel('r (Mpc)')\n",
        "    plt.ylabel('Correlation')\n",
        "    plt.title('Correlation Function')\n",
        "    plt.legend()\n",
        "    plt.grid()\n",
        "    plt.savefig(os.path.join(plot_path, f\"correlation_N{N}_T{T_steps}.png\"))\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Simulation was unstable, cannot compute observables.\")"
      ],
      "metadata": {
        "id": "analysis_code"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}