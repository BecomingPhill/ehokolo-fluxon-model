{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-lss-batch-optimized"
      },
      "source": [
        "# EFM First-Principles Simulation: Batch-Optimized for Maximum Performance\n",
        "\n",
        "## Objective: A High-Throughput Test of Self-Organization\n",
        "\n",
        "This notebook implements a crucial optimization to the first-principles simulation. Analysis of the previous run revealed a significant performance bottleneck: the `512³` simulation was too small to fully saturate the computational capacity of the NVIDIA A100 GPU, leading to low resource utilization and extended runtimes.\n",
        "\n",
        "To solve this, this version has been re-architected to use **batch processing**. Instead of running one simulation, it runs a **batch of simulations simultaneously** in a single pass. This maximizes GPU parallelism and throughput, dramatically reducing the total computation time.\n",
        "\n",
        "**Key Optimizations Implemented:**\n",
        "1.  **Batch Processing:** All core tensors are now shaped `(B, N, N, N)`, where `B` is the batch size. This allows the GPU to process multiple simulations in parallel.\n",
        "2.  **GPU-Centric JIT Compilation:** All core functions are JIT-compiled to handle batched tensors, ensuring maximum computational efficiency.\n",
        "3.  **Dynamic Parameter Fields:** The stability (`m`) and emergence (`g`) parameters are calculated dynamically for each simulation within the batch, based on local field density.\n",
        "4.  **Numerical Stability:** The refined physics model incorporating `g_min` and `ν∇⁴φ` biharmonic damping is retained to ensure stability across the entire run.\n",
        "\n",
        "This notebook represents the most computationally efficient method for testing the EFM's self-organization hypothesis on the given hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "environment-setup-batch"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import gc\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "except ImportError:\n",
        "    pass # Continue silently if not in Colab\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device('cuda:0')\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(device)}, VRAM: {torch.cuda.get_device_properties(device).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    device = torch.device('cpu')\n",
        "    print(\"No GPU available, running on CPU.\")\n",
        "\n",
        "data_path_dynamic = '/content/drive/My Drive/EFM_Simulations/data/FirstPrinciples_Dynamic_N512_Batch/'\n",
        "os.makedirs(data_path_dynamic, exist_ok=True)\n",
        "print(f\"Batch-Optimized Simulation Data will be saved to: {data_path_dynamic}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config-batch"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'batch_size': 4, # <-- KEY OPTIMIZATION\n",
        "    'N': 512,\n",
        "    'L_sim_unit': 40.0,\n",
        "    'T_steps': 100000,\n",
        "    'dt_cfl_factor': 0.001,\n",
        "    'c_sim_unit': 1.0,\n",
        "\n",
        "    # Base EFM Parameters\n",
        "    'k_efm_gravity_coupling': 0.005,\n",
        "    'eta_sim': 0.01,\n",
        "    'alpha_sim': 0.1,\n",
        "    'delta_sim': 0.0002,\n",
        "\n",
        "    # Dynamic Parameter and Stability Config\n",
        "    'rho_ref': 1.5,\n",
        "    'm_ref': 1.0,\n",
        "    'g_ref': 0.1,\n",
        "    'g_sign_threshold': 1.0,\n",
        "    'g_min': 1e-5,\n",
        "    'nu_damping': 1e-4,\n",
        "    \n",
        "    # Initial conditions\n",
        "    'initial_perturbation_amplitude': 15.0,\n",
        "    'initial_perturbation_width': 2.0,\n",
        "    'background_noise_amplitude': 1.0e-4,\n",
        "\n",
        "    'history_every_n_steps': 500\n",
        "}\n",
        "\n",
        "config['dx_sim_unit'] = config['L_sim_unit'] / config['N']\n",
        "config['dt_sim_unit'] = config['dt_cfl_factor'] * config['dx_sim_unit'] / config['c_sim_unit']\n",
        "config['run_id'] = f\"DynamicParams_B{config['batch_size']}_N{config['N']}_Stable\"\n",
        "\n",
        "print(f\"--- EFM Batch-Optimized Simulation Configuration ({config['run_id']}) ---\")\n",
        "for key, value in config.items():\n",
        "    print(f\"{key}: {value}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jit-functions-batch"
      },
      "source": [
        "## Core Simulation Functions (Batch-Aware & JIT-Compiled)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jit-code-batch"
      },
      "outputs": [],
      "source": [
        "@torch.jit.script\n",
        "def conv_laplacian_batch_gpu(phi_batch: torch.Tensor, dx: float) -> torch.Tensor:\n",
        "    \"\"\"JIT-compiled 3D Laplacian for a batch of fields.\n",
        "    Input shape: (B, D, H, W). Output shape: (B, D, H, W).\"\"\"\n",
        "    B, D, H, W = phi_batch.shape\n",
        "    # Pad the spatial dimensions (D, H, W)\n",
        "    phi_padded = F.pad(phi_batch.unsqueeze(1), (1,1,1,1,1,1), mode='circular')\n",
        "    \n",
        "    stencil = torch.tensor([[[0.,0.,0.],[0.,1.,0.],[0.,0.,0.]],[[0.,1.,0.],[1.,-6.,1.],[0.,1.,0.]],[[0.,0.,0.],[0.,1.,0.],[0.,0.,0.]]], \n",
        "                           dtype=phi_batch.dtype, device=phi_batch.device) / (dx**2)\n",
        "    stencil = stencil.view(1, 1, 3, 3, 3)\n",
        "    \n",
        "    # Apply to the whole batch at once\n",
        "    return F.conv3d(phi_padded, stencil, groups=B, padding=0).squeeze(1)\n",
        "\n",
        "@torch.jit.script\n",
        "def biharmonic_operator_batch_gpu(phi_batch: torch.Tensor, dx: float) -> torch.Tensor:\n",
        "    \"\"\"Computes the biharmonic operator for a batch of fields.\"\"\"\n",
        "    lap_phi = conv_laplacian_batch_gpu(phi_batch, dx)\n",
        "    return conv_laplacian_batch_gpu(lap_phi, dx)\n",
        "\n",
        "@torch.jit.script\n",
        "def nlkg_derivative_dynamic_batch_gpu(phi: torch.Tensor, phi_dot: torch.Tensor, \n",
        "                                      k_gravity: float, eta: float, c_sq: float, alpha: float, \n",
        "                                      delta: float, dx: float, rho_ref: float, m_ref_sq: float, \n",
        "                                      g_ref: float, g_min: float, g_sign_thresh: float, \n",
        "                                      nu_damping: float) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    phi_f32 = phi.to(torch.float32)\n",
        "    phi_dot_f32 = phi_dot.to(torch.float32)\n",
        "\n",
        "    rho_local = k_gravity * phi_f32**2 + 1e-12\n",
        "    m_sq_field = m_ref_sq * (rho_local / rho_ref)\n",
        "    g_field_unsigned = g_min + g_ref * (rho_local / rho_ref)\n",
        "    sign_field = torch.where(rho_local > g_sign_thresh, -1.0, 1.0)\n",
        "    g_field_signed = sign_field * g_field_unsigned\n",
        "    \n",
        "    lap_phi = conv_laplacian_batch_gpu(phi_f32, dx)\n",
        "    potential_force = m_sq_field * phi_f32 + g_field_signed * torch.pow(phi_f32, 3) + eta * torch.pow(phi_f32, 5)\n",
        "    \n",
        "    grad_phi_x = (torch.roll(phi_f32, -1, 1) - torch.roll(phi_f32, 1, 1)) / (2 * dx)\n",
        "    grad_phi_y = (torch.roll(phi_f32, -1, 2) - torch.roll(phi_f32, 1, 2)) / (2 * dx)\n",
        "    grad_phi_z = (torch.roll(phi_f32, -1, 3) - torch.roll(phi_f32, 1, 3)) / (2 * dx)\n",
        "    grad_phi_abs_sq = grad_phi_x**2 + grad_phi_y**2 + grad_phi_z**2\n",
        "\n",
        "    alpha_term = alpha * phi_f32 * phi_dot_f32 * grad_phi_abs_sq\n",
        "    delta_term = delta * torch.pow(phi_dot_f32, 2) * phi_f32\n",
        "    biharmonic_term = nu_damping * biharmonic_operator_batch_gpu(phi_f32, dx)\n",
        "\n",
        "    phi_ddot = c_sq * lap_phi - potential_force + alpha_term - delta_term - biharmonic_term\n",
        "    return phi_dot, phi_ddot.to(phi.dtype)\n",
        "\n",
        "@torch.jit.script\n",
        "def update_phi_rk4_dynamic_batch_gpu(phi_current, phi_dot_current, dt, dx, params): \n",
        "    args = (params[0], params[1], params[2], params[3], params[4], dx, \n",
        "            params[5], params[6], params[7], params[8], params[9], params[10])\n",
        "\n",
        "    k1_v, k1_a = nlkg_derivative_dynamic_batch_gpu(phi_current, phi_dot_current, *args)\n",
        "    k2_v, k2_a = nlkg_derivative_dynamic_batch_gpu(phi_current + 0.5*dt*k1_v, phi_dot_current + 0.5*dt*k1_a, *args)\n",
        "    k3_v, k3_a = nlkg_derivative_dynamic_batch_gpu(phi_current + 0.5*dt*k2_v, phi_dot_current + 0.5*dt*k2_a, *args)\n",
        "    k4_v, k4_a = nlkg_derivative_dynamic_batch_gpu(phi_current + dt*k3_v, phi_dot_current + dt*k3_a, *args)\n",
        "\n",
        "    phi_next = phi_current + (dt / 6.0) * (k1_v + 2*k2_v + 2*k3_v + k4_v)\n",
        "    phi_dot_next = phi_dot_current + (dt / 6.0) * (k1_a + 2*k2_a + 2*k3_a + k4_a)\n",
        "    return phi_next, phi_dot_next\n",
        "\n",
        "print(\"Batch-Aware, JIT-Optimized simulation functions defined.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main-execution-batch"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    print(\"--- INITIATING BATCH-OPTIMIZED FIRST-PRINCIPLES SIMULATION ---\")\n",
        "    torch.manual_seed(42)\n",
        "    B, N = config['batch_size'], config['N']\n",
        "\n",
        "    # --- Initialize Batched Fields --- \n",
        "    coords = torch.linspace(-config['L_sim_unit']/2, config['L_sim_unit']/2, N, device=device)\n",
        "    X, Y, Z = torch.meshgrid(coords, coords, coords, indexing='ij')\n",
        "    r_sq = X**2 + Y**2 + Z**2\n",
        "    \n",
        "    # Create a batch of initial states\n",
        "    central_pulse = config['initial_perturbation_amplitude'] * torch.exp(-r_sq / (config['initial_perturbation_width']**2))\n",
        "    # Add slightly different noise to each simulation in the batch\n",
        "    noise = torch.rand(B, N, N, N, device=device) * config['background_noise_amplitude']\n",
        "    phi_current = (central_pulse.unsqueeze(0) + noise).to(torch.float16)\n",
        "    phi_dot_current = torch.zeros_like(phi_current, dtype=torch.float16)\n",
        "    del X, Y, Z, r_sq, coords, central_pulse, noise; gc.collect(); torch.cuda.empty_cache()\n",
        "    print(f\"Initialized a batch of {B} simulations on the GPU.\")\n",
        "\n",
        "    # --- Pack config into a tensor for JIT --- \n",
        "    config_params = torch.tensor([\n",
        "        config['k_efm_gravity_coupling'], config['eta_sim'], config['c_sim_unit']**2, config['alpha_sim'], config['delta_sim'],\n",
        "        config['rho_ref'], config['m_ref']**2, config['g_ref'], config['g_min'], config['g_sign_threshold'], config['nu_damping']\n",
        "    ], device=device, dtype=torch.float32)\n",
        "\n",
        "    # --- Simulation Loop ---\n",
        "    pbar = tqdm(range(config['T_steps']), desc=f\"Batch Sim (B={B}, N={N}³)\")\n",
        "    sim_start_time = time.time()\n",
        "\n",
        "    for t_step in pbar:\n",
        "        phi_current, phi_dot_current = update_phi_rk4_dynamic_batch_gpu(\n",
        "            phi_current, phi_dot_current, config['dt_sim_unit'], config['dx_sim_unit'], config_params\n",
        "        )\n",
        "\n",
        "        if (t_step + 1) % config['history_every_n_steps'] == 0:\n",
        "            if torch.any(torch.isinf(phi_current)) or torch.any(torch.isnan(phi_current)):\n",
        "                print(f\"\\nERROR: NaN/Inf detected at step {t_step + 1}! Halting.\"); break\n",
        "            max_phi = torch.max(torch.abs(phi_current)).item()\n",
        "            pbar.set_postfix({'Max|φ|': f'{max_phi:.3e}'})\n",
        "            if max_phi > 5e7: print(f\"\\nWarning: Instability detected. Halting.\"); break\n",
        "    \n",
        "    sim_duration = time.time() - sim_start_time\n",
        "    print(f\"Simulation finished in {sim_duration:.2f} seconds. Effective it/s: {config['T_steps']*B/sim_duration:.2f}\")\n",
        "\n",
        "    # --- Save Final State (saving only the first simulation of the batch for analysis) --- \n",
        "    final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    final_data_filename = os.path.join(data_path_dynamic, f\"FINAL_DATA_{config['run_id']}_{final_timestamp}.npz\")\n",
        "    np.savez_compressed(final_data_filename, \n",
        "                        phi_final_cpu=phi_current[0].cpu().numpy(), \n",
        "                        config=config)\n",
        "    print(f\"Final state of the first simulation saved to {final_data_filename}\")\n",
        "\n",
        "    del phi_current, phi_dot_current, config_params; gc.collect(); torch.cuda.empty_cache()\n",
        "    print(\"\\n--- SIMULATION COMPLETE. ANALYSIS CAN NOW PROCEED. ---\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}