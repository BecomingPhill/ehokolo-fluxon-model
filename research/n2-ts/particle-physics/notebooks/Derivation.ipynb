# -*- coding: utf-8 -*-
"""EFM_HDS_Validation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1... # <-- Add your specific Colab link here if desired

This notebook performs a high-fidelity simulation to validate the Harmonic Density State (HDS) hypothesis in the Ehokolo Fluxon Model (EFM). It simulates the baseline NLKG equation for various initial average densities to determine which ones lead to stable evolution, testing the ρ_n' = ρ_ref / n' relationship.

**Objectives**
- Implement the baseline EFM NLKG solver in 3D using PyTorch on a GPU.
- Simulate the field evolution starting from different average density levels (effective n').
- Monitor stability metrics (max amplitude, average density, energy).
- Analyze results to determine which density levels are computationally stable.
- Ensure reproducibility with documented code, parameters, hardware, and results saving.

**Hardware Target**
- **GPU**: NVIDIA A100 (40GB VRAM) or similar via Google Colab Pro+.
- **RAM**: Sufficient system RAM (>64GB recommended for larger N).

**Setup Instructions**
1. Go to `Runtime` > `Change runtime type` > Select `GPU` (ideally A100).
2. Run `!nvidia-smi` to verify GPU.
3. Execute all cells sequentially.
"""

# Clear GPU memory at the start (important for Colab)
import torch
import gc
if torch.cuda.is_available():
    torch.cuda.empty_cache()
gc.collect()

# Install and import required libraries
# !pip install torch numpy matplotlib tqdm psutil scipy -q
!nvidia-smi

import torch
import numpy as np
import matplotlib.pyplot as plt
from tqdm.notebook import tqdm
import psutil
import time
from datetime import datetime
from google.colab import drive
import os
import gc

# Check GPU and memory
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")
if device.type == "cuda":
    print(f"GPU Name: {torch.cuda.get_device_name(0)}")
    print(f"GPU VRAM Total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
print(f"System RAM Total: {psutil.virtual_memory().total / 1e9:.2f} GB")

# Mount Google Drive for checkpoints and data
drive.mount('/content/drive')
base_path = '/content/drive/MyDrive/EFM_Simulations/'
hds_path = os.path.join(base_path, 'HDS_Validation/')
checkpoint_path = os.path.join(hds_path, 'checkpoints/')
data_path = os.path.join(hds_path, 'data/')
plot_path = os.path.join(hds_path, 'plots/')
os.makedirs(checkpoint_path, exist_ok=True)
os.makedirs(data_path, exist_ok=True)
os.makedirs(plot_path, exist_ok=True)
print(f"Paths created/checked:\n Checkpoints: {checkpoint_path}\n Data: {data_path}\n Plots: {plot_path}")

"""## Simulation Parameters"""

# --- Numerical Parameters ---
N = 200  # Grid size (Start smaller: 200 or 400, then scale to 800 for full run)
L = 10.0  # Box size (simulation units)
dx = L / N
dt = 0.0005 # Time step (adjust based on stability, CFL condition proxy dx/c)
T_steps = 5000 # Total steps (adjust based on stability time scale)
save_interval = 500 # How often to save metrics/plots
checkpoint_interval = 1000 # How often to save full state

# --- Physical Parameters (Baseline NLKG: ∂²ϕ/∂t² - c²∇²ϕ + m²ϕ - gφ³ + ηφ⁵ = 0) ---
c_eff = 1.0    # Effective speed of light (simulation units)
m2 = 1.0     # Mass term squared (m^2)
g = 0.1     # Cubic nonlinearity coefficient (g)
eta = 0.01    # Quintic nonlinearity coefficient (η) - for 3D stability
k_rho = 0.01  # Density coupling constant (rho = k * phi^2)

# --- HDS Parameters ---
rho_ref = 1.5 # Reference density (for n'=1, adjust based on results)
target_n_primes = [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0] # Effective n' values to test

# --- Initial Conditions ---
noise_level = 0.01 # Small noise amplitude relative to field amplitude

# --- Precision ---
dtype = torch.float16 # Use float16 for memory, might need float32 for stability

# --- Reporting ---
print("--- Simulation Parameters ---")
print(f"Grid Size (N): {N}^3")
print(f"Box Size (L): {L}")
print(f"Spatial Step (dx): {dx}")
print(f"Time Step (dt): {dt}")
print(f"Total Steps (T_steps): {T_steps}")
print(f"Effective c (c_eff): {c_eff}")
print(f"Mass Term (m^2): {m2}")
print(f"Cubic Term (g): {g}")
print(f"Quintic Term (eta): {eta}")
print(f"Density Const (k_rho): {k_rho}")
print(f"Reference Density (rho_ref): {rho_ref}")
print(f"Target n' values: {target_n_primes}")
print(f"Precision (dtype): {dtype}")
print("-----------------------------")

"""## Helper Functions (Potential, NLKG Derivative, RK4, Metrics)"""

# Potential function V(phi) = 0.5*m2*phi^2 - 0.25*g*phi^4 + 0.1667*eta*phi^6
# Note: Derivative V'(phi) = m2*phi - g*phi^3 + eta*phi^5 is used in NLKG
def potential(phi, m2_p, g_p, eta_p):
    phi_f32 = phi.to(torch.float32) # Use float32 for potential calculation
    m2_p_f32 = torch.tensor(m2_p, dtype=torch.float32, device=phi.device)
    g_p_f32 = torch.tensor(g_p, dtype=torch.float32, device=phi.device)
    eta_p_f32 = torch.tensor(eta_p, dtype=torch.float32, device=phi.device)
    term2 = 0.5 * m2_p_f32 * phi_f32**2
    term4 = -0.25 * g_p_f32 * phi_f32**4
    term6 = (1.0/6.0) * eta_p_f32 * phi_f32**6
    return (term2 + term4 + term6).to(phi.dtype)

# NLKG derivative calculation with absorbing boundary conditions
def nlkg_derivative(phi, phi_dot, m2_p, g_p, eta_p, c_eff_p, L_p, N_p, dx_p, device_p):
    # Calculate Laplacian using finite differences (consider higher order later if needed)
    laplacian = torch.zeros_like(phi)
    # Roll along each dimension (x, y, z) - using float32 for accumulation
    laplacian_f32 = laplacian.to(torch.float32)
    phi_f32 = phi.to(torch.float32)
    dx_p_f32 = torch.tensor(dx_p, dtype=torch.float32, device=device_p)

    for dim in range(3):
        laplacian_f32 += torch.roll(phi_f32, shifts=1, dims=dim)
        laplacian_f32 += torch.roll(phi_f32, shifts=-1, dims=dim)
    laplacian = (laplacian_f32 - 6.0 * phi_f32) / dx_p_f32**2

    # Calculate potential derivative V'(phi) = m2*phi - g*phi^3 + eta*phi^5
    m2_p_f32 = torch.tensor(m2_p, dtype=torch.float32, device=device_p)
    g_p_f32 = torch.tensor(g_p, dtype=torch.float32, device=device_p)
    eta_p_f32 = torch.tensor(eta_p, dtype=torch.float32, device=device_p)
    dV_dphi = m2_p_f32 * phi_f32 - g_p_f32 * phi_f32**3 + eta_p_f32 * phi_f32**5

    # Calculate phi_ddot = c^2 * Laplacian - V'(phi)
    c_eff_p_f32 = torch.tensor(c_eff_p, dtype=torch.float32, device=device_p)
    phi_ddot = c_eff_p_f32**2 * laplacian - dV_dphi

    # Apply absorbing boundary conditions (damping mask)
    boundary_width = int(0.1 * N_p) # 10% boundary width
    damping_factor = 0.1 # Strength of damping
    mask = torch.ones_like(phi)
    for dim in range(3):
        indices = torch.arange(N_p, device=device_p, dtype=phi.dtype)
        # Create damping profile for one dimension
        damping_profile = torch.ones(N_p, device=device_p, dtype=phi.dtype)
        # Ramp down near min boundary
        damping_profile[:boundary_width] = torch.linspace(damping_factor, 1.0, boundary_width, device=device_p, dtype=phi.dtype)
        # Ramp down near max boundary
        damping_profile[-boundary_width:] = torch.linspace(1.0, damping_factor, boundary_width, device=device_p, dtype=phi.dtype)

        # Apply mask dimension by dimension using broadcasting
        if dim == 0:
            mask *= damping_profile[:, None, None]
        elif dim == 1:
            mask *= damping_profile[None, :, None]
        else:
            mask *= damping_profile[None, None, :]

    # Apply damping to phi_dot only (common technique for wave absorption)
    phi_dot_damped = phi_dot * mask

    return phi_dot_damped.to(phi_dot.dtype), phi_ddot.to(phi_dot.dtype) # Return damped velocity, undamped acceleration

# RK4 integrator
def update_phi_rk4(phi, phi_dot, dt_p, m2_p, g_p, eta_p, c_eff_p, L_p, N_p, dx_p, device_p):
    with torch.no_grad(): # Ensure no gradient tracking during update
        # Use float32 for RK4 intermediate steps for better precision
        phi_f32 = phi.to(torch.float32)
        phi_dot_f32 = phi_dot.to(torch.float32)
        dt_p_f32 = torch.tensor(dt_p, dtype=torch.float32, device=device_p)

        # k1
        k1_v, k1_a = nlkg_derivative(phi_f32, phi_dot_f32, m2_p, g_p, eta_p, c_eff_p, L_p, N_p, dx_p, device_p)
        k1_v = k1_v.to(torch.float32)
        k1_a = k1_a.to(torch.float32)


        # k2
        phi_temp = phi_f32 + 0.5 * dt_p_f32 * k1_v
        phi_dot_temp = phi_dot_f32 + 0.5 * dt_p_f32 * k1_a
        k2_v, k2_a = nlkg_derivative(phi_temp, phi_dot_temp, m2_p, g_p, eta_p, c_eff_p, L_p, N_p, dx_p, device_p)
        k2_v = k2_v.to(torch.float32)
        k2_a = k2_a.to(torch.float32)


        # k3
        phi_temp = phi_f32 + 0.5 * dt_p_f32 * k2_v
        phi_dot_temp = phi_dot_f32 + 0.5 * dt_p_f32 * k2_a
        k3_v, k3_a = nlkg_derivative(phi_temp, phi_dot_temp, m2_p, g_p, eta_p, c_eff_p, L_p, N_p, dx_p, device_p)
        k3_v = k3_v.to(torch.float32)
        k3_a = k3_a.to(torch.float32)


        # k4
        phi_temp = phi_f32 + dt_p_f32 * k3_v
        phi_dot_temp = phi_dot_f32 + dt_p_f32 * k3_a
        k4_v, k4_a = nlkg_derivative(phi_temp, phi_dot_temp, m2_p, g_p, eta_p, c_eff_p, L_p, N_p, dx_p, device_p)
        k4_v = k4_v.to(torch.float32)
        k4_a = k4_a.to(torch.float32)


        # Update
        phi_new_f32 = phi_f32 + (dt_p_f32 / 6.0) * (k1_v + 2.0 * k2_v + 2.0 * k3_v + k4_v)
        phi_dot_new_f32 = phi_dot_f32 + (dt_p_f32 / 6.0) * (k1_a + 2.0 * k2_a + 2.0 * k3_a + k4_a)

        # Clamp to prevent numerical overflow (adjust limits if needed)
        # phi_new_f32 = torch.clamp(phi_new_f32, -50.0, 50.0)
        # phi_dot_new_f32 = torch.clamp(phi_dot_new_f32, -50.0, 50.0)

        # Convert back to original dtype
        phi_new = phi_new_f32.to(phi.dtype)
        phi_dot_new = phi_dot_new_f32.to(phi_dot.dtype)

        # Memory cleanup for intermediate tensors
        del phi_f32, phi_dot_f32, k1_v, k1_a, k2_v, k2_a, k3_v, k3_a, k4_v, k4_a, phi_temp, phi_dot_temp
        if torch.cuda.is_available():
             torch.cuda.empty_cache()
        gc.collect()

        return phi_new, phi_dot_new

# Function to compute stability metrics
def compute_stability_metrics(phi, phi_dot, k_rho_p, m2_p, g_p, eta_p, c_eff_p, dx_p, device_p):
    with torch.no_grad():
        # Calculate metrics using float32 for stability
        phi_f32 = phi.to(torch.float32)
        phi_dot_f32 = phi_dot.to(torch.float32)

        # Max Amplitude
        max_amp = torch.max(torch.abs(phi_f32)).item()

        # Average Density
        avg_density = k_rho_p * torch.mean(phi_f32**2).item()

        # Energy components (use float32 internally)
        kinetic_energy_density = 0.5 * phi_dot_f32**2
        grad_phi = torch.stack(torch.gradient(phi_f32, spacing=dx_p, dim=[0, 1, 2]))
        gradient_energy_density = 0.5 * c_eff_p**2 * torch.sum(grad_phi**2, dim=0)
        potential_energy_density = potential(phi_f32, m2_p, g_p, eta_p)

        # Total Energy (integrate density over volume)
        total_energy = torch.sum(kinetic_energy_density + gradient_energy_density + potential_energy_density).item() * dx_p**3

        # Check for NaNs
        if np.isnan(max_amp) or np.isnan(avg_density) or np.isnan(total_energy):
             return float('nan'), float('nan'), float('nan')

        return max_amp, avg_density, total_energy


"""## Simulation Loop"""

all_results = {}
total_start_time = time.time()

# Ensure simulation parameters are defined globally before the loop
N_sim = N
L_sim = L
dx_sim = dx
dt_sim = dt
T_steps_sim = T_steps
m2_sim = m2
g_sim = g
eta_sim = eta
c_eff_sim = c_eff
k_rho_sim = k_rho
rho_ref_sim = rho_ref
noise_level_sim = noise_level


for n_prime in target_n_primes:
    print(f"\n--- Starting Simulation for n' = {n_prime:.2f} ---")
    sim_start_time = time.time()

    # Calculate target density and initial field amplitude
    target_rho = rho_ref_sim / n_prime
    initial_amp = np.sqrt(target_rho / k_rho_sim) if k_rho_sim > 0 else 1.0 # Avoid division by zero
    print(f" Target Density: {target_rho:.4f}, Initial Amplitude: {initial_amp:.4f}")

    # Initialize fields on GPU with target amplitude + noise
    phi = (torch.ones((N_sim, N_sim, N_sim), device=device, dtype=dtype) * initial_amp *
           (1 + noise_level_sim * torch.randn((N_sim, N_sim, N_sim), device=device, dtype=dtype)))
    phi_dot = torch.zeros((N_sim, N_sim, N_sim), device=device, dtype=dtype)

    # Track metrics for this run
    max_amp_history = []
    avg_density_history = []
    energy_history = []
    stable = True

    pbar = tqdm(range(T_steps_sim), desc=f"n'={n_prime:.2f}", leave=False)
    for t in pbar:
        # Update fields using RK4
        try:
            phi, phi_dot = update_phi_rk4(phi, phi_dot, dt_sim, m2_sim, g_sim, eta_sim, c_eff_sim, L_sim, N_sim, dx_sim, device)
        except RuntimeError as e:
             if "out of memory" in str(e):
                 print(f"\n!!! CUDA out of memory at step {t} for n'={n_prime:.2f}. Skipping rest of this run. Try reducing N. !!!")
                 stable = False
                 # Clean up tensors before breaking
                 del phi, phi_dot, pbar
                 if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                 gc.collect()
                 break # Exit inner loop
             else:
                 raise e # Re-raise other runtime errors

        # Compute and store metrics periodically
        if t % (save_interval // 10) == 0 or t == T_steps_sim - 1: # Check metrics more often
             max_amp, avg_density, total_energy = compute_stability_metrics(
                 phi, phi_dot, k_rho_sim, m2_sim, g_sim, eta_sim, c_eff_sim, dx_sim, device
             )

             # Check for instability
             if np.isnan(max_amp) or np.isnan(avg_density) or np.isnan(total_energy) or max_amp > 100.0: # Added amplitude check
                 print(f"\n!!! Instability detected at step {t} for n'={n_prime:.2f} (Max Amp: {max_amp:.2e}) !!!")
                 stable = False
                 # Store last valid metrics if possible
                 if len(max_amp_history) > 0:
                      max_amp_history.append(max_amp_history[-1])
                      avg_density_history.append(avg_density_history[-1])
                      energy_history.append(energy_history[-1])
                 else: # If instability on first metric check
                     max_amp_history.append(float('nan'))
                     avg_density_history.append(float('nan'))
                     energy_history.append(float('nan'))

                 # Clean up tensors before breaking
                 del phi, phi_dot, pbar
                 if torch.cuda.is_available():
                     torch.cuda.empty_cache()
                 gc.collect()
                 break # Exit inner loop
             else:
                max_amp_history.append(max_amp)
                avg_density_history.append(avg_density)
                energy_history.append(total_energy)

             pbar.set_postfix({'Max|φ|': f'{max_amp:.2f}', '<ρ>': f'{avg_density:.4f}', 'E': f'{total_energy:.2e}'})

        # Optional: Save checkpoint periodically
        # if t % checkpoint_interval == 0 and t > 0:
        #     try:
        #         chkpt_file = os.path.join(checkpoint_path, f"hds_n{n_prime:.1f}_step{t}.pt")
        #         torch.save({'step': t, 'phi': phi, 'phi_dot': phi_dot}, chkpt_file)
        #     except Exception as e:
        #         print(f"Warning: Could not save checkpoint at step {t} for n'={n_prime:.1f}: {e}")

    # Store results for this n'
    all_results[n_prime] = {
        'stable': stable,
        'max_amp': np.array(max_amp_history),
        'avg_rho': np.array(avg_density_history),
        'energy': np.array(energy_history),
        'final_phi_slice': phi[N_sim//2, :, :].cpu().numpy() if stable else None # Save a slice of stable final state
    }

    # Save individual run data immediately
    try:
        np.savez(os.path.join(data_path, f"hds_results_n{n_prime:.1f}.npz"),
                 **all_results[n_prime])
        print(f"Saved results for n' = {n_prime:.2f}")
    except Exception as e:
        print(f"Error saving results for n' = {n_prime:.2f}: {e}")


    # Clean up GPU memory after each n' run
    del phi, phi_dot, max_amp_history, avg_density_history, energy_history, pbar
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

    sim_end_time = time.time()
    print(f"--- Simulation for n' = {n_prime:.2f} finished in {sim_end_time - sim_start_time:.2f} seconds ---")


total_end_time = time.time()
print(f"\n=== All simulations completed in {(total_end_time - total_start_time) / 60:.2f} minutes ===")

"""## Analysis and Visualization"""

# --- Plot Stability Metrics vs Time for selected n' values ---
plt.figure(figsize=(18, 5))

# Max Amplitude
plt.subplot(1, 3, 1)
for n_prime, results in all_results.items():
    if results['stable']:
        plt.plot(results['max_amp'], label=f"n'={n_prime:.1f}")
    else:
        plt.plot(results['max_amp'], label=f"n'={n_prime:.1f} (Unstable)", linestyle=':')
plt.xlabel(f"Step (x{save_interval // 10})")
plt.ylabel("Max |φ|")
plt.title("Max Amplitude vs Time")
plt.yscale('log') # Use log scale if amplitudes vary widely
plt.grid(True, which="both", ls="--")
plt.legend(fontsize='small', ncol=2)

# Average Density
plt.subplot(1, 3, 2)
for n_prime, results in all_results.items():
     target_rho = rho_ref_sim / n_prime
     plt.plot(results['avg_rho'], label=f"n'={n_prime:.1f}")
     plt.axhline(target_rho, linestyle='--', color='gray', alpha=0.5) # Show target density
plt.xlabel(f"Step (x{save_interval // 10})")
plt.ylabel("Average Density <ρ>")
plt.title("Average Density vs Time")
plt.grid(True, which="both", ls="--")
plt.legend(fontsize='small', ncol=2)

# Total Energy
plt.subplot(1, 3, 3)
for n_prime, results in all_results.items():
    plt.plot(results['energy'], label=f"n'={n_prime:.1f}")
plt.xlabel(f"Step (x{save_interval // 10})")
plt.ylabel("Total Energy E")
plt.title("Total Energy vs Time")
plt.grid(True, which="both", ls="--")
plt.legend(fontsize='small', ncol=2)

plt.tight_layout()
plt.savefig(os.path.join(plot_path, "hds_stability_metrics_vs_time.png"))
plt.show()

# --- Plot Final Stability Indicators vs n' ---
final_max_amp = []
final_avg_rho = []
is_stable = []
n_primes_tested = sorted(all_results.keys())

for n_prime in n_primes_tested:
    results = all_results[n_prime]
    is_stable.append(results['stable'])
    if results['stable'] and len(results['max_amp']) > 0:
        final_max_amp.append(results['max_amp'][-1])
        final_avg_rho.append(results['avg_rho'][-1])
    else:
        # Use NaN or a large value to indicate instability on summary plot
        final_max_amp.append(float('nan'))
        final_avg_rho.append(float('nan'))

plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(n_primes_tested, final_max_amp, 'bo-', label='Final Max |φ|')
plt.scatter([n for n, stable in zip(n_primes_tested, is_stable) if not stable],
            [100]*sum(not s for s in is_stable), marker='x', color='r', s=100, label='Unstable') # Mark unstable
plt.xlabel("Effective n'")
plt.ylabel("Final Max |φ|")
plt.title("Final Amplitude vs n'")
plt.yscale('log')
plt.grid(True, which="both", ls="--")
plt.legend()

plt.subplot(1, 2, 2)
plt.plot(n_primes_tested, final_avg_rho, 'go-', label='Final <ρ>')
plt.plot(n_primes_tested, [rho_ref_sim / n_p for n_p in n_primes_tested], 'k--', label='Target ρ_ref/n\'')
plt.scatter([n for n, stable in zip(n_primes_tested, is_stable) if not stable],
            [np.nanmax(final_avg_rho)*1.1 if np.isfinite(final_avg_rho).any() else 1.0] * sum(not s for s in is_stable),
            marker='x', color='r', s=100, label='Unstable') # Mark unstable
plt.xlabel("Effective n'")
plt.ylabel("Final Average Density <ρ>")
plt.title("Final Average Density vs n'")
# plt.yscale('log')
plt.grid(True, which="both", ls="--")
plt.legend()


plt.tight_layout()
plt.savefig(os.path.join(plot_path, "hds_stability_summary_vs_n_prime.png"))
plt.show()

# --- Visualize Final States for Stable Runs ---
num_stable = sum(res['stable'] for res in all_results.values())
if num_stable > 0:
    cols = min(4, num_stable)
    rows = (num_stable + cols - 1) // cols
    plt.figure(figsize=(4*cols, 4*rows))
    stable_count = 0
    for i, n_prime in enumerate(n_primes_tested):
         results = all_results[n_prime]
         if results['stable'] and results['final_phi_slice'] is not None:
             stable_count += 1
             ax = plt.subplot(rows, cols, stable_count)
             im = ax.imshow(results['final_phi_slice'], extent=[-L/2, L/2, -L/2, L/2], cmap='viridis')
             plt.colorbar(im)
             ax.set_title(f"Final φ (z=0) for n'={n_prime:.1f}")
             ax.set_xlabel("x")
             ax.set_ylabel("y")
    plt.tight_layout()
    plt.savefig(os.path.join(plot_path, "hds_stable_final_states.png"))
    plt.show()
else:
    print("No stable final states to visualize.")


"""## Simulation Report"""

print("\n--- Simulation Report ---")
print(f"Simulation Timestamp: {datetime.now()}")
print(f"Grid Size (N): {N_sim}^3")
print(f"Total Steps: {T_steps_sim}")
print(f"Number of n' values tested: {len(target_n_primes)}")
print(f"Total Runtime: {(total_end_time - total_start_time):.2f} seconds ({(total_end_time - total_start_time)/60:.2f} minutes)")
print("\nStability Summary:")
stable_n_primes = []
unstable_n_primes = []
for n_prime, results in all_results.items():
    status = "Stable" if results['stable'] else "Unstable"
    if results['stable']:
        stable_n_primes.append(n_prime)
        print(f" n' = {n_prime:.2f}: {status} (Final Max|φ|={results['max_amp'][-1]:.2f}, Final <ρ>={results['avg_rho'][-1]:.4f})")
    else:
        unstable_n_primes.append(n_prime)
        print(f" n' = {n_prime:.2f}: {status}")

print(f"\nStable n' values found: {sorted(stable_n_primes)}")
print(f"Unstable n' values found: {sorted(unstable_n_primes)}")

print("\nAnalysis plots saved to:", plot_path)
print("Data files saved to:", data_path)
print("Checkpoints saved to:", checkpoint_path)
print("------------------------")