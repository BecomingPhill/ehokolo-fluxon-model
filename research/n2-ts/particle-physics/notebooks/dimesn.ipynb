{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-lss-param-sweep"
      },
      "source": [
        "# EFM Large-Scale Structure (LSS) Parameter Sweep (Dimensionless, A100 Focused)\n",
        "\n",
        "This notebook performs a systematic parameter sweep for key dimensionless coefficients in the Eholoko Fluxon Model (EFM) Nonlinear Klein-Gordon (NLKG) equation. The objective is to identify optimal parameter combinations that robustly produce the characteristic Large-Scale Structure (LSS) clustering scales (147 Mpc and 628 Mpc) predicted by EFM.\n",
        "\n",
        "Building upon previous LSS simulations (e.g., LSS_N450_T200000_..._Seeded) that demonstrated the amplification of seeded modes, this sweep focuses on refining the `g_sim` (cubic nonlinearity) and `k_efm_gravity_coupling` (self-gravity strength) parameters. These parameters are crucial for driving the 'Fluxonic Clustering' mechanism, where the scalar field self-organizes into cosmic structures without the need for dark matter.\n",
        "\n",
        "Each simulation in the sweep will operate in dimensionless units on an A100 GPU, ensuring consistency with EFM's fundamental theoretical framework.\n",
        "\n",
        "## EFM Theoretical Grounding:\n",
        "\n",
        "The simulation uses the dimensionless NLKG equation from the 'Ehokolo Fluxon Model: Unifying Cosmic Structure, Non-Gaussianity, and Gravitational Waves Across Scales' paper [1]. The parameters being swept directly influence the field's self-interaction and self-gravity, which are the driving forces behind EFM's structure formation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount-drive-instruction-sweep"
      },
      "source": [
        "## Google Drive Setup (for Colab)\n",
        "\n",
        "To ensure data and plots are saved to and retrieved from your Google Drive, please execute the following cell to mount your Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive-code-sweep"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Not in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}. Please ensure you're logged in and have granted permissions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "environment-setup-sweep"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import psutil\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from scipy.fft import fftn, fftfreq, ifftn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp as amp\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import scipy.signal  # For find_peaks\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "num_gpus_available = torch.cuda.device_count()\n",
        "available_devices_list = [torch.device(f'cuda:{i}') for i in range(num_gpus_available)]\n",
        "print(f\"Number of GPUs available: {num_gpus_available}, Available devices: {available_devices_list}\")\n",
        "if num_gpus_available > 0:\n",
        "    current_gpu_device = torch.device('cuda:0')\n",
        "    print(f\"Using GPU 0: {torch.cuda.get_device_name(current_gpu_device)}, VRAM: {torch.cuda.get_device_properties(current_gpu_device).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    current_gpu_device = torch.device('cpu')\n",
        "    print(\"No GPU available, running on CPU. Performance may be limited.\")\n",
        "print(f\"System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "\n",
        "# Define paths for checkpoints and data/plots\n",
        "checkpoint_path_lss_sweep = '/content/drive/My Drive/EFM_Simulations/checkpoints/LSS_DIMLESS_A100_Sweep/'\n",
        "data_path_lss_sweep = '/content/drive/My Drive/EFM_Simulations/data/LSS_DIMLESS_A100_Sweep/'\n",
        "os.makedirs(checkpoint_path_lss_sweep, exist_ok=True)\n",
        "os.makedirs(data_path_lss_sweep, exist_ok=True)\n",
        "print(f\"LSS Sweep Checkpoints will be saved to: {checkpoint_path_lss_sweep}\")\n",
        "print(f\"LSS Sweep Data/Plots will be saved to: {data_path_lss_sweep}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-lss-sweep"
      },
      "source": [
        "## Parameter Sweep Configuration\n",
        "\n",
        "This configuration defines the ranges for the parameter sweep. The core simulation parameters are fixed (similar to LSS_v4, but N/T_steps adjusted for faster individual runs).\n",
        "\n",
        "**Parameters for Sweep:**\n",
        "*   `g_sim`: Cubic nonlinearity coefficient [1, Section 2].\n",
        "*   `k_efm_gravity_coupling`: EFM self-gravity coupling [1, Section 2].\n",
        "\n",
        "**Fixed Core Parameters (Dimensionless):**\n",
        "*   `N`: Grid size. Set to `250` for faster sweep points.\n",
        "*   `T_steps`: Total timesteps. Set to `50000` for faster sweep points.\n",
        "*   `m_sim_unit_inv`: Mass term. Fixed at `1.0` [1].\n",
        "*   `alpha_sim`: State parameter. Fixed at `0.7` for S/T state [1].\n",
        "*   `delta_sim`: Dissipation term. Fixed at `0.0002` [1].\n",
        "*   `seeded_perturbation_amplitude`: Amplitude of seeded modes. Fixed at `1.0e-3`.\n",
        "*   `background_noise_amplitude`: Amplitude of general random noise. Fixed at `1.0e-6`.\n",
        "*   `k_seed_primary`, `k_seed_secondary`: Wavenumbers for seeded modes. Fixed as per previous optimal LSS runs.\n",
        "\n",
        "This sweep aims to systematically explore the interplay of `g_sim` and `k_efm_gravity_coupling` to find the most prominent clustering features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-config-sweep"
      },
      "outputs": [],
      "source": [
        "# --- Fixed Core Simulation Parameters (Dimensionless) ---\n",
        "base_config = {\n",
        "    'N': 250,  # Grid size for sweep runs (smaller for faster iterations)\n",
        "    'L_sim_unit': 10.0,\n",
        "    'c_sim_unit': 1.0,\n",
        "    'dt_cfl_factor': 0.001,\n",
        "    'T_steps': 50000,  # Timesteps for sweep runs (shorter for faster iterations)\n",
        "    'm_sim_unit_inv': 1.0,\n",
        "    'eta_sim': 0.01,\n",
        "    'G_sim_unit': 1.0,\n",
        "    'alpha_sim': 0.7,\n",
        "    'delta_sim': 0.0002,\n",
        "    'seeded_perturbation_amplitude': 1.0e-3,\n",
        "    'background_noise_amplitude': 1.0e-6,\n",
        "    'k_seed_primary': 2 * np.pi / (10.0 / 2.0),  # L/2 wavelength\n",
        "    'k_seed_secondary': 2 * np.pi / (10.0 / 8.0),  # L/8 wavelength (targets 1.25 dimless for 157 Mpc)\n",
        "    'history_every_n_steps': 1000,\n",
        "    'checkpoint_every_n_steps': 5000,\n",
        "}\n",
        "base_config['dx_sim_unit'] = base_config['L_sim_unit'] / base_config['N']\n",
        "base_config['dt_sim_unit'] = base_config['dt_cfl_factor'] * base_config['dx_sim_unit'] / base_config['c_sim_unit']\n",
        "\n",
        "# --- Parameters to Sweep ---\n",
        "g_values = [0.05, 0.1, 0.5]  # Values for g_sim (cubic nonlinearity)\n",
        "k_gravity_values = [0.001, 0.005, 0.01]  # Values for k_efm_gravity_coupling\n",
        "\n",
        "sweep_params = []\n",
        "for g in g_values:\n",
        "    for k_g in k_gravity_values:\n",
        "        # Create a mutable copy of base_config for each sweep point\n",
        "        config = base_config.copy()\n",
        "        config['g_sim'] = g\n",
        "        config['k_efm_gravity_coupling'] = k_g\n",
        "        config['run_id'] = (\n",
        "            f\"LSS_Sweep_N{config['N']}_T{config['T_steps']}_\" +\n",
        "            f\"g{config['g_sim']:.1e}_k{config['k_efm_gravity_coupling']:.1e}_\" +\n",
        "            f\"A100_DIMLESS_Sweep\"\n",
        "        )\n",
        "        sweep_params.append(config)\n",
        "\n",
        "print(f\"Prepared {len(sweep_params)} sweep configurations.\")\n",
        "for i, p in enumerate(sweep_params):\n",
        "    print(f\"Sweep {i+1}: g={p['g_sim']:.2g}, k_gravity={p['k_efm_gravity_coupling']:.2g}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simulation-functions-sweep"
      },
      "source": [
        "## Core Simulation Functions\n",
        "\n",
        "These functions define the EFM NLKG module, the RK4 time integration, and the energy/density norm calculation. These are identical to the optimized versions used in the main LSS simulation notebook (`lssv20.ipynb`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-simulation-functions-sweep"
      },
      "outputs": [],
      "source": [
        "class EFMLSSModule(nn.Module):\n",
        "    \"\"\"EFM Module for the NLKG equation for LSS, using dimensionless parameters.\"\"\"\n",
        "    def __init__(self, dx, m_sq, g, eta, k_gravity, G_gravity, c_sq, alpha_param, delta_param):\n",
        "        super(EFMLSSModule, self).__init__()\n",
        "        self.dx = dx\n",
        "        self.m_sq = m_sq\n",
        "        self.g = g\n",
        "        self.eta = eta\n",
        "        self.k_gravity = k_gravity\n",
        "        self.G_gravity = G_gravity\n",
        "        self.c_sq = c_sq\n",
        "        self.alpha_param = alpha_param\n",
        "        self.delta_param = delta_param\n",
        "        stencil_np = np.array([[[0,0,0],[0,1,0],[0,0,0]],\n",
        "                               [[0,1,0],[1,-6,1],[0,1,0]],\n",
        "                               [[0,0,0],[0,1,0],[0,0,0]]], dtype=np.float32)\n",
        "        self.stencil = torch.from_numpy(stencil_np / (dx**2)).to(torch.float16).view(1, 1, 3, 3, 3)\n",
        "\n",
        "    def conv_laplacian(self, phi_field):\n",
        "        stencil_dev = self.stencil.to(phi_field.device, phi_field.dtype)\n",
        "        phi_reshaped = phi_field.unsqueeze(0).unsqueeze(0)\n",
        "        phi_padded = F.pad(phi_reshaped, (1,1,1,1,1,1), mode='circular')\n",
        "        laplacian = F.conv3d(phi_padded, stencil_dev, padding=0)\n",
        "        return laplacian.squeeze(0).squeeze(0)\n",
        "\n",
        "    def nlkg_derivative_lss(self, phi, phi_dot):\n",
        "        lap_phi = self.conv_laplacian(phi)\n",
        "        potential_force = self.m_sq * phi + self.g * torch.pow(phi, 3) + self.eta * torch.pow(phi, 5)\n",
        "        grad_phi_x = (torch.roll(phi, shifts=-1, dims=0) - torch.roll(phi, shifts=1, dims=0)) / (2 * self.dx)\n",
        "        grad_phi_y = (torch.roll(phi, shifts=-1, dims=1) - torch.roll(phi, shifts=1, dims=1)) / (2 * self.dx)\n",
        "        grad_phi_z = (torch.roll(phi, shifts=-1, dims=2) - torch.roll(phi, shifts=1, dims=2)) / (2 * self.dx)\n",
        "        grad_phi_abs_sq = grad_phi_x**2 + grad_phi_y**2 + grad_phi_z**2\n",
        "        alpha_term = self.alpha_param * phi * phi_dot * grad_phi_abs_sq\n",
        "        delta_term = self.delta_param * torch.pow(phi_dot, 2) * phi\n",
        "        source_gravity = 8.0 * float(np.pi) * self.G_gravity * self.k_gravity * torch.pow(phi, 2)\n",
        "        phi_ddot = self.c_sq * lap_phi - potential_force + alpha_term + delta_term + source_gravity\n",
        "        return phi_dot, phi_ddot\n",
        "\n",
        "def update_phi_rk4_lss(phi_current: torch.Tensor, phi_dot_current: torch.Tensor,\n",
        "                       dt: float, model_instance: EFMLSSModule) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    with amp.autocast(device_type=phi_current.device.type, dtype=torch.float16):\n",
        "        k1_v, k1_a = model_instance.nlkg_derivative_lss(phi_current, phi_dot_current)\n",
        "        phi_temp_k2 = phi_current + 0.5 * dt * k1_v\n",
        "        phi_dot_temp_k2 = phi_dot_current + 0.5 * dt * k1_a\n",
        "        k2_v, k2_a = model_instance.nlkg_derivative_lss(phi_temp_k2, phi_dot_temp_k2)\n",
        "        phi_temp_k3 = phi_current + 0.5 * dt * k2_v\n",
        "        phi_dot_temp_k3 = phi_dot_current + 0.5 * dt * k2_a\n",
        "        k3_v, k3_a = model_instance.nlkg_derivative_lss(phi_temp_k3, phi_dot_temp_k3)\n",
        "        phi_temp_k4 = phi_current + dt * k3_v\n",
        "        phi_dot_temp_k4 = phi_dot_current + dt * k3_a\n",
        "        k4_v, k4_a = model_instance.nlkg_derivative_lss(phi_temp_k4, phi_dot_temp_k4)\n",
        "        phi_next = phi_current + (dt / 6.0) * (k1_v + 2*k2_v + 2*k3_v + k4_v)\n",
        "        phi_dot_next = phi_dot_current + (dt / 6.0) * (k1_a + 2*k2_a + 2*k3_a + k4_a)\n",
        "    del k1_v, k1_a, k2_v, k2_a, k3_v, k3_a, k4_v, k4_a\n",
        "    del phi_temp_k2, phi_dot_temp_k2, phi_temp_k3, phi_dot_temp_k3, phi_temp_k4, phi_dot_temp_k4\n",
        "    return phi_next, phi_dot_next\n",
        "\n",
        "def compute_total_energy_lss(phi: torch.Tensor, phi_dot: torch.Tensor,\n",
        "                             m_sq_param: float, g_param: float, eta_param: float,\n",
        "                             dx: float, c_sq_param: float) -> float:\n",
        "    vol_element = dx**3\n",
        "    phi_f32 = phi.to(dtype=torch.float32)\n",
        "    phi_dot_f32 = phi_dot.to(dtype=torch.float32)\n",
        "    with amp.autocast(device_type=phi.device.type, dtype=torch.float16):\n",
        "        kinetic_density = 0.5 * torch.pow(phi_dot_f32, 2)\n",
        "        potential_density = (0.5 * m_sq_param * torch.pow(phi_f32, 2) +\n",
        "                             0.25 * g_param * torch.pow(phi_f32, 4) +\n",
        "                             (1.0/6.0) * eta_param * torch.pow(phi_f32, 6))\n",
        "        grad_phi_x = (torch.roll(phi_f32, shifts=-1, dims=0) - torch.roll(phi_f32, shifts=1, dims=0)) / (2 * dx)\n",
        "        grad_phi_y = (torch.roll(phi_f32, shifts=-1, dims=1) - torch.roll(phi_f32, shifts=1, dims=1)) / (2 * dx)\n",
        "        grad_phi_z = (torch.roll(phi_f32, shifts=-1, dims=2) - torch.roll(phi_f32, shifts=1, dims=2)) / (2 * dx)\n",
        "        grad_phi_abs_sq = grad_phi_x**2 + grad_phi_y**2 + grad_phi_z**2\n",
        "        gradient_energy_density = 0.5 * c_sq_param * grad_phi_abs_sq\n",
        "        total_energy_current_chunk = torch.sum(kinetic_density + potential_density + gradient_energy_density) * vol_element\n",
        "    if torch.isnan(total_energy_current_chunk) or torch.isinf(total_energy_current_chunk):\n",
        "        return float('nan')\n",
        "    total_energy_val = total_energy_current_chunk.item()\n",
        "    del phi_f32, phi_dot_f32, kinetic_density, potential_density, gradient_energy_density\n",
        "    del grad_phi_x, grad_phi_y, grad_phi_z, grad_phi_abs_sq\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return total_energy_val\n",
        "\n",
        "def compute_power_spectrum_lss(phi_cpu_np_array: np.ndarray, k_val_range: list,\n",
        "                               dx_val_param: float, N_grid_param: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    if not isinstance(phi_cpu_np_array, np.ndarray):\n",
        "        phi_cpu_np_array = phi_cpu_np_array.cpu().numpy()\n",
        "    phi_fft_transform = fftn(phi_cpu_np_array.astype(np.float32))\n",
        "    power_spectrum_raw_data = np.abs(phi_fft_transform)**2 / (N_grid_param**6)\n",
        "    del phi_fft_transform\n",
        "    gc.collect()\n",
        "    kx_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    ky_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    kz_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    kxx_mesh, kyy_mesh, kzz_mesh = np.meshgrid(kx_coords, ky_coords, kz_coords, indexing='ij', sparse=True)\n",
        "    k_magnitude_values = np.sqrt(kxx_mesh**2 + kyy_mesh**2 + kzz_mesh**2)\n",
        "    del kxx_mesh, kyy_mesh, kzz_mesh, kx_coords, ky_coords, kz_coords\n",
        "    gc.collect()\n",
        "    k_bins_def = np.linspace(k_val_range[0], k_val_range[1], 50)\n",
        "    power_binned_values, _ = np.histogram(\n",
        "        k_magnitude_values.ravel(), bins=k_bins_def,\n",
        "        weights=power_spectrum_raw_data.ravel(), density=False\n",
        "    )\n",
        "    counts_in_bins, _ = np.histogram(k_magnitude_values.ravel(), bins=k_bins_def)\n",
        "    power_binned_final = np.divide(power_binned_values, counts_in_bins, out=np.zeros_like(power_binned_values), where=counts_in_bins!=0)\n",
        "    k_bin_centers_final = (k_bins_def[:-1] + k_bins_def[1:]) / 2\n",
        "    del k_magnitude_values, power_spectrum_raw_data, counts_in_bins\n",
        "    gc.collect()\n",
        "    return k_bin_centers_final, power_binned_final\n",
        "\n",
        "def compute_correlation_function_lss(phi_cpu_np_array: np.ndarray, dx_val_param: float,\n",
        "                                     N_grid_param: int, L_box_param: float) -> tuple[np.ndarray, np.ndarray]:\n",
        "    if not isinstance(phi_cpu_np_array, np.ndarray):\n",
        "        phi_cpu_np_array = phi_cpu_np_array.cpu().numpy()\n",
        "    phi_fft_transform = fftn(phi_cpu_np_array.astype(np.float32))\n",
        "    power_spectrum_raw_data = np.abs(phi_fft_transform)**2\n",
        "    del phi_fft_transform\n",
        "    gc.collect()\n",
        "    correlation_func_raw_data = ifftn(power_spectrum_raw_data).real / (N_grid_param**3)\n",
        "    del power_spectrum_raw_data\n",
        "    gc.collect()\n",
        "    indices_shifted = np.fft.ifftshift(np.arange(N_grid_param)) - (N_grid_param // 2)\n",
        "    rx_coords = indices_shifted * dx_val_param\n",
        "    ry_coords = indices_shifted * dx_val_param\n",
        "    rz_coords = indices_shifted * dx_val_param\n",
        "    rxx_mesh, ryy_mesh, rzz_mesh = np.meshgrid(rx_coords, ry_coords, rz_coords, indexing='ij', sparse=True)\n",
        "    r_magnitude_values = np.sqrt(rxx_mesh**2 + ryy_mesh**2 + rzz_mesh**2)\n",
        "    del rx_coords, ry_coords, rz_coords, rxx_mesh, ryy_mesh, rzz_mesh\n",
        "    gc.collect()\n",
        "    r_bins_def = np.linspace(0, L_box_param / 2, 50)\n",
        "    corr_binned_values, _ = np.histogram(\n",
        "        r_magnitude_values.ravel(), bins=r_bins_def,\n",
        "        weights=correlation_func_raw_data.ravel()\n",
        "    )\n",
        "    counts_in_bins, _ = np.histogram(r_magnitude_values.ravel(), bins=r_bins_def)\n",
        "    corr_binned_final = np.divide(corr_binned_values, counts_in_bins, out=np.zeros_like(corr_binned_values), where=counts_in_bins!=0)\n",
        "    r_bin_centers_final = (r_bins_def[:-1] + r_bins_def[1:]) / 2\n",
        "    del r_magnitude_values, correlation_func_raw_data, counts_in_bins\n",
        "    gc.collect()\n",
        "    return r_bin_centers_final, corr_binned_final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simulation-orchestration-sweep"
      },
      "source": [
        "## Simulation Orchestration for Parameter Sweep\n",
        "\n",
        "This section iterates through the defined sweep parameters, runs the LSS simulation for each combination, and collects the results. To manage output for multiple runs, detailed plots for each run will be saved to Google Drive, and a summary of key metrics (peak locations, fNL) will be printed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-simulation-orchestration-sweep"
      },
      "outputs": [],
      "source": [
        "def run_lss_sweep_simulation(config: dict, device: torch.device):\n",
        "    print(f\"Starting LSS Sweep Run ({config['run_id']}) on {device}...\")\n",
        "    \n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "\n",
        "    # Initial conditions: Seeded sinusoidal perturbations + background noise\n",
        "    x_coords = np.linspace(-config['L_sim_unit']/2, config['L_sim_unit']/2, config['N'], dtype=np.float32)\n",
        "    X, Y, Z = np.meshgrid(x_coords, x_coords, x_coords, indexing='ij')\n",
        "    seeded_modes_field = config['seeded_perturbation_amplitude'] * (\n",
        "        np.sin(config['k_seed_primary'] * X) +\n",
        "        np.sin(config['k_seed_secondary'] * Y) +\n",
        "        np.cos(config['k_seed_primary'] * Z)\n",
        "    )\n",
        "    random_background_noise = config['background_noise_amplitude'] * (np.random.rand(config['N'], config['N'], config['N']) - 0.5)\n",
        "    initial_phi_np = seeded_modes_field + random_background_noise\n",
        "    phi = torch.from_numpy(initial_phi_np.astype(np.float16)).to(device, dtype=torch.float16)\n",
        "    phi_dot = torch.zeros_like(phi, dtype=torch.float16, device=device)\n",
        "\n",
        "    efm_model = EFMLSSModule(\n",
        "        dx=config['dx_sim_unit'], m_sq=config['m_sim_unit_inv']**2, g=config['g_sim'], eta=config['eta_sim'],\n",
        "        k_gravity=config['k_efm_gravity_coupling'], G_gravity=config['G_sim_unit'], c_sq=config['c_sim_unit']**2,\n",
        "        alpha_param=config['alpha_sim'], delta_param=config['delta_sim']\n",
        "    ).to(device)\n",
        "    efm_model.eval()\n",
        "\n",
        "    # Removed history lists to optimize memory for many runs\n",
        "\n",
        "    sim_start_time = time.time()\n",
        "    numerical_error = False\n",
        "\n",
        "    for t_step in tqdm(range(config['T_steps']), desc=f\"Sweep Run ({config['run_id']})\"):\n",
        "        if torch.any(torch.isinf(phi)) or torch.any(torch.isnan(phi)) or \\\n",
        "           torch.any(torch.isinf(phi_dot)) or torch.any(torch.isnan(phi_dot)):\n",
        "            print(f\"\\nERROR: NaN/Inf detected in fields at step {t_step + 1}! Stopping run {config['run_id']}.\\n\")\n",
        "            numerical_error = True\n",
        "            break\n",
        "        phi, phi_dot = update_phi_rk4_lss(phi, phi_dot, config['dt_sim_unit'], efm_model)\n",
        "\n",
        "    sim_duration = time.time() - sim_start_time\n",
        "    print(f\"Sweep Run {config['run_id']} finished in {sim_duration:.2f} seconds. Error: {numerical_error}\")\n",
        "\n",
        "    # Save final state only for analysis (no history)\n",
        "    final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    final_data_filename = os.path.join(data_path_lss_sweep, f\"SWEEP_DATA_{config['run_id']}_{final_timestamp}.npz\")\n",
        "    np.savez_compressed(final_data_filename,\n",
        "                        phi_final_cpu=phi.cpu().numpy(),\n",
        "                        config_lss=config,\n",
        "                        sim_had_numerical_error=numerical_error)\n",
        "    print(f\"Final state saved to {final_data_filename}\")\n",
        "\n",
        "    del phi, phi_dot, efm_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return final_data_filename\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis-plotting-sweep"
      },
      "source": [
        "## Analysis and Plotting for Sweep Results\n",
        "\n",
        "This section defines how to analyze each sweep run's final state for clustering peaks and non-Gaussianity. A summary table will be generated for all sweep points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-analysis-plotting-sweep"
      },
      "outputs": [],
      "source": [
        "def analyze_and_plot_sweep_result(data_file_path: str, sweep_results_summary: list):\n",
        "    print(f\"Analyzing sweep result from: {data_file_path}\")\n",
        "    try:\n",
        "        data = np.load(data_file_path, allow_pickle=True)\n",
        "        phi_final_cpu = data['phi_final_cpu']\n",
        "        config = data['config_lss'].item()\n",
        "        sim_had_numerical_error = data['sim_had_numerical_error'].item()\n",
        "\n",
        "        run_summary = {\n",
        "            'run_id': config['run_id'],\n",
        "            'g_sim': config['g_sim'],\n",
        "            'k_efm_gravity_coupling': config['k_efm_gravity_coupling'],\n",
        "            'status': 'Error' if sim_had_numerical_error else 'Completed',\n",
        "            'max_phi_amplitude': np.max(np.abs(phi_final_cpu)),\n",
        "            'pk_peak_k': np.nan,\n",
        "            'pk_peak_lambda_sim': np.nan,\n",
        "            'xi_peak_r': np.nan,\n",
        "            'fNL_calc': np.nan,\n",
        "            'physical_primary_lambda': np.nan,\n",
        "            'physical_secondary_lambda': np.nan,\n",
        "            'secondary_match_percent': np.nan\n",
        "        }\n",
        "\n",
        "        if sim_had_numerical_error:\n",
        "            sweep_results_summary.append(run_summary)\n",
        "            return\n",
        "\n",
        "        # --- Power Spectrum and Correlation Function Analysis (Dimensionless) ---\n",
        "        k_min_plot_sim = 2 * np.pi / config['L_sim_unit'] * 0.5\n",
        "        k_max_plot_sim = np.pi / config['dx_sim_unit'] * 0.9\n",
        "        k_bins_sim, pk_vals_sim = compute_power_spectrum_lss(phi_final_cpu, k_val_range=[k_min_plot_sim, k_max_plot_sim], dx_val_param=config['dx_sim_unit'], N_grid_param=config['N'])\n",
        "        r_bins_sim, xi_vals_sim = compute_correlation_function_lss(phi_final_cpu, dx_val_param=config['dx_sim_unit'], N_grid_param=config['N'], L_box_param=config['L_sim_unit'])\n",
        "\n",
        "        # --- Plot P(k) and xi(r) for current run ---\n",
        "        plt.figure(figsize=(16,6))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.loglog(k_bins_sim, pk_vals_sim)\n",
        "        plt.title(f\"P(k) for g={config['g_sim']:.2g}, k_grav={config['k_efm_gravity_coupling']:.2g}\")\n",
        "        plt.xlabel('k (Dimless)'); plt.ylabel('P(k) (Dimless)'); plt.grid(True, which='both')\n",
        "        plt.axvline(config['k_seed_primary'], color='orange', linestyle='--', label=f\"Seeded k_primary ({config['k_seed_primary']:.2f})\")\n",
        "        plt.axvline(config['k_seed_secondary'], color='purple', linestyle='--', label=f\"Seeded k_secondary ({config['k_seed_secondary']:.2f})\")\n",
        "        plt.legend(); plt.xlim([k_min_plot_sim, k_max_plot_sim])\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(r_bins_sim, xi_vals_sim)\n",
        "        plt.title(f\"xi(r) for g={config['g_sim']:.2g}, k_grav={config['k_efm_gravity_coupling']:.2g}\")\n",
        "        plt.xlabel('r (Dimless)'); plt.ylabel('xi(r) (Dimless)'); plt.grid(True)\n",
        "        plt.axhline(0, color='black', linewidth=0.5)\n",
        "        plt.axvline(config['L_sim_unit'] / 2.0, color='orange', linestyle='--', label=f\"Seeded lambda1 ({config['L_sim_unit'] / 2.0:.2f})\")\n",
        "        plt.axvline(config['L_sim_unit'] / 8.0, color='purple', linestyle='--', label=f\"Seeded lambda2 ({config['L_sim_unit'] / 8.0:.2f})\")\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plot_filename_obs = os.path.join(data_path_lss_sweep, f\"sweep_obs_{config['run_id']}.png\")\n",
        "        plt.savefig(plot_filename_obs)\n",
        "        plt.close()\n",
        "\n",
        "        # --- Identify and Quantify Emergent Dimensionless Scales ---\n",
        "        # Find prominent peaks in P(k)\n",
        "        pk_peaks, pk_properties = scipy.signal.find_peaks(pk_vals_sim, height=np.mean(pk_vals_sim)*5, distance=5)\n",
        "        if len(pk_peaks) > 0:\n",
        "            idx_primary_pk_peak = pk_peaks[np.argmin(np.abs(k_bins_sim[pk_peaks] - config['k_seed_primary']))]\n",
        "            emergent_k_peak_primary = k_bins_sim[idx_primary_pk_peak]\n",
        "            if emergent_k_peak_primary > 0:\n",
        "                emergent_lambda_peak_primary = 2*np.pi / emergent_k_peak_primary\n",
        "            run_summary['pk_peak_k'] = emergent_k_peak_primary\n",
        "            run_summary['pk_peak_lambda_sim'] = emergent_lambda_peak_primary\n",
        "\n",
        "        # Find prominent peaks in xi(r)\n",
        "        xi_positive_part = xi_vals_sim[r_bins_sim > 0.1]\n",
        "        r_positive_part = r_bins_sim[r_bins_sim > 0.1]\n",
        "        xi_peaks_r, xi_properties = scipy.signal.find_peaks(xi_positive_part, height=np.max(xi_positive_part)*0.1, distance=5)\n",
        "        if len(xi_peaks_r) > 0:\n",
        "            emergent_r_peak_primary = r_positive_part[xi_peaks_r[0]]\n",
        "            run_summary['xi_peak_r'] = emergent_r_peak_primary\n",
        "\n",
        "        # --- Physical Interpretation and Match to EFM Predictions ---\n",
        "        EFM_PRIMARY_LSS_Mpc = 628.0  # Mpc [1]\n",
        "        EFM_SECONDARY_LSS_Mpc = 157.0  # Mpc (BAO-like) [1]\n",
        "\n",
        "        if not np.isnan(run_summary['pk_peak_lambda_sim']) and run_summary['pk_peak_lambda_sim'] > 0:\n",
        "            scaling_factor_L = EFM_PRIMARY_LSS_Mpc / run_summary['pk_peak_lambda_sim']\n",
        "            physical_primary_lambda_seeded = (config['L_sim_unit'] / 2.0) * scaling_factor_L\n",
        "            physical_secondary_lambda_seeded = (config['L_sim_unit'] / 8.0) * scaling_factor_L\n",
        "            secondary_match_percent = abs((physical_secondary_lambda_seeded - EFM_SECONDARY_LSS_Mpc) / EFM_SECONDARY_LSS_Mpc) * 100\n",
        "            \n",
        "            run_summary['physical_primary_lambda'] = physical_primary_lambda_seeded\n",
        "            run_summary['physical_secondary_lambda'] = physical_secondary_lambda_seeded\n",
        "            run_summary['secondary_match_percent'] = secondary_match_percent\n",
        "\n",
        "        # --- Non-Gaussianity Analysis (fNL) ---\n",
        "        N_grid = config['N']\n",
        "        dx_val = config['dx_sim_unit']\n",
        "        rho_final_np_for_fNL = (config['k_efm_gravity_coupling'] * phi_final_cpu**2).astype(np.float32)\n",
        "        rhok_fft = fftn(rho_final_np_for_fNL)\n",
        "        kx_coords_f = fftfreq(N_grid, d=dx_val) * 2 * np.pi\n",
        "        ky_coords_f = fftfreq(N_grid, d=dx_val) * 2 * np.pi\n",
        "        kz_coords_f = fftfreq(N_grid, d=dx_val) * 2 * np.pi\n",
        "        k_magnitude_f = np.sqrt(kx_coords_f[:, None, None]**2 + ky_coords_f[None, :, None]**2 + kz_coords_f[None, None, :]**2)\n",
        "        \n",
        "        target_k_for_fNL_sim = config['k_seed_primary']  # Use the primary seeded k-mode\n",
        "        k_tolerance = 0.05 * target_k_for_fNL_sim \n",
        "        mask_fNL = (k_magnitude_f > (target_k_for_fNL_sim - k_tolerance)) & (k_magnitude_f < (target_k_for_fNL_sim + k_tolerance))\n",
        "        \n",
        "        if not np.any(mask_fNL) and len(pk_peaks) > 0:\n",
        "            k_tolerance = 0.5 * target_k_for_fNL_sim\n",
        "            mask_fNL = (k_magnitude_f > (target_k_for_fNL_sim - k_tolerance)) & (k_magnitude_f < (target_k_for_fNL_sim + k_tolerance))\n",
        "        \n",
        "        fNL_calculated = np.nan\n",
        "        if np.any(mask_fNL):\n",
        "            rhok_fft_f32 = rhok_fft.astype(np.float32)\n",
        "            B_simplified = np.mean(np.abs(rhok_fft_f32 * np.roll(rhok_fft_f32, shifts=-1, axis=0) * np.roll(rhok_fft_f32, shifts=-1, axis=1))[mask_fNL])\n",
        "            P_simplified = np.mean(np.abs(rhok_fft_f32)**2)[mask_fNL]\n",
        "            if P_simplified > 1e-30:\n",
        "                fNL_calculated = (5/3) * B_simplified / (3 * P_simplified**2)\n",
        "        run_summary['fNL_calc'] = fNL_calculated\n",
        "        \n",
        "        sweep_results_summary.append(run_summary)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error analyzing/plotting sweep result for {data_file_path}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        # Ensure summary reflects error\n",
        "        run_summary = {\n",
        "            'run_id': os.path.basename(data_file_path),\n",
        "            'status': f'Analysis Error: {e}',\n",
        "            'g_sim': 'N/A',\n",
        "            'k_efm_gravity_coupling': 'N/A',\n",
        "            'max_phi_amplitude': 'N/A',\n",
        "            'pk_peak_k': 'N/A',\n",
        "            'pk_peak_lambda_sim': 'N/A',\n",
        "            'xi_peak_r': 'N/A',\n",
        "            'fNL_calc': 'N/A',\n",
        "            'physical_primary_lambda': 'N/A',\n",
        "            'physical_secondary_lambda': 'N/A',\n",
        "            'secondary_match_percent': 'N/A'\n",
        "        }\n",
        "        sweep_results_summary.append(run_summary)\n",
        "\n",
        "    del phi_final_cpu\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-orchestration-sweep"
      },
      "source": [
        "## Main Orchestration Loop\n",
        "\n",
        "This is the primary execution block that runs all sweep simulations and performs the analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-main-orchestration-sweep"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # --- Colab specific setup ---\n",
        "    # Define paths for checkpoints and data/plots\n",
        "    checkpoint_path_lss_sweep = '/content/drive/My Drive/EFM_Simulations/checkpoints/LSS_DIMLESS_A100_Sweep/'\n",
        "    data_path_lss_sweep = '/content/drive/My Drive/EFM_Simulations/data/LSS_DIMLESS_A100_Sweep/'\n",
        "\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "        os.makedirs(checkpoint_path_lss_sweep, exist_ok=True)\n",
        "        os.makedirs(data_path_lss_sweep, exist_ok=True)\n",
        "    except ImportError:\n",
        "        print(\"Not in Google Colab environment. Skipping Google Drive mount.\")\n",
        "        checkpoint_path_lss_sweep = './EFM_Simulations/checkpoints/LSS_DIMLESS_A100_Sweep/'\n",
        "        data_path_lss_sweep = './EFM_Simulations/data/LSS_DIMLESS_A100_Sweep/'\n",
        "        os.makedirs(checkpoint_path_lss_sweep, exist_ok=True)\n",
        "        os.makedirs(data_path_lss_sweep, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}. Please ensure you're logged in and have granted permissions.\")\n",
        "        checkpoint_path_lss_sweep = './EFM_Simulations/checkpoints/LSS_DIMLESS_A100_Sweep/'\n",
        "        data_path_lss_sweep = './EFM_Simulations/data/LSS_DIMLESS_A100_Sweep/'\n",
        "        os.makedirs(checkpoint_path_lss_sweep, exist_ok=True)\n",
        "        os.makedirs(data_path_lss_sweep, exist_ok=True)\n",
        "\n",
        "    print(f\"LSS Sweep Checkpoints will be saved to: {checkpoint_path_lss_sweep}\")\n",
        "    print(f\"LSS Sweep Data/Plots will be saved to: {data_path_lss_sweep}\")\n",
        "\n",
        "    # --- Prepare sweep configurations (defined in previous cell `code-config-sweep`) ---\n",
        "    # Ensure base_config, g_values, k_gravity_values, and sweep_params are defined there\n",
        "    # (Re-run that cell if needed to make sweep_params available)\n",
        "    \n",
        "    # --- Explicitly define base_config and sweep_params here for robustness ---\n",
        "    # This is a copy-paste from the 'code-config-sweep' cell\n",
        "    base_config = {\n",
        "        'N': 250,  # Grid size for sweep runs (smaller for faster iterations)\n",
        "        'L_sim_unit': 10.0,\n",
        "        'c_sim_unit': 1.0,\n",
        "        'dt_cfl_factor': 0.001,\n",
        "        'T_steps': 50000,  # Timesteps for sweep runs (shorter for faster iterations)\n",
        "        'm_sim_unit_inv': 1.0,\n",
        "        'eta_sim': 0.01,\n",
        "        'G_sim_unit': 1.0,\n",
        "        'alpha_sim': 0.7,\n",
        "        'delta_sim': 0.0002,\n",
        "        'seeded_perturbation_amplitude': 1.0e-3,\n",
        "        'background_noise_amplitude': 1.0e-6,\n",
        "        'k_seed_primary': 2 * np.pi / (10.0 / 2.0),  # L/2 wavelength\n",
        "        'k_seed_secondary': 2 * np.pi / (10.0 / 8.0),  # L/8 wavelength (targets 1.25 dimless for 157 Mpc)\n",
        "        'history_every_n_steps': 1000,\n",
        "        'checkpoint_every_n_steps': 5000,\n",
        "    }\n",
        "    base_config['dx_sim_unit'] = base_config['L_sim_unit'] / base_config['N']\n",
        "    base_config['dt_sim_unit'] = base_config['dt_cfl_factor'] * base_config['dx_sim_unit'] / base_config['c_sim_unit']\n",
        "\n",
        "    g_values = [0.05, 0.1, 0.5]\n",
        "    k_gravity_values = [0.001, 0.005, 0.01]\n",
        "\n",
        "    sweep_params = []\n",
        "    for g in g_values:\n",
        "        for k_g in k_gravity_values:\n",
        "            config = base_config.copy()\n",
        "            config['g_sim'] = g\n",
        "            config['k_efm_gravity_coupling'] = k_g\n",
        "            config['run_id'] = (\n",
        "                f\"LSS_Sweep_N{config['N']}_T{config['T_steps']}_\" +\n",
        "                f\"g{config['g_sim']:.1e}_k{config['k_efm_gravity_coupling']:.1e}_\" +\n",
        "                f\"A100_DIMLESS_Sweep\"\n",
        "            )\n",
        "            sweep_params.append(config)\n",
        "    # --- End of explicit definition ---\n",
        "\n",
        "    print(f\"Prepared {len(sweep_params)} sweep configurations.\")\n",
        "\n",
        "    # Determine the device\n",
        "    main_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    sweep_results_summary = []\n",
        "\n",
        "    for i, config_lss_run_current in enumerate(sweep_params):  # Use a different variable name to avoid conflict\n",
        "        print(f\"\\n--- Running Sweep Point {i+1}/{len(sweep_params)}: g={config_lss_run_current['g_sim']:.2g}, k_grav={config_lss_run_current['k_efm_gravity_coupling']:.2g} ---\\n\")\n",
        "        \n",
        "        try:\n",
        "            # Ensure unique directory for each run's checkpoints if needed, otherwise it's just general sweep dir.\n",
        "            # For simplicity, saving all sweep data to data_path_lss_sweep directly as SWEEP_DATA_run_id.npz\n",
        "            # No specific checkpointing directories per run for this sweep.\n",
        "            pass  # No per-run checkpoint directory setup here to simplify\n",
        "        except Exception as e_clean:\n",
        "            print(f\"Warning: Could not clean checkpoint directory for {config_lss_run_current['run_id']}: {e_clean}. Proceeding anyway.\")\n",
        "\n",
        "        final_data_file_path = run_lss_sweep_simulation(config_lss_run_current, main_device)\n",
        "\n",
        "        if final_data_file_path:\n",
        "            analyze_and_plot_sweep_result(final_data_file_path, sweep_results_summary)\n",
        "        else:\n",
        "            run_summary = {\n",
        "                'run_id': config_lss_run_current['run_id'],\n",
        "                'status': 'Simulation Failed to Produce File',\n",
        "                'g_sim': config_lss_run_current['g_sim'],\n",
        "                'k_efm_gravity_coupling': config_lss_run_current['k_efm_gravity_coupling'],\n",
        "                'max_phi_amplitude': 'N/A',\n",
        "                'pk_peak_k': 'N/A',\n",
        "                'pk_peak_lambda_sim': 'N/A',\n",
        "                'xi_peak_r': 'N/A',\n",
        "                'fNL_calc': 'N/A',\n",
        "                'physical_primary_lambda': 'N/A',\n",
        "                'physical_secondary_lambda': 'N/A',\n",
        "                'secondary_match_percent': 'N/A'\n",
        "            }\n",
        "            sweep_results_summary.append(run_summary)\n",
        "\n",
        "    print(f\"\\n--- Parameter Sweep Summary ({datetime.now().strftime('%Y%m%d_%H%M%S')}) ---\\n\")\n",
        "    # Print a formatted table of results\n",
        "    headers = [\"Run ID\", \"g_sim\", \"k_grav\", \"Status\", \"Max Phi\", \"PK Peak k\", \"PK Peak L\", \"Xi Peak r\", \"fNL\", \"Phys L1\", \"Phys L2\", \"L2 Match %\"]\n",
        "    \n",
        "    # Helper to format values for table\n",
        "    def format_val_for_table(val):\n",
        "        if isinstance(val, float) and np.isnan(val):\n",
        "            return 'N/A'\n",
        "        if isinstance(val, (float, np.float32, np.float64)):\n",
        "            return f\"{val:.2e}\"\n",
        "        return str(val)\n",
        "\n",
        "    # Print header row\n",
        "    header_row_str = \"{:<30} {:<8} {:<8} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(*headers)\n",
        "    print(header_row_str)\n",
        "    print(\"-\" * len(header_row_str))  # Adjust separator length dynamically\n",
        "\n",
        "    # Print data rows\n",
        "    for res in sweep_results_summary:\n",
        "        row_vals = [\n",
        "            res['run_id'],\n",
        "            format_val_for_table(res['g_sim']),\n",
        "            format_val_for_table(res['k_efm_gravity_coupling']),\n",
        "            res['status'],\n",
        "            format_val_for_table(res['max_phi_amplitude']),\n",
        "            format_val_for_table(res['pk_peak_k']),\n",
        "            format_val_for_table(res['pk_peak_lambda_sim']),\n",
        "            format_val_for_table(res['xi_peak_r']),\n",
        "            format_val_for_table(res['fNL_calc']),\n",
        "            format_val_for_table(res['physical_primary_lambda']),\n",
        "            format_val_for_table(res['physical_secondary_lambda']),\n",
        "            format_val_for_table(res['secondary_match_percent'])\n",
        "        ]\n",
        "        print(\"{:<30} {:<8} {:<8} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(*row_vals))\n",
        "    print(\"-\" * len(header_row_str))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}