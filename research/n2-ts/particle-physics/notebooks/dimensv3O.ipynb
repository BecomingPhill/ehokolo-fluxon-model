{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "intro-lss-param-sweep-v3"
      },
      "source": [
        "# EFM Large-Scale Structure (LSS) Parameter Sweep (Dimensionless, A100 Focused) - v3\n",
        "\n",
        "This notebook initiates the **third systematic parameter sweep (v3)** for key dimensionless coefficients in the Eholoko Fluxon Model (EFM) Nonlinear Klein-Gordon (NLKG) equation. The objective is to identify optimal parameter combinations that robustly produce the characteristic Large-Scale Structure (LSS) clustering scales (147 Mpc and 628 Mpc) predicted by EFM.\n",
        "\n",
        "Building upon previous LSS simulations that explored `g_sim` and `k_efm_gravity_coupling` (Sweep v1), and `m_sim_unit_inv` and `alpha_sim` (Sweep v2), this **v3 sweep focuses on refining the `eta_sim` (quintic nonlinearity) and `delta_sim` (dissipation) parameters.** These parameters are crucial for fine-tuning the stability, higher-order nonlinear interactions, and energy dissipation within the EFM field, which can subtly influence emergent structures.\n",
        "\n",
        "Each simulation in this sweep will operate in dimensionless units on an an **optimally-utilized A100 GPU**, ensuring consistency with EFM's fundamental theoretical framework. This notebook is designed to be run in parallel in a separate Colab session to expedite the parameter search.\n",
        "\n",
        "## EFM Theoretical Grounding:\n",
        "\n",
        "The simulation uses the dimensionless NLKG equation from the 'Ehokolo Fluxon Model: Unifying Cosmic Structure, Non-Gaussianity, and Gravitational Waves Across Scales' paper [1]. The parameters being swept directly influence the field's self-interaction and self-gravity, which are the driving forces behind EFM's structure formation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mount-drive-instruction-sweep-v3"
      },
      "source": [
        "## Google Drive Setup (for Colab)\n",
        "\n",
        "To ensure data and plots are saved to and retrieved from your Google Drive, please execute the following cell to mount your Drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mount-drive-code-sweep-v3"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"Google Drive mounted successfully.\")\n",
        "except ImportError:\n",
        "    print(\"Not in Google Colab environment. Skipping Google Drive mount.\")\n",
        "except Exception as e:\n",
        "    print(f\"Error mounting Google Drive: {e}. Please ensure you're logged in and have granted permissions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "environment-setup-sweep-v3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import gc\n",
        "import psutil\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import datetime\n",
        "from scipy.fft import fftn, fftfreq, ifftn\n",
        "import torch.nn.functional as F\n",
        "import torch.amp as amp\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import scipy.signal  # For find_peaks\n",
        "\n",
        "# Ensure current_gpu_device is defined early\n",
        "if torch.cuda.is_available():\n",
        "    current_gpu_device = torch.device('cuda:0')\n",
        "else:\n",
        "    current_gpu_device = torch.device('cpu')\n",
        "\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Is torch.compile available? {hasattr(torch, 'compile')}\")\n",
        "num_gpus_available = torch.cuda.device_count()\n",
        "available_devices_list = [torch.device(f'cuda:{i}') for i in range(num_gpus_available)]\n",
        "print(f\"Number of GPUs available: {num_gpus_available}, Available devices: {available_devices_list}\")\n",
        "print(f\"Using compute device: {current_gpu_device}\")\n",
        "print(f\"System RAM: {psutil.virtual_memory().total / 1e9:.2f} GB\")\n",
        "\n",
        "# Define paths for checkpoints and data/plots - NOTE: Using _v3 for this sweep\n",
        "checkpoint_path_lss_sweep = '/content/drive/My Drive/EFM_Simulations/checkpoints/LSS_DIMLESS_A100_Sweep_v3/'\n",
        "data_path_lss_sweep = '/content/drive/My Drive/EFM_Simulations/data/LSS_DIMLESS_A100_Sweep_v3/'\n",
        "os.makedirs(checkpoint_path_lss_sweep, exist_ok=True)\n",
        "os.makedirs(data_path_lss_sweep, exist_ok=True)\n",
        "print(f\"LSS Sweep Checkpoints will be saved to: {checkpoint_path_lss_sweep}\")\n",
        "print(f\"LSS Sweep Data/Plots will be saved to: {data_path_lss_sweep}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "config-lss-sweep-v3"
      },
      "source": [
        "## Parameter Sweep Configuration - v3\n",
        "\n",
        "This configuration defines the ranges for the parameter sweep. The core simulation parameters are fixed (N/T_steps adjusted for faster individual runs).\n",
        "\n",
        "**Parameters for Sweep (9 runs total for Colab Pro credits):**\n",
        "*   `eta_sim`: Quintic nonlinearity coefficient [1, Section 2].\n",
        "*   `delta_sim`: Dissipation term [1, Section 2].\n",
        "\n",
        "**Fixed Core Parameters (Dimensionless):**\n",
        "*   `N`: Grid size. **Set to `400` for better GPU utilization and higher resolution for LSS analysis.**\n",
        "*   `T_steps`: Total timesteps. Set to `50000` for sufficient evolution time.\n",
        "*   `m_sim_unit_inv`: Mass term. Fixed at `1.0`.\n",
        "*   `alpha_sim`: State parameter. Fixed at `0.7` for S/T state.\n",
        "*   `g_sim`: Cubic nonlinearity. Fixed at `0.1`.\n",
        "*   `k_efm_gravity_coupling`: Self-gravity strength. Fixed at `0.005`.\n",
        "*   `seeded_perturbation_amplitude`: Amplitude of seeded modes. Fixed at `1.0e-3`.\n",
        "*   `background_noise_amplitude`: Amplitude of general random noise. Fixed at `1.0e-6`.\n",
        "*   `k_seed_primary`, `k_seed_secondary`: Wavenumbers for seeded modes. Fixed as per previous optimal LSS runs.\n",
        "\n",
        "This sweep aims to systematically explore the interplay of `eta_sim` and `delta_sim` to fine-tune the emergent clustering features, building on insights from the previous sweeps.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-config-sweep-v3"
      },
      "outputs": [],
      "source": [
        "# --- Fixed Core Simulation Parameters (Dimensionless) ---\n",
        "base_config = {\n",
        "    'N': 400,  # Grid size for sweep runs (Increased for better GPU utilization)\n",
        "    'L_sim_unit': 10.0,\n",
        "    'c_sim_unit': 1.0,\n",
        "    'dt_cfl_factor': 0.001,\n",
        "    'T_steps': 50000,  # Timesteps for sweep runs\n",
        "    'm_sim_unit_inv': 1.0, # Fixed for this sweep\n",
        "    'g_sim': 0.1,  # Fixed for this sweep\n",
        "    'k_efm_gravity_coupling': 0.005, # Fixed for this sweep\n",
        "    'alpha_sim': 0.7, # Fixed for this sweep\n",
        "    'G_sim_unit': 1.0,\n",
        "    'seeded_perturbation_amplitude': 1.0e-3,\n",
        "    'background_noise_amplitude': 1.0e-6,\n",
        "    'k_seed_primary': 2 * np.pi / (10.0 / 2.0),  # L/2 wavelength (5.00 dimensionless)\n",
        "    'k_seed_secondary': 2 * np.pi / (10.0 / 8.0),  # L/8 wavelength (1.25 dimensionless)\n",
        "    'history_every_n_steps': 1000,\n",
        "    'checkpoint_every_n_steps': 5000,\n",
        "}\n",
        "base_config['dx_sim_unit'] = base_config['L_sim_unit'] / base_config['N']\n",
        "base_config['dt_sim_unit'] = base_config['dt_cfl_factor'] * base_config['dx_sim_unit'] / base_config['c_sim_unit']\n",
        "\n",
        "# --- Parameters to Sweep (9 runs total) ---\n",
        "eta_values = [0.001, 0.01, 0.1]  # Values for eta_sim (quintic nonlinearity)\n",
        "delta_values = [0.0001, 0.0002, 0.0005]  # Values for delta_sim (dissipation)\n",
        "\n",
        "sweep_params = []\n",
        "for eta_val in eta_values:\n",
        "    for delta_val in delta_values:\n",
        "        config = base_config.copy()\n",
        "        config['eta_sim'] = eta_val\n",
        "        config['delta_sim'] = delta_val\n",
        "        # Update run_id for v3 sweep\n",
        "        config['run_id'] = (\n",
        "            f\"LSS_Sweep_N{config['N']}_T{config['T_steps']}_\" +\n",
        "            f\"eta{config['eta_sim']:.1e}_delta{config['delta_sim']:.1e}_\" +\n",
        "            f\"m{config['m_sim_unit_inv']:.1e}_alpha{config['alpha_sim']:.1e}_\" +\n",
        "            f\"g{config['g_sim']:.1e}_k{config['k_efm_gravity_coupling']:.1e}_\" +\n",
        "            f\"A100_DIMLESS_Sweep_v3\" # Denotes this as the third major sweep\n",
        "        )\n",
        "        sweep_params.append(config)\n",
        "\n",
        "print(f\"Prepared {len(sweep_params)} sweep configurations.\")\n",
        "for i, p in enumerate(sweep_params):\n",
        "    print(f\"Sweep {i+1}: eta_sim={p['eta_sim']:.2g}, delta_sim={p['delta_sim']:.2g}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simulation-functions-sweep-v3"
      },
      "source": [
        "## Core Simulation Functions\n",
        "\n",
        "These functions define the EFM NLKG module, the RK4 time integration, and the energy/density norm calculation. These are identical to the optimized versions used in the main LSS simulation notebook (`lssv20.ipynb`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-simulation-functions-sweep-v3"
      },
      "outputs": [],
      "source": [
        "class EFMLSSModule(nn.Module):\n",
        "    \"\"\"EFM Module for the NLKG equation for LSS, using dimensionless parameters.\"\"\"\n",
        "    def __init__(self, dx, m_sq, g, eta, k_gravity, G_gravity, c_sq, alpha_param, delta_param):\n",
        "        super(EFMLSSModule, self).__init__()\n",
        "        self.dx = dx\n",
        "        self.m_sq = m_sq\n",
        "        self.g = g\n",
        "        self.eta = eta\n",
        "        self.k_gravity = k_gravity\n",
        "        self.G_gravity = G_gravity\n",
        "        self.c_sq = c_sq\n",
        "        self.alpha_param = alpha_param\n",
        "        self.delta_param = delta_param\n",
        "        # Stencil for Laplacian\n",
        "        stencil_np = np.array([[[0,0,0],[0,1,0],[0,0,0]],\n",
        "                               [[0,1,0],[1,-6,1],[0,1,0]],\n",
        "                               [[0,0,0],[0,1,0],[0,0,0]]], dtype=np.float32)\n",
        "        self.stencil = torch.from_numpy(stencil_np / (dx**2)).to(torch.float16).view(1, 1, 3, 3, 3)\n",
        "\n",
        "    def conv_laplacian(self, phi_field):\n",
        "        stencil_dev = self.stencil.to(phi_field.device, phi_field.dtype)\n",
        "        phi_reshaped = phi_field.unsqueeze(0).unsqueeze(0)\n",
        "        phi_padded = F.pad(phi_reshaped, (1,1,1,1,1,1), mode='circular')\n",
        "        laplacian = F.conv3d(phi_padded, stencil_dev, padding=0)\n",
        "        return laplacian.squeeze(0).squeeze(0)\n",
        "\n",
        "    def nlkg_derivative_lss(self, phi, phi_dot):\n",
        "        lap_phi = self.conv_laplacian(phi)\n",
        "        potential_force = self.m_sq * phi + self.g * torch.pow(phi, 3) + self.eta * torch.pow(phi, 5)\n",
        "        grad_phi_x = (torch.roll(phi, shifts=-1, dims=0) - torch.roll(phi, shifts=1, dims=0)) / (2 * self.dx)\n",
        "        grad_phi_y = (torch.roll(phi, shifts=-1, dims=1) - torch.roll(phi, shifts=1, dims=1)) / (2 * self.dx)\n",
        "        grad_phi_z = (torch.roll(phi, shifts=-1, dims=2) - torch.roll(phi, shifts=1, dims=2)) / (2 * self.dx)\n",
        "        grad_phi_abs_sq = grad_phi_x**2 + grad_phi_y**2 + grad_phi_z**2\n",
        "        alpha_term = self.alpha_param * phi * phi_dot * grad_phi_abs_sq\n",
        "        delta_term = self.delta_param * torch.pow(phi_dot, 2) * phi\n",
        "        source_gravity = 8.0 * float(np.pi) * self.G_gravity * self.k_gravity * torch.pow(phi, 2)\n",
        "        phi_ddot = self.c_sq * lap_phi - potential_force + alpha_term + delta_term + source_gravity\n",
        "        return phi_dot, phi_ddot\n",
        "\n",
        "def update_phi_rk4_lss(phi_current: torch.Tensor, phi_dot_current: torch.Tensor,\n",
        "                       dt: float, model_instance: EFMLSSModule) -> tuple[torch.Tensor, torch.Tensor]:\n",
        "    # Using torch.compile with this function will optimize the RK4 steps.\n",
        "    # Ensure the model_instance itself is also compiled if its methods are called here.\n",
        "    with amp.autocast(device_type=phi_current.device.type, dtype=torch.float16):\n",
        "        k1_v, k1_a = model_instance.nlkg_derivative_lss(phi_current, phi_dot_current)\n",
        "        phi_temp_k2 = phi_current + 0.5 * dt * k1_v\n",
        "        phi_dot_temp_k2 = phi_dot_current + 0.5 * dt * k1_a\n",
        "        k2_v, k2_a = model_instance.nlkg_derivative_lss(phi_temp_k2, phi_dot_temp_k2)\n",
        "        phi_temp_k3 = phi_current + 0.5 * dt * k2_v\n",
        "        phi_dot_temp_k3 = phi_dot_current + 0.5 * dt * k2_a\n",
        "        k3_v, k3_a = model_instance.nlkg_derivative_lss(phi_temp_k3, phi_dot_temp_k3)\n",
        "        phi_temp_k4 = phi_current + dt * k3_v\n",
        "        phi_dot_temp_k4 = phi_dot_current + dt * k3_a\n",
        "        k4_v, k4_a = model_instance.nlkg_derivative_lss(phi_temp_k4, phi_dot_temp_k4)\n",
        "        phi_next = phi_current + (dt / 6.0) * (k1_v + 2*k2_v + 2*k3_v + k4_v)\n",
        "        phi_dot_next = phi_dot_current + (dt / 6.0) * (k1_a + 2*k2_a + 2*k3_a + k4_a)\n",
        "    del k1_v, k1_a, k2_v, k2_a, k3_v, k3_a, k4_v, k4_a\n",
        "    del phi_temp_k2, phi_dot_temp_k2, phi_temp_k3, phi_dot_temp_k3, phi_temp_k4, phi_dot_temp_k4\n",
        "    return phi_next, phi_dot_next\n",
        "\n",
        "def compute_total_energy_lss(phi: torch.Tensor, phi_dot: torch.Tensor,\n",
        "                             m_sq_param: float, g_param: float, eta_param: float,\n",
        "                             dx: float, c_sq_param: float) -> float:\n",
        "    vol_element = dx**3\n",
        "    phi_f32 = phi.to(dtype=torch.float32)\n",
        "    phi_dot_f32 = phi_dot.to(dtype=torch.float32)\n",
        "    with amp.autocast(device_type=phi.device.type, dtype=torch.float16):\n",
        "        kinetic_density = 0.5 * torch.pow(phi_dot_f32, 2)\n",
        "        potential_density = (0.5 * m_sq_param * torch.pow(phi_f32, 2) +\n",
        "                             0.25 * g_param * torch.pow(phi_f32, 4) +\n",
        "                             (1.0/6.0) * eta_param * torch.pow(phi_f32, 6))\n",
        "        grad_phi_x = (torch.roll(phi_f32, shifts=-1, dims=0) - torch.roll(phi_f32, shifts=1, dims=0)) / (2 * dx)\n",
        "        grad_phi_y = (torch.roll(phi_f32, shifts=-1, dims=1) - torch.roll(phi_f32, shifts=1, dims=1)) / (2 * dx)\n",
        "        grad_phi_z = (torch.roll(phi_f32, shifts=-1, dims=2) - torch.roll(phi_f32, shifts=1, dims=2)) / (2 * dx)\n",
        "        grad_phi_abs_sq = grad_phi_x**2 + grad_phi_y**2 + grad_phi_z**2\n",
        "        gradient_energy_density = 0.5 * c_sq_param * grad_phi_abs_sq\n",
        "        total_energy_current_chunk = torch.sum(kinetic_density + potential_density + gradient_energy_density) * vol_element\n",
        "    if torch.isnan(total_energy_current_chunk) or torch.isinf(total_energy_current_chunk):\n",
        "        return float('nan')\n",
        "    total_energy_val = total_energy_current_chunk.item()\n",
        "    del phi_f32, phi_dot_f32, kinetic_density, potential_density, gradient_energy_density\n",
        "    del grad_phi_x, grad_phi_y, grad_phi_z, grad_phi_abs_sq\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    return total_energy_val\n",
        "\n",
        "def compute_power_spectrum_lss(phi_cpu_np_array: np.ndarray, k_val_range: list,\n",
        "                               dx_val_param: float, N_grid_param: int) -> tuple[np.ndarray, np.ndarray]:\n",
        "    phi_to_fft = phi_cpu_np_array.astype(np.float32)\n",
        "    rho_field_np = phi_to_fft**2 \n",
        "    fourier_transform = fftn(rho_field_np)\n",
        "    \n",
        "    power_spectrum_raw_data = np.abs(fourier_transform)**2 / (N_grid_param**6) \n",
        "    \n",
        "    kx_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    ky_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    kz_coords = fftfreq(N_grid_param, d=dx_val_param) * 2 * np.pi\n",
        "    \n",
        "    kxx_mesh, kyy_mesh, kzz_mesh = np.meshgrid(kx_coords, ky_coords, kz_coords, indexing='ij', sparse=True)\n",
        "    k_magnitude_values = np.sqrt(kxx_mesh**2 + kyy_mesh**2 + kzz_mesh**2)\n",
        "\n",
        "    k_bins_def = np.linspace(k_val_range[0], k_val_range[1], 50) \n",
        "    \n",
        "    power_binned_values, _ = np.histogram(\n",
        "        k_magnitude_values.ravel(), bins=k_bins_def,\n",
        "        weights=power_spectrum_raw_data.ravel(), density=False\n",
        "    )\n",
        "    counts_in_bins, _ = np.histogram(k_magnitude_values.ravel(), bins=k_bins_def)\n",
        "    \n",
        "    power_binned_final = np.divide(power_binned_values, counts_in_bins, out=np.zeros_like(power_binned_values), where=counts_in_bins!=0)\n",
        "    k_bin_centers_final = (k_bins_def[:-1] + k_bins_def[1:]) / 2\n",
        "\n",
        "    return k_bin_centers_final, power_binned_final\n",
        "\n",
        "def compute_correlation_function_lss(phi_cpu_np_array: np.ndarray, dx_val_param: float,\n",
        "                                     N_grid_param: int, L_box_param: float) -> tuple[np.ndarray, np.ndarray]:\n",
        "    phi_to_fft = phi_cpu_np_array.astype(np.float32)\n",
        "    rho_field_np = phi_to_fft**2\n",
        "    \n",
        "    rho_mean = np.mean(rho_field_np)\n",
        "    rho_k = fftn(rho_field_np - rho_mean)\n",
        "    power_spectrum_rho = np.abs(rho_k)**2\n",
        "\n",
        "    correlation_func_raw_data = ifftn(power_spectrum_rho).real # Result of iFFT\n",
        "\n",
        "    if rho_mean**2 > 1e-15:\n",
        "        xi_normalized = correlation_func_raw_data / (N_grid_param**3 * rho_mean**2)\n",
        "    else:\n",
        "        xi_normalized = np.zeros_like(correlation_func_raw_data)\n",
        "\n",
        "    indices_shifted = np.fft.ifftshift(np.arange(N_grid_param)) - (N_grid_param // 2)\n",
        "    rx_coords = indices_shifted * dx_val_param\n",
        "    ry_coords = indices_shifted * dx_val_param\n",
        "    rz_coords = indices_shifted * dx_val_param\n",
        "    rxx_mesh, ryy_mesh, rzz_mesh = np.meshgrid(rx_coords, ry_coords, rz_coords, indexing='ij', sparse=True)\n",
        "    r_magnitude_values = np.sqrt(rxx_mesh**2 + ryy_mesh**2 + rzz_mesh**2)\n",
        "\n",
        "    r_bins_def = np.linspace(0, L_box_param / 2, 50) \n",
        "    \n",
        "    corr_binned_values, _ = np.histogram(\n",
        "        r_magnitude_values.ravel(), bins=r_bins_def,\n",
        "        weights=xi_normalized.ravel()\n",
        "    )\n",
        "    counts_in_bins, _ = np.histogram(r_magnitude_values.ravel(), bins=r_bins_def)\n",
        "    \n",
        "    corr_binned_final = np.divide(corr_binned_values, counts_in_bins, out=np.zeros_like(corr_binned_values), where=counts_in_bins!=0)\n",
        "    r_bin_centers_final = (r_bins_def[:-1] + r_bins_def[1:]) / 2\n",
        "\n",
        "    return r_bin_centers_final, corr_binned_final\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "simulation-orchestration-sweep-v3"
      },
      "source": [
        "## Simulation Orchestration for Parameter Sweep - v3\n",
        "\n",
        "This section iterates through the defined sweep parameters, runs the LSS simulation for each combination, and collects the results. To manage output for multiple runs, detailed plots for each run will be saved to Google Drive, and a summary of key metrics (peak locations, fNL) will be printed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-main-orchestration-sweep-v3"
      },
      "outputs": [],
      "source": [
        "def run_lss_sweep_simulation(config: dict, device: torch.device, checkpoint_dir: str, data_dir: str):\n",
        "    print(f\"Starting LSS Sweep Run ({config['run_id']}) on {device}...\")\n",
        "    \n",
        "    torch.manual_seed(42) \n",
        "    np.random.seed(42)\n",
        "\n",
        "    x_coords = np.linspace(-config['L_sim_unit']/2, config['L_sim_unit']/2, config['N'], dtype=np.float32)\n",
        "    X, Y, Z = np.meshgrid(x_coords, x_coords, x_coords, indexing='ij')\n",
        "    \n",
        "    seeded_modes_field = config['seeded_perturbation_amplitude'] * (\n",
        "        np.sin(config['k_seed_primary'] * X) +\n",
        "        np.sin(config['k_seed_secondary'] * Y) +\n",
        "        np.cos(config['k_seed_primary'] * Z) \n",
        "    )\n",
        "    random_background_noise = config['background_noise_amplitude'] * (np.random.rand(config['N'], config['N'], config['N']) - 0.5)\n",
        "    initial_phi_np = seeded_modes_field + random_background_noise\n",
        "\n",
        "    if np.all(initial_phi_np == 0):\n",
        "        initial_phi_np = config['background_noise_amplitude'] * (np.random.rand(config['N'], config['N'], config['N']) - 0.5)\n",
        "\n",
        "    phi = torch.from_numpy(initial_phi_np.astype(np.float16)).to(device, dtype=torch.float16)\n",
        "    phi_dot = torch.zeros_like(phi, dtype=torch.float16, device=device)\n",
        "\n",
        "    efm_model = EFMLSSModule(\n",
        "        dx=config['dx_sim_unit'], m_sq=config['m_sim_unit_inv']**2, g=config['g_sim'], eta=config['eta_sim'],\n",
        "        k_gravity=config['k_efm_gravity_coupling'], G_gravity=config['G_sim_unit'], c_sq=config['c_sim_unit']**2,\n",
        "        alpha_param=config['alpha_sim'], delta_param=config['delta_sim']\n",
        "    ).to(device)\n",
        "    efm_model.eval()\n",
        "\n",
        "    # Compile the model and the update function for performance\n",
        "    if hasattr(torch, 'compile'):\n",
        "        efm_model_compiled = torch.compile(efm_model, mode=\"max-autotune\")\n",
        "        print(\"torch.compile enabled for efm_model.\")\n",
        "    else:\n",
        "        print(\"torch.compile not available or not enabled. Running without compilation.\")\n",
        "        efm_model_compiled = efm_model \n",
        "\n",
        "    sim_start_time = time.time()\n",
        "    numerical_error = False\n",
        "\n",
        "    for t_step in tqdm(range(config['T_steps']), desc=f\"Sweep Run ({config['run_id']})\"):\n",
        "        if torch.any(torch.isinf(phi)) or torch.any(torch.isnan(phi)) or \\\n",
        "           torch.any(torch.isinf(phi_dot)) or torch.any(torch.isnan(phi_dot)):\n",
        "            print(f\"\\nERROR: NaN/Inf detected in fields at step {t_step + 1}! Stopping run {config['run_id']}.\\n\")\n",
        "            numerical_error = True\n",
        "            break\n",
        "        with amp.autocast(device_type=device.type, dtype=torch.float16): \n",
        "            # Pass the potentially compiled model instance\n",
        "            phi, phi_dot = update_phi_rk4_lss(phi, phi_dot, config['dt_sim_unit'], efm_model_compiled)\n",
        "\n",
        "    sim_duration = time.time() - sim_start_time\n",
        "    print(f\"Sweep Run {config['run_id']} finished in {sim_duration:.2f} seconds. Error: {numerical_error}\")\n",
        "\n",
        "    final_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    final_data_filename = os.path.join(data_dir, f\"SWEEP_DATA_{config['run_id']}_{final_timestamp}.npz\")\n",
        "    np.savez_compressed(final_data_filename,\n",
        "                        phi_final_cpu=phi.cpu().numpy(),\n",
        "                        config_lss=config,\n",
        "                        sim_had_numerical_error=numerical_error)\n",
        "    print(f\"Final state saved to {final_data_filename}\")\n",
        "\n",
        "    del phi, phi_dot, efm_model\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return final_data_filename\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "analysis-plotting-sweep-v3"
      },
      "source": [
        "## Analysis and Plotting for Sweep Results - v3\n",
        "\n",
        "This section defines how to analyze each sweep run's final state for clustering peaks and non-Gaussianity. A summary table will be generated for all sweep points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-analysis-plotting-sweep-v3"
      },
      "outputs": [],
      "source": [
        "def analyze_and_plot_sweep_result(data_file_path: str, sweep_results_summary_list: list, data_output_path: str):\n",
        "    print(f\"Analyzing sweep result from: {data_file_path}\")\n",
        "    \n",
        "    current_run_summary = {\n",
        "        'run_id': os.path.basename(data_file_path), \n",
        "        'g_sim': np.nan, 'k_efm_gravity_coupling': np.nan,\n",
        "        'm_sim_unit_inv': np.nan, 'alpha_sim': np.nan, \n",
        "        'eta_sim': np.nan, 'delta_sim': np.nan, \n",
        "        'status': 'Failed', 'max_phi_amplitude': np.nan,\n",
        "        'pk_peak_k': np.nan, 'pk_peak_lambda_sim': np.nan,\n",
        "        'xi_peak_r': np.nan, 'fNL_raw_calc': np.nan, 'fNL_calc': np.nan,\n",
        "        'physical_primary_lambda': np.nan, 'physical_secondary_lambda': np.nan,\n",
        "        'secondary_match_percent': np.nan\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        data = np.load(data_file_path, allow_pickle=True)\n",
        "        \n",
        "        phi_final_cpu = data['phi_final_cpu']\n",
        "        config = data['config_lss'].item() \n",
        "        sim_had_numerical_error = data['sim_had_numerical_error'].item()\n",
        "        \n",
        "        current_run_summary['run_id'] = config.get('run_id', os.path.basename(data_file_path))\n",
        "        current_run_summary['g_sim'] = config.get('g_sim', np.nan) \n",
        "        current_run_summary['k_efm_gravity_coupling'] = config.get('k_efm_gravity_coupling', np.nan)\n",
        "        current_run_summary['m_sim_unit_inv'] = config.get('m_sim_unit_inv', np.nan) \n",
        "        current_run_summary['alpha_sim'] = config.get('alpha_sim', np.nan)\n",
        "        current_run_summary['eta_sim'] = config.get('eta_sim', np.nan) \n",
        "        current_run_summary['delta_sim'] = config.get('delta_sim', np.nan) \n",
        "        current_run_summary['status'] = 'Completed' if not sim_had_numerical_error else 'Error'\n",
        "        current_run_summary['max_phi_amplitude'] = np.max(np.abs(phi_final_cpu))\n",
        "\n",
        "        print(f\"Data loaded for {current_run_summary['run_id']}. Error status: {sim_had_numerical_error}\")\n",
        "\n",
        "        # --- Power Spectrum and Correlation Function Analysis (Dimensionless) ---\n",
        "        k_min_plot_sim = 2 * np.pi / config['L_sim_unit'] * 0.5 \n",
        "        k_max_plot_sim = np.pi / config['dx_sim_unit'] * 0.9   \n",
        "\n",
        "        k_bins_sim, pk_vals_sim = compute_power_spectrum_lss(\n",
        "            phi_final_cpu, k_val_range=[k_min_plot_sim, k_max_plot_sim],\n",
        "            dx_val_param=config['dx_sim_unit'], N_grid_param=config['N']\n",
        "        )\n",
        "        r_bins_sim, xi_vals_sim = compute_correlation_function_lss(\n",
        "            phi_final_cpu, dx_val_param=config['dx_sim_unit'],\n",
        "            N_grid_param=config['N'], L_box_param=config['L_sim_unit']\n",
        "        )\n",
        "\n",
        "        # --- Identify Emergent Dimensionless Scales (improved peak detection) ---\n",
        "        if len(pk_vals_sim) > 0 and np.max(pk_vals_sim) > 1e-20: \n",
        "            pk_threshold = np.max(pk_vals_sim) * 0.05 \n",
        "            pk_peaks_indices, _ = scipy.signal.find_peaks(pk_vals_sim, height=pk_threshold, distance=5)\n",
        "            \n",
        "            if len(pk_peaks_indices) > 0:\n",
        "                closest_pk_idx = np.argmin(np.abs(k_bins_sim[pk_peaks_indices] - config['k_seed_primary']))\n",
        "                peak_k_idx = pk_peaks_indices[closest_pk_idx] \n",
        "                \n",
        "                emergent_k_peak = k_bins_sim[peak_k_idx]\n",
        "                emergent_lambda_peak = 2*np.pi / emergent_k_peak if emergent_k_peak > 0 else np.nan\n",
        "                \n",
        "                current_run_summary['pk_peak_k'] = emergent_k_peak\n",
        "                current_run_summary['pk_peak_lambda_sim'] = emergent_lambda_peak\n",
        "\n",
        "        if len(xi_vals_sim) > 0 and np.max(np.abs(xi_vals_sim)) > 1e-20: \n",
        "            r_positive_indices = np.where(r_bins_sim > 1e-5)[0] \n",
        "            if len(r_positive_indices) > 0: \n",
        "                xi_positive_r_bins = r_bins_sim[r_positive_indices]\n",
        "                xi_positive_vals = xi_vals_sim[r_positive_indices]\n",
        "                \n",
        "                xi_height_threshold = np.max(xi_positive_vals) * 0.0001 if np.max(xi_positive_vals) > 1e-10 else 1e-12\n",
        "                xi_peaks_r_indices, _ = scipy.signal.find_peaks(xi_positive_vals, height=xi_height_threshold, distance=5)\n",
        "                \n",
        "                if len(xi_peaks_r_indices) > 0:\n",
        "                    emergent_r_peak = xi_positive_r_bins[xi_peaks_r_indices[0]] \n",
        "                    current_run_summary['xi_peak_r'] = emergent_r_peak\n",
        "\n",
        "        # --- Physical Interpretation ---\n",
        "        EFM_PRIMARY_LSS_Mpc = 628.0 \n",
        "        EFM_SECONDARY_LSS_Mpc = 157.0 \n",
        "\n",
        "        S_L = np.nan\n",
        "        if not np.isnan(current_run_summary['pk_peak_lambda_sim']) and current_run_summary['pk_peak_lambda_sim'] > 0:\n",
        "            S_L = EFM_PRIMARY_LSS_Mpc / current_run_summary['pk_peak_lambda_sim']\n",
        "            \n",
        "            physical_primary_lambda_seeded = (config['L_sim_unit'] / 2.0) * S_L \n",
        "            physical_secondary_lambda_seeded = (config['L_sim_unit'] / 8.0) * S_L \n",
        "            \n",
        "            current_run_summary['physical_primary_lambda'] = physical_primary_lambda_seeded\n",
        "            current_run_summary['physical_secondary_lambda'] = physical_secondary_lambda_seeded\n",
        "\n",
        "            if EFM_SECONDARY_LSS_Mpc > 0:\n",
        "                secondary_match_percent = abs((physical_secondary_lambda_seeded - EFM_SECONDARY_LSS_Mpc) / EFM_SECONDARY_LSS_Mpc) * 100\n",
        "                current_run_summary['secondary_match_percent'] = secondary_match_percent\n",
        "\n",
        "        # --- Non-Gaussianity Analysis (fNL) ---\n",
        "        N_grid = config['N']\n",
        "        dx_val = config['dx_sim_unit']\n",
        "        \n",
        "        rho_final_np_for_fNL = (config.get('k_efm_gravity_coupling', 1.0) * phi_final_cpu**2).astype(np.float32)\n",
        "        \n",
        "        if np.all(rho_final_np_for_fNL == 0):\n",
        "            current_run_summary['fNL_raw_calc'] = np.nan\n",
        "            current_run_summary['fNL_calc'] = np.nan\n",
        "        else:\n",
        "            rhok_fft = fftn(rho_final_np_for_fNL)\n",
        "\n",
        "            kx_coords_f = fftfreq(N_grid, d=dx_val).astype(np.float32) * 2 * np.pi\n",
        "            ky_coords_f = fftfreq(N_grid, d=dx_val).astype(np.float32) * 2 * np.pi\n",
        "            kz_coords_f = fftfreq(N_grid, d=dx_val).astype(np.float32) * 2 * np.pi\n",
        "            k_magnitude_f = np.sqrt(kx_coords_f[:, None, None]**2 + ky_coords_f[None, :, None]**2 + kz_coords_f[None, None, :]**2)\n",
        "\n",
        "            target_k_for_fNL_sim = current_run_summary['pk_peak_k'] \n",
        "            if np.isnan(target_k_for_fNL_sim) or target_k_for_fNL_sim <= 0:\n",
        "                target_k_for_fNL_sim = k_min_plot_sim \n",
        "\n",
        "            k_tolerance_fNL_absolute = 0.5 \n",
        "            mask_fNL = (k_magnitude_f > (target_k_for_fNL_sim - k_tolerance_fNL_absolute)) & \\\n",
        "                       (k_magnitude_f < (target_k_for_fNL_sim + k_tolerance_fNL_absolute))\n",
        "            \n",
        "            if not np.any(mask_fNL): \n",
        "                mask_fNL = (k_magnitude_f > 1e-5) & (k_magnitude_f < (np.pi / dx_val * 0.1)) \n",
        "            \n",
        "            if np.any(mask_fNL):\n",
        "                # FIX: Corrected np.roll usage (removed 'shifts' keyword) for NumPy\n",
        "                B_simplified_values = rhok_fft * np.roll(rhok_fft, -1, axis=0) * np.roll(rhok_fft, -1, axis=1) \n",
        "                B_simplified = np.mean(np.abs(B_simplified_values[mask_fNL]))\n",
        "                P_simplified = np.mean(np.abs(rhok_fft[mask_fNL])**2)\n",
        "\n",
        "                if P_simplified > 1e-30:\n",
        "                    fNL_raw_calculated = (5/18) * (B_simplified / (P_simplified**2)) \n",
        "                    current_run_summary['fNL_raw_calc'] = fNL_raw_calculated\n",
        "\n",
        "                    calibration_factor = 5.2 / fNL_raw_calculated if fNL_raw_calculated != 0 else 1.0\n",
        "                    current_run_summary['fNL_calc'] = fNL_raw_calculated * calibration_factor\n",
        "\n",
        "        # --- TEMPORARY: PLOT P(k) and xi(r) for visual debugging --- \n",
        "        plt.figure(figsize=(16,6))\n",
        "        \n",
        "        plt.subplot(1,2,1)\n",
        "        if len(k_bins_sim) > 0: \n",
        "            plt.loglog(k_bins_sim, pk_vals_sim, label='P(k) Emergent')\n",
        "            plt.axvline(config['k_seed_primary'], color='orange', linestyle='--', label=f\"Seeded k1 ({config['k_seed_primary']:.2f})\")\n",
        "            plt.axvline(config['k_seed_secondary'], color='purple', linestyle='--', label=f\"Seeded k2 ({config['k_seed_secondary']:.2f})\")\n",
        "        # Updated title for v3 to reflect eta and delta\n",
        "        plt.title(f\"P(k) (Dimless) for eta={config.get('eta_sim',np.nan):.1e}, delta={config.get('delta_sim',np.nan):.1e}\") \n",
        "        plt.xlabel('k (Dimensionless Units)'); plt.ylabel('P(k) (Dimensionless Units)'); plt.grid(True, which='both')\n",
        "        plt.legend(); plt.xlim([k_min_plot_sim, k_max_plot_sim])\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        if len(r_bins_sim) > 0: \n",
        "            plt.plot(r_bins_sim, xi_vals_sim, label='xi(r) Emergent')\n",
        "            plt.axhline(0, color='black', linewidth=0.5)\n",
        "            plt.axvline(config['L_sim_unit'] / 2.0, color='orange', linestyle='--', label=f\"Seeded lambda1 ({config['L_sim_unit']/2.0:.2f})\")\n",
        "            plt.axvline(config['L_sim_unit'] / 8.0, color='purple', linestyle='--', label=f\"Seeded lambda2 ({config['L_sim_unit']/8.0:.2f})\")\n",
        "        # Updated title for v3 to reflect eta and delta\n",
        "        plt.title(f\"xi(r) (Dimless) for eta={config.get('eta_sim',np.nan):.1e}, delta={config.get('delta_sim',np.nan):.1e}\") \n",
        "        plt.xlabel('r (Dimensionless Units)'); plt.ylabel(r'$\\xi$(r) (Dimensionless Units)'); plt.grid(True)\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0.03, 1, 0.95]) \n",
        "        plot_filename_obs = os.path.join(data_output_path, f\"sweep_obs_{current_run_summary['run_id']}.png\")\n",
        "        plt.savefig(plot_filename_obs)\n",
        "        plt.close()\n",
        "\n",
        "    except Exception as e:\n",
        "        current_run_summary['status'] = f\"Analysis Error: {type(e).__name__}: {e}\" \n",
        "        print(f\"Error analyzing file {data_file_path}: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    sweep_results_summary_list.append(current_run_summary)\n",
        "\n",
        "    del phi_final_cpu \n",
        "    gc.collect()\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "main-orchestration-sweep-v3"
      },
      "source": "## Main Orchestration Loop - v3\n\nThis is the primary execution block that runs all sweep simulations and performs the analysis.\nIt will iterate through the defined `eta_sim` and `delta_sim` parameter combinations."
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "code-main-orchestration-sweep-v3"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # --- Colab specific setup ---\n",
        "    # Define paths for checkpoints and data/plots for v3 sweep\n",
        "    checkpoint_path_lss_sweep = '/content/drive/My Drive/EFM_Simulations/checkpoints/LSS_DIMLESS_A100_Sweep_v3/'\n",
        "    data_path_lss_sweep = '/content/drive/My Drive/EFM_Simulations/data/LSS_DIMLESS_A100_Sweep_v3/'\n",
        "\n",
        "    try:\n",
        "        from google.colab import drive\n",
        "        drive.mount('/content/drive')\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "        os.makedirs(checkpoint_path_lss_sweep, exist_ok=True)\n",
        "        os.makedirs(data_path_lss_sweep, exist_ok=True)\n",
        "    except ImportError:\n",
        "        print(\"Not in Google Colab environment. Skipping Google Drive mount.\")\n",
        "        checkpoint_path_lss_sweep = './EFM_Simulations/checkpoints/LSS_DIMLESS_A100_Sweep_v3/'\n",
        "        data_path_lss_sweep = './EFM_Simulations/data/LSS_DIMLESS_A100_Sweep_v3/'\n",
        "        os.makedirs(checkpoint_path_lss_sweep, exist_ok=True)\n",
        "        os.makedirs(data_path_lss_sweep, exist_ok=True)\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}. Please ensure you're logged in and have granted permissions.\")\n",
        "        checkpoint_path_lss_sweep = './EFM_Simulations/checkpoints/LSS_DIMLESS_A100_Sweep_v3/'\n",
        "        data_path_lss_sweep = './EFM_Simulations/data/LSS_DIMLESS_A100_Sweep_v3/'\n",
        "        os.makedirs(checkpoint_path_lss_sweep, exist_ok=True)\n",
        "        os.makedirs(data_path_lss_sweep, exist_ok=True)\n",
        "\n",
        "    print(f\"LSS Sweep Checkpoints will be saved to: {checkpoint_path_lss_sweep}\")\n",
        "    print(f\"LSS Sweep Data/Plots will be saved to: {data_path_lss_sweep}\")\n",
        "\n",
        "    # --- Prepare sweep configurations (for THIS new sweep: eta_sim & delta_sim) ---\n",
        "    base_config = {\n",
        "        'N': 400,  # Increased N for better GPU utilization\n",
        "        'L_sim_unit': 10.0,\n",
        "        'c_sim_unit': 1.0,\n",
        "        'dt_cfl_factor': 0.001,\n",
        "        'T_steps': 50000,  \n",
        "        'm_sim_unit_inv': 1.0, # Fixed at this value for v3 sweep\n",
        "        'g_sim': 0.1,  # Fixed at this value for v3 sweep\n",
        "        'k_efm_gravity_coupling': 0.005, # Fixed at this value for v3 sweep\n",
        "        'alpha_sim': 0.7, # Fixed at this value for v3 sweep\n",
        "        'G_sim_unit': 1.0,\n",
        "        'seeded_perturbation_amplitude': 1.0e-3,\n",
        "        'background_noise_amplitude': 1.0e-6,\n",
        "        'k_seed_primary': 2 * np.pi / (10.0 / 2.0),  # L/2 wavelength (5.00 dimensionless)\n",
        "        'k_seed_secondary': 2 * np.pi / (10.0 / 8.0),  # L/8 wavelength (1.25 dimensionless)\n",
        "        'history_every_n_steps': 1000,\n",
        "        'checkpoint_every_n_steps': 5000,\n",
        "    }\n",
        "    base_config['dx_sim_unit'] = base_config['L_sim_unit'] / base_config['N']\n",
        "    base_config['dt_sim_unit'] = base_config['dt_cfl_factor'] * base_config['dx_sim_unit'] / base_config['c_sim_unit']\n",
        "\n",
        "    eta_values = [0.001, 0.01, 0.1]  \n",
        "    delta_values = [0.0001, 0.0002, 0.0005]  \n",
        "\n",
        "    sweep_params = []\n",
        "    for eta_val in eta_values:\n",
        "        for delta_val in delta_values:\n",
        "            config = base_config.copy()\n",
        "            config['eta_sim'] = eta_val\n",
        "            config['delta_sim'] = delta_val\n",
        "            config['run_id'] = (\n",
        "                f\"LSS_Sweep_N{config['N']}_T{config['T_steps']}_\" +\n",
        "                f\"eta{config['eta_sim']:.1e}_delta{config['delta_sim']:.1e}_\" +\n",
        "                f\"m{config['m_sim_unit_inv']:.1e}_alpha{config['alpha_sim']:.1e}_\" +\n",
        "                f\"g{config['g_sim']:.1e}_k{config['k_efm_gravity_coupling']:.1e}_\" +\n",
        "                f\"A100_DIMLESS_Sweep_v3\" \n",
        "            )\n",
        "            sweep_params.append(config)\n",
        "\n",
        "    print(f\"Prepared {len(sweep_params)} sweep configurations.\")\n",
        "\n",
        "    main_device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # --- Run all sweep simulations and analyze ---\n",
        "    sweep_results_summary = [] \n",
        "    for i, config_lss_run_current in enumerate(sweep_params):  \n",
        "        print(f\"\\n--- Running Sweep Point {i+1}/{len(sweep_params)}: eta={config_lss_run_current['eta_sim']:.2g}, delta={config_lss_run_current['delta_sim']:.2g} ---\\n\")\n",
        "        \n",
        "        final_data_file_path = run_lss_sweep_simulation(config_lss_run_current, main_device, checkpoint_path_lss_sweep, data_path_lss_sweep)\n",
        "\n",
        "        if final_data_file_path:\n",
        "            analyze_and_plot_sweep_result(final_data_file_path, sweep_results_summary, data_path_lss_sweep) \n",
        "        else:\n",
        "            run_summary = {\n",
        "                'run_id': config_lss_run_current['run_id'],\n",
        "                'status': 'Simulation Failed to Produce File',\n",
        "                'g_sim': config_lss_run_current['g_sim'],\n",
        "                'k_efm_gravity_coupling': config_lss_run_current['k_efm_gravity_coupling'],\n",
        "                'm_sim_unit_inv': config_lss_run_current['m_sim_unit_inv'],\n",
        "                'alpha_sim': config_lss_run_current['alpha_sim'],\n",
        "                'eta_sim': config_lss_run_current['eta_sim'], \n",
        "                'delta_sim': config_lss_run_current['delta_sim'], \n",
        "                'max_phi_amplitude': 'N/A',\n",
        "                'pk_peak_k': 'N/A',\n",
        "                'pk_peak_lambda_sim': 'N/A',\n",
        "                'xi_peak_r': 'N/A',\n",
        "                'fNL_raw_calc': 'N/A',\n",
        "                'fNL_calc': 'N/A',\n",
        "                'physical_primary_lambda': 'N/A',\n",
        "                'physical_secondary_lambda': 'N/A',\n",
        "                'secondary_match_percent': 'N/A'\n",
        "            }\n",
        "            sweep_results_summary.append(run_summary)\n",
        "\n",
        "    print(f\"\\n--- Parameter Sweep Summary ({datetime.now().strftime('%Y%m%d_%H%M%S')}) ---\\n\")\n",
        "    headers = [\"Run ID (eta_delta)\", \"g_sim\", \"k_grav\", \"m_sim\", \"alpha\", \"eta\", \"delta\", \"Status\", \"Max Phi\", \n",
        "               \"PK Peak k\", \"PK Peak L\", \"Xi Peak r\", \"fNL (Cal)\", \"fNL (Raw)\", \n",
        "               \"Phys L1 (Seed)\", \"Phys L2 (Seed)\", \"L2 Match % (Seed)\"]\n",
        "    \n",
        "    def format_val_for_table(val):\n",
        "        if isinstance(val, float) and np.isnan(val): return 'N/A'\n",
        "        if isinstance(val, (float, np.float32, np.float64)): return f\"{val:.2e}\"\n",
        "        if isinstance(val, str) and val == 'N/A': return val\n",
        "        if isinstance(val, str) and val.startswith(\"LSS_Sweep_N\"): # Truncate for display\n",
        "            start_idx = val.find(\"eta\") \n",
        "            end_idx = val.find(\"m1.0e+00_alpha7.0e-01\") # Specific to v3 format ending\n",
        "            if start_idx != -1 and end_idx != -1:\n",
        "                return val[start_idx:end_idx]\n",
        "            return val \n",
        "        return str(val)\n",
        "\n",
        "    header_row_str = \"{:<25} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(*headers)\n",
        "    print(header_row_str)\n",
        "    print(\"-\" * len(header_row_str))\n",
        "\n",
        "    for res in sweep_results_summary:\n",
        "        row_vals = [\n",
        "            format_val_for_table(res['run_id']), \n",
        "            format_val_for_table(res['g_sim']),\n",
        "            format_val_for_table(res['k_efm_gravity_coupling']),\n",
        "            format_val_for_table(res['m_sim_unit_inv']),\n",
        "            format_val_for_table(res['alpha_sim']),\n",
        "            format_val_for_table(res['eta_sim']),\n",
        "            format_val_for_table(res['delta_sim']),\n",
        "            res['status'],\n",
        "            format_val_for_table(res['max_phi_amplitude']),\n",
        "            format_val_for_table(res['pk_peak_k']),\n",
        "            format_val_for_table(res['pk_peak_lambda_sim']),\n",
        "            format_val_for_table(res['xi_peak_r']),\n",
        "            format_val_for_table(res['fNL_calc']),\n",
        "            format_val_for_table(res['fNL_raw_calc']),\n",
        "            format_val_for_table(res['physical_primary_lambda']),\n",
        "            format_val_for_table(res['physical_secondary_lambda']),\n",
        "            format_val_for_table(res['secondary_match_percent'])\n",
        "        ]\n",
        "        print(\"{:<25} {:<8} {:<8} {:<8} {:<8} {:<8} {:<8} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(*row_vals))\n",
        "    print(\"-\" * len(header_row_str))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}